[{"categories":["分布式系统"],"content":"Multi-DLedger ","date":"2022-05-09","objectID":"/multi-dledger/:0:0","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"DLedger的Multi-Raft架构 整体结构如下 DLedgerProxy具体架构如下 针对上图的几大组件，做如下详细解释。 ","date":"2022-05-09","objectID":"/multi-dledger/:1:0","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"DLedgerProxy 我们之前在DLedger中，是直接使用了DLedgerServer作为接收处理请求的载体。若需要升级为Multi-Raft架构，则需要将对于收发请求的部分和处理请求的部分进行解耦。也就是我们可以使用一个类似容器的思想，使用一层代理来代理下层的Raft节点，我们可以多个DLedgerServer公用一个RPCService来进行请求处理，那么这个DLedgerProxy就是将之前实现的handle请求等方法进行包装，并且可以根据配置进行路由。也就是我们ip+port不能唯一标识一个Raft节点了，因为ip+port也可以表现为一个Multi-Raft结构对外服务，那么这时候，我们就需要一个selfID来对该Multi-Raft中的不同的Raft进行区分。此时我们的所有DLedger通信都需要带上selfID来进行定位。并且我们在DLedgerProxy中加入多个模块，分别是ConfigManager和DLedgerManager。这两个模块分别用来管理Raft-Group的配置和本机DLedgerServer实例的配置以及处理逻辑。 为什么使用ip:port+selfID来区分？ 我们之前是一个ip+port就可以定位一个Raft实例，但是现在我们ip+port定位到的进程是一个Multi-Raft，因此需要根据selfID来进行在进程中的定位。 为什么要合用一个RPCService？ 如果我们给一个DLedger就实例化一个RPCService来进行请求响应的话。 首先对于实例化RPCService是有一定的开销，如果开多个RPCService就需要绑定多个端口，对于程序和系统都是一定的消耗。 其次，我们不仅处理DLedgerServer的请求响应，同样的对于后续若需要进行配置更改等操作，我们可以统一在一个RPCService处接收处理。而且可以将请求响应与实际逻辑处理解耦合，有更强的拓展性。 最后，我们使用同一个RPCService进行网络请求处理，可以做许多优化，比如说对于发往同一个DLedger的请求进行合并，减少网络io消耗；对于同一个DLedger，共享一个心跳计时器，减少网络io。 ","date":"2022-05-09","objectID":"/multi-dledger/:1:1","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"ConfigManager ConfigManager是DLedger的配置管理模块。后续如果使用配置中心等方案，可以动态的对于Raft-Group负责的Region进行在线更改。此时ConfigManager就可以进行相应的请求和响应。因为我们多个DLedgerServer当然是可以公用一个RegionConfig的，因为RegionConfig是对于所有该集群中的Raft都是一样的。 同时可以管理和配置本机的DLedger实例的配置，可以在启动时根据启动参数进行DLedger实例化，也可以实时的根据网络请求来对本机的DLedger实例的配置文件进行更改，从而管理本机的DLedger实例。 ","date":"2022-05-09","objectID":"/multi-dledger/:1:2","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"DLedgerManager DLedgerManager是对DLedgerServer进行管理的模块，功能包括，对于针对DLedgerServer的网络请求，可以根据配置来进行路由到某个DLedgerServer；对于后续的优化，比如上述提到的请求合并，可以在此模块统一处理。 ","date":"2022-05-09","objectID":"/multi-dledger/:1:3","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"Load balance 我们需要Multi-Raft架构来进行集群的负载均衡。通过使用Multi-Raft，让一个进程中开启多个DLedgerServer，并且这个DLedgerServer处于不同的RaftGroup，而且各个Group的Leader在集群中是均衡分配的。也就是若为三RaftGroup三物理节点，每个节点开启一个Multi-Raft DLedger，并且分别配置三个Raft节点，那么就是每个物理节点上实际上是有一个RaftGroup的Leader的，那么可以一定程度上让各个物理节点的负载均衡。 怎么实现Leader均匀分配？ 使用perferLeader来在一开始的配置中分配好Leader即可。DLedger已经实现该功能。 怎么知道是否均衡分配了？ 留一个查询本Multi-Raft进程中所有Raft的信息，根据信息可以知道哪些节点是否成为Leader。 ","date":"2022-05-09","objectID":"/multi-dledger/:1:4","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"MultiStorage 我们同样也需要给每个DLedger一个存储空间，并且对不同的DLedger的存储之间进行隔离。这里我们只需要对多个Storage实例进行管理即可。 ","date":"2022-05-09","objectID":"/multi-dledger/:1:5","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"开发任务 完成将DLedgerProxy和DLedgerServer的分离。兼容单DLedgerServer的初始化。 增加Multi-Raft配置，可以根据配置进行多DLedgerServer实例化。 增加ConfigManager模块。 增加DLedgerManager模块。 对MultiStorage进行适配。 对PerferLeader的Multi-Raft结构进行适配。 ","date":"2022-05-09","objectID":"/multi-dledger/:1:6","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"优化任务 动态监听更改进程的DLedger实例配置。 ConfigManager的RegionConfig的在线更新。 针对同一个DLedger的请求合并。 ","date":"2022-05-09","objectID":"/multi-dledger/:1:7","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"如何以Multi-DLedger启动 若需要以Multi-DLedger启动，需要使用配置文件启动 简易配置包括： DLedgerConfigs:- group:g0id:n0peers:n0-127.0.0.1:10000;n1-127.0.0.1:10001;n2-127.0.0.1:10002preferredLeaderId:n0- DLedgerConfig:group:g1id:a0peers:a0-127.0.0.1:10000;a1-127.0.0.1:10001;a2-127.0.0.1:10002preferredLeaderId:a1- DLedgerConfig:group:g2id:b0peers:b0-127.0.0.1:10000;b1-127.0.0.1:10001;b2-127.0.0.1:10002preferredLeaderId:b2 根据上述配置进行多个DLedgerConfig的实例化，然后根据DLedgerConfig来进行DLedgerServer来实例化。我们可以看到我们现在一个端口用来处理多个Raft节点的请求，使用id来进行表示。 ","date":"2022-05-09","objectID":"/multi-dledger/:2:0","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"需要完成的要点 对配置启动参数进行更新，添加yaml配置文件启动的方式，先对单Raft节点的进行启动适配。 对多DLedgerServer的配置文件进行管理，使用上述提到的ConfigManager来进行管理。 ","date":"2022-05-09","objectID":"/multi-dledger/:2:1","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"接收请求和响应的流程 ","date":"2022-05-09","objectID":"/multi-dledger/:3:0","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"之前的请求响应流程 由于只有一个DLedgerServer，因此可以直接调用DLedgerServer的Handle等方法。 ","date":"2022-05-09","objectID":"/multi-dledger/:3:1","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"现在的请求响应流程 现在由于我们有多个DLedgerServer实例，那么需要根据请求的id来具体路由到某个DLedgerServer上。因此我们需要让DLedgerProxy中的DLedgerManager来根据selfID-\u003eDLedgerServer来找到具体的DLedgerServer来处理该请求。 ","date":"2022-05-09","objectID":"/multi-dledger/:3:2","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"需要完成的要点 将原来的DLedgerRpcNettyService接收请求后直接调用DLedgerServer改成调用DLedgerProxy。 现在的DLedgerRpcNettyService的注册是根据一个DLedgerServer来的。但是我们现在需要注册多个进去。并且对于同一个ip+port的remoteClient来进行合并，不需要创建多个，造成资源浪费。 ","date":"2022-05-09","objectID":"/multi-dledger/:3:3","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"在线增删DLedger节点 我们可以预留两种请求响应，称为addDLedgerRequest和deleteDLedgerRequest来让一个Multi-DLedger组来增删DLedgerServer。当然这些新增的不能是新加入某个Raft-Group的。该Raft可以是处于一个新的Raft-Group；也可以是该Raft之前被从该Multi-DLedger中删去了，后来又恢复了。这两种情况是允许的。由于目前没有做RaftGroup成员变更功能，所以只能支持上述两种情况。 ","date":"2022-05-09","objectID":"/multi-dledger/:4:0","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"增加DLedger请求流程 我们在接收到增加DLedger的请求后，由DLedgerProxy来做处理，首先它会对该请求进行判断，是否符合要求（比如说是否已有该DLedger）。接下来它会让DLedgerManager来根据配置进行初始化新的DLedger并启动，同时注册进RPC，同时也让ConfigManager来更新管理的DLedgerConfig。 ","date":"2022-05-09","objectID":"/multi-dledger/:4:1","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"删除DLedger请求流程 删除DLedger请求，也是由DLedgerProxy来做处理，先对删除的DLedgerId进行判断（是否存在）。接下来让DLedgeManager来删除该DLedgerServer的配置，同时让DLedgerManager来关闭并移除DLedgerServer。也需要对RPC中不再需要的数据进行删除。 ","date":"2022-05-09","objectID":"/multi-dledger/:4:2","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"需要完成的点 对于增删的接口的实现，以及RPC注册。 完成DLedgerProxy的对DLedgerManager和ConfigManager以及DLedgerRpcNettyService的调度。 完成DLedgerManager中增删节点的方法。 完成ConfigManager中增删节点的方法。 完成DLedgerRpcNettyService中增删节点的方法。 ","date":"2022-05-09","objectID":"/multi-dledger/:4:3","tags":["分布式系统","Raft","Java","DLedger"],"title":"Multi DLedger","uri":"/multi-dledger/"},{"categories":["分布式系统"],"content":"Distributed-txn Lab1 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:0:0","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"The Design 在lab1中，我们已经完成了Raft日志引擎以及存储引擎。使用Raft日志引擎，事务日志能够\"可靠的\"持久化，并且可以在故障转移后恢复状态。在这个章节，我们将讨论分布式事务层的设计与实现。raft日志引擎已经提供了持久性(Durability)以及状态恢复保险，事务层需要保证原子性(Atomicity)以及并发正确或者称为隔离性(Isolation)。 percolator协议被用于保证原子性(Atomicity)，以及使用全局时间戳来对并发事务的执行进行顺序排序。基于这些，可以为客户端提供一个强隔离级别，通常被成为快照隔离(Snapshot Isolation)或可重复读(Repeatable Read)。percolator协议在tinysql和tinykv的服务端都被实现了，以及全局事务时间戳分配由tinyscheduler服务端完成，以及所有逻辑时间戳都是单调递增的。在这个lab中，我们将要实现tinykv中的percolator部分，或者说是分布式事务的参与者部分。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:1:0","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Implement The Core Interface of StandAloneStorage 尝试去实现kv/storage/standalone_storage/standalone_storage.go中的缺失代码，这些代码部分被使用如下注释标记： // YOUR CODE HERE (lab1). 完成这些部分之后，运行make lab1P0命令来检查是否所有的测试用例都通过了。注意事项： 由于badger被用于存储引擎，因此常见的用法可以在其文档和存储库中找到。 badger存储引擎不支持column family。percolator事务模型需要列族，在tinykv中，列族相关功能已经被包装在kv/util/engine_util/util.go中，当处理storage.Modify时，被写入到存储引擎中的键应该使用KeyWithCF函数来编码，考虑到它所期望的列族。在tinykv中有两种Modify类型，检查kv/storage/modify.go来获取更多的信息。 Scheduler_client.Client在standAloneServer中不会被使用，因此它可以被跳过。 考虑下badger提供的拥有txn特性的读写接口。去BadgerReader中获取更多消息。 一些测试用例可能有助于理解存储接口的用法。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:1:1","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Implement Percolator In TinyKV 实现TinyKV中的percolator 事务处理流程如下： 用户发送写请求比如说Insert到tinysql服务端，该请求被解析以及执行，然后用户数据被从行数据转变为键值对。然后事务模块将负责提交该键值对到存储引擎。由于不同的键可能分布在不同的区域，而这些区域可能又分布在不同的tinykv服务器上，因此事务引擎必须确保提交过程最终成功或者什么也不做，这就是percolator协议。 考虑提交一个多键的分布式事务。首先，将被提交的键中有一个被选择作为主键(Primary Key)，然后事务状态仅取决于主键的状态。换句话说，事务将被认为成功提交只有当主键(Primary Key)被成功提交。提交过程被分成两阶段，第一阶段是Prewrite，第二阶段是Commit。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:2:0","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Prewrite Phase 预写阶段 在键值对准备好之后，事务进入到Prewrite阶段，在该阶段所有的键将被预写到不同的tinykv服务器包含不同的区域领袖。每一个Prewrite请求将会被一个tinykv服务器处理，预写锁(Prewrite Lock)将会被放入存储引擎中的每个键的锁列族(lock column family)中。如果任何的预写处理失败那么提交处理将会立即失败，然后任何预写锁都会被清理。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:2:1","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Commit Phase 提交阶段 如果所有的预写处理都成功了，事务将会进入提交(Commit)阶段，该阶段中主键(Primary Key)将会被第一个提交。提交(Commit)处理操作会在存储引擎中提交一个写记录(Write Record)到写列族(Write cloumn family) 并且解锁锁列族(lock column family)中的预写锁(Prewrite Lock)。如果主键的提交操作已经成功那么事务将被认为成功提交了并且将发送成功相应到客户端。所有被称为副键(Secondary Keys)将会在后台任务中被异步 提交。 在一般的情况下，两阶段已经足够了，但是没有做故障恢复。在分布式环境下任何地方都有可能失败，比如说tinysql服务器可能在完成两阶段提交(Two Phase Commit)前发生了失败，tinykv服务器也一样。当一个新的区域领袖被选举出来然后开始处理请求，我们怎么恢复未完成的事务并且确保正确性？ ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:2:2","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Rollback Record 回滚记录 一旦事务被决定为失败，它留下的锁应该被清理以及一条回滚记录应该被放入到存储引擎以防止将来可能的预写或者提交(考虑到网络延迟等)。因此如果将回滚记录放在事务的主键上，则事务状态被确定为回滚。它将永远不会被提交，并且必须失败。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:2:3","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Check The Transaction Status 检查事务状态 如上所述，事务的最终状态只取决于主键(Primary Key)或者主锁(Primary Lock)的状态。如果一些事务状态没有被决定，它需要检查它的主键(Primary Key)或者主锁(Primary Lock)的状态。如果有主键的提交或者回滚记录，然后我们可以安全的认为事务已经被提交了或者回滚了。如果主锁仍然存在或者没有没有过期，那么事务提交仍是可能的，仍在进行。如果没有锁记录和提交/回滚记录，那么事务状态就是未确定的，我们可以选择等待或者写入一个回滚记录来防止之后的提交，然后该事务就决定回滚了。每一个在两阶段提交中的预写锁(Prewrite Lock)将拥有一个存活时间(Time To Live)字段，如果锁的ttl已经过期了那么该锁将会被并发命令比如说CheckTxnSrtatus来回滚，之后该事务最终一定会失败。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:2:4","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Process Conflicts And Do Recovery 处理冲突以及恢复 不同的事务协调者位于不同的tinysql服务端，读和写请求可能在它们之中发生冲突。考虑如下情况，一个事务txn1在键k1上有一个预写锁，另一个读事务txn2尝试读取键k1，那么txn2该如何处理呢？ 读和写请求无法继续因为锁记录阻塞了它们，有如下几种可能性： 锁属于的事务已经提交了，同样也可以提交锁，接下来在该键上阻塞的请求可以继续。 锁属于的事务已经回滚了，同样也可以回滚锁，接下来在该键上阻塞的请求可以继续。 锁属于的事务仍在继续，阻塞的请求必须等待该事务完成。 这些冲突处理在tinysql/tinykv集群中被称为Resolve。一旦请求被一个别的事务的预写锁阻塞的时候，resolve将用于帮助确定锁的状态然后帮助该锁提交或者回滚，以至于该请求可以继续。Resolve操作隐式的协助了事务恢复。假设一个情况，事务调度器完成了预写锁然后tinysql服务器宕机了以及那些留下来的锁将在别的tinysql服务器的并发事务请求中被解决。 Lab2 我们将会在tinykv服务端实现上述接口以及处理逻辑。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:2:5","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Code ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:3:0","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"The Command Abstraction 命令的抽象 在kv/transaction/commands/command.go中，那里有所有事务命令的接口。 // Command is an abstraction which covers the process from receiving a request from gRPC to returning a response. type Command interface { Context() *kvrpcpb.Context StartTs() uint64 // WillWrite returns a list of all keys that might be written by this command. Return nil if the command is readonly. WillWrite() [][]byte // Read executes a readonly part of the command. Only called if WillWrite returns nil. If the command needs to write // to the DB it should return a non-nil set of keys that the command will write. Read(txn *mvcc.RoTxn) (interface{}, [][]byte, error) // PrepareWrites is for building writes in an mvcc transaction. Commands can also make non-transactional // reads and writes using txn. Returning without modifying txn means that no transaction will be executed. PrepareWrites(txn *mvcc.MvccTxn) (interface{}, error) } WillWrite创建该请求所需写入的内容，Read将执行该命令所需的读取请求。PrepareWrites用来为这个命令构建实际的写内容，它是写命令处理的核心部分。由于每个事务都有它唯一的标识符，即start_ts也就是分配的全局时间戳，因此可以使用StartTs来返回当前命令的值。 尝试理解整个客户端请求处理的过程(事务命令处理和raftstore日志提交/应用)。事务命令导致一些写突变，这些突变将会被转为raft命令请求，然后发送到raftstore，处理之后，在raftstore中提交并应用处理，事务命令被认为时成功的，结果发回给客户端。 这个lab忽略了raftstore，对于该兴趣的同学，可以去TinyKV看看。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:3:1","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Implement the Get Command 实现Get命令 KvGet用来执行来获取特定键的值，尝试实现kv/transaction/commands/get.go中缺失的代码，这些代码部分被使用如下标记： // YOUR CODE HERE (lab1). ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:3:2","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Implement the Prewrite and Commit Commands 实现预写(Prewrite)以及提交(Commit)命令 关于我们的事务引擎有两个最重要的接口，尝试去实现kv/transaction/commands/prewrite.go和kv/transaction/commands/commit.go中缺失的代码。这些代码部分被使用如下标记： // YOUR CODE HERE (lab1). 注意事项： 可能会有重复的请求，因为这些请求是由tinysql服务端发送的RPC请求。 StartTS只是一个事务开始的标识。 CommitTS是一个记录的预期提交的时间戳。 考虑读写处理冲突。 完成这两部分，运行make lab2P1来检查是否所有的测试都通过了。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:3:3","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Implement the Rollback and CheckTxnStatus Commands 实现回滚(Rollback)和检查事务状态(CheckTxnStatus)两个命令 回滚(Rollback)用于解锁键，以及添加一个键的回滚记录(Rollback Record)。检查事务状态(CheckTxnStatus)用于查询指定事务的主键锁的状态。尝试实现kv/transaction/commands/rollback.go和kv/transaction/commands/checkTxn.go中缺失的代码。这些代码部分被使用如下标记： // YOUR CODE HERE (lab1). 完成这些部分，运行make lab2P2来检查是否所有的测试都已经通过。注意事项如下： 考虑到查询的锁不存在的情况。 可能会有重复的请求，因为这些请求是由tinysql服务端发送的RPC请求。 在CheckTxnStatusResponse中有三种不同的响应操作。Action_TTLExpireRollback意味着目标锁已经被回滚了因为锁已经过期了，以及Action_LockNotExistRollback意味着目标锁不存在以及回滚记录已经被写入了。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:3:4","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Implement the ResolveLock Command 实现ResolveLock命令 Resolve用于提交或者回滚锁当它关联的事务的状态已经被确定了。尝试实现kv/transaction/commands/resolve.go中缺失的代码。这些代码部分被使用如下标记： // YOUR CODE HERE (lab1). 当完成这些部分时，运行make lab2P3来检查是否所有测试都已经被通过了。注意事项如下： 事务状态已经在输入请求参数中确定。 rollbackKey和commitKey是有用的。 当所有的命令和测试都完成了，运行make lab2P4来检查是否别的情况测试也能通过。在下一个lab我们尝试实现tinysql服务端中percolator协议中的事务协调者部分，以及将被使用的所有命令。 ","date":"2022-04-22","objectID":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/:3:5","tags":["分布式系统","Golang","事务","PingCAP"],"title":"Distributed Txn Lab翻译","uri":"/distributed-txn-lab%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"MIT-6.824-Lab3 ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:0:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"前言 这个Lab，我们需要实现一个基于Raft库实现的容错性分布式KV存储服务。 实现之前，先仔细阅读官方Lab指导 官方Lab指导 个人翻译后的官方Lab指导 再去了解一下线性一致性读 关于一致性 以及官方提供给的一个交互图 官方交互图 各个模块交互流程image-20220320152754046 \"\r各个模块交互流程\r 并且阅读Raft博士论文中的Figure6.1 Raft博士论文 ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:1:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"ClientRequest RPC 参数： clientId 调用请求的客户端id sequenceNum 用于消除重复请求 command 请求状态机的命令，可能会影响状态 结果： status 如果状态机成功应用命令则返回OK response 如果请求成功，则为状态机的输出 leaderHint 最近的领袖的地址，如果服务器知道。(用于不为领袖的服务器接收到之后将正确的领袖地址返回让客户端请求) 接收者实现： 如果接收者不为领袖，回复NOT_LEADER，必要时提供提示。(也就是当该接收者知道最近的领袖是谁的时候，将领袖地址回复给客户端) 追加命令到日志，复制并提交它。 如果没有该客户端id的记录或者该客户端的sequenceNum的响应内容已经被丢弃了。 如果客户端请求的序列号sequenceNum已经被处理了，那么回复OK并且携带存储的响应。 按照顺序应用日志。 保存状态机的输出和该客户端的序列号，丢弃任何先前的回复。 携带状态机的输出并返回OK。 lab实现版本 上述论文提及的ClientRequest RPC较为复杂，我们采用简化的版本，即只保存当前最大请求的序列号，而且不提供重定向到领袖的功能。 ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:1:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"客户端实现 根据论文以及分析，我们需要实现一个含有id以及还有自增的请求序列号的客户端，当请求失败的时候，不断重试该请求。 客户端数据结构 type Clerk struct { servers []*labrpc.ClientEnd // You will have to modify this struct. lastLeader int //上一次RPC发现的主机id mu sync.Mutex //锁 clientId int64 //client唯一id lastAppliedCommandId int //Command的唯一id } 初始化 func MakeClerk(servers []*labrpc.ClientEnd) *Clerk { ck := new(Clerk) ck.servers = servers // You'll have to add code here. ck.lastLeader = 0 ck.clientId = nrand() ck.lastAppliedCommandId = 0 return ck } Get方法 当正常查询到结果(即使不存在)才会结束该次Get方法，否则继续在该序列号上重试 func (ck *Clerk) Get(key string) string { // You will have to modify this function. commandId := ck.lastAppliedCommandId + 1 args := GetArgs{ Key: key, ClientId: ck.clientId, CommandId: commandId, } DPrintf(\"client[%d]: 开始发送Get RPC;args=[%v]\\n\", ck.clientId, args) //第一个发送的目标server是上一次RPC发现的leader serverId := ck.lastLeader serverNum := len(ck.servers) for ; ; serverId = (serverId + 1) % serverNum { var reply GetReply DPrintf(\"client[%d]: 开始发送Get RPC;args=[%v]到server[%d]\\n\", ck.clientId, args, serverId) ok := ck.servers[serverId].Call(\"KVServer.Get\", \u0026args, \u0026reply) //当发送失败或者返回不是leader时,则继续到下一个server进行尝试 if !ok || reply.Err == ErrTimeout || reply.Err == ErrWrongLeader { DPrintf(\"client[%d]: 发送Get RPC;args=[%v]到server[%d]失败,ok = %v,Reply=[%v]\\n\", ck.clientId, args, serverId, ok, reply) continue } DPrintf(\"client[%d]: 发送Get RPC;args=[%v]到server[%d]成功,Reply=[%v]\\n\", ck.clientId, args, serverId, reply) //若发送成功,则更新最近发现的leader ck.lastLeader = serverId ck.lastAppliedCommandId = commandId if reply.Err == ErrNoKey { return \"\" } return reply.Value } } PutAppend方法 和Get方法同理 func (ck *Clerk) PutAppend(key string, value string, op string) { //fmt.Println(\"key=\", key, \"value=\", value, \"op=\", op) // You will have to modify this function. commandId := ck.lastAppliedCommandId + 1 args := PutAppendArgs{ Key: key, Value: value, Op: op, ClientId: ck.clientId, CommandId: commandId, } //第一个发送的目标server是上一次RPC发现的leader DPrintf(\"client[%d]: 开始发送PutAppend RPC;args=[%v]\\n\", ck.clientId, args) serverId := ck.lastLeader serverNum := len(ck.servers) for ; ; serverId = (serverId + 1) % serverNum { var reply PutAppendReply DPrintf(\"client[%d]: 开始发送PutAppend RPC;args=[%v]到server[%d]\\n\", ck.clientId, args, serverId) ok := ck.servers[serverId].Call(\"KVServer.PutAppend\", \u0026args, \u0026reply) //当发送失败或者返回不是leader时,则继续到下一个server进行尝试 if !ok || reply.Err == ErrTimeout || reply.Err == ErrWrongLeader { DPrintf(\"client[%d]: 发送PutAppend RPC;args=[%v]到server[%d]失败,ok = %v,Reply=[%v]\\n\", ck.clientId, args, serverId, ok, reply) continue } DPrintf(\"client[%d]: 发送PutAppend RPC;args=[%v]到server[%d]成功,Reply=[%v]\\n\", ck.clientId, args, serverId, reply) //若发送成功,则更新最近发现的leader以及commandId ck.lastLeader = serverId ck.lastAppliedCommandId = commandId return } } ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:2:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"kv服务层数据结构 接下来需要实现本lab最重要的模块，也就是一个kv存储服务，我们暂时不用管真正的持久化操作，只需要写入到内存即可。 首先，我们需要一个用来存放kv数据的结构，那么就使用哈希表来存放。 接下来根据上述提到的客户端交互方法，我们还需要一个数据机构来存放客户端的id和客户端请求序列号，那么也可以采用哈希表，以客户端id为key，以请求的序列号和响应为value。 还需要记录当前状态机最后应用的日志索引。 type KVServer struct { mu sync.Mutex me int rf *raft.Raft applyCh chan raft.ApplyMsg dead int32 // set by Kill() maxraftstate int // snapshot if log grows this big // Your definitions here. clientReply map[int64]CommandContext //客户端的唯一指令和响应的对应map kvDataBase KvDataBase //数据库,可自行定义和更换 storeInterface store //数据库接口 replyChMap map[int]chan ApplyNotifyMsg //某index的响应的chan lastApplied int //上一条应用的log的index,防止快照导致回退 } // ApplyNotifyMsg 可表示GetReply和PutAppendReply type ApplyNotifyMsg struct { Err Err Value string //当Put/Append请求时此处为空 //该被应用的command的term,便于RPC handler判断是否为过期请求(之前为leader并且start了,但是后来没有成功commit就变成了follower,导致一开始Start()得到的index处的命令不一定是之前的那个,所以需要拒绝掉; //或者是处于少部分分区的leader接收到命令,后来恢复分区之后,index处的log可能换成了新的leader的commit的log了 Term int } type CommandContext struct { Command int //该client的目前的commandId Reply ApplyNotifyMsg //该command的响应 } ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:3:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"接收Raft提交的日志 Raft会通过applyCh提交日志到状态机中，让状态机应用命令，因此我们需要一个不断接收applyCh中日志的协程，并应用命令。 代码实现如下： func (kv *KVServer) ReceiveApplyMsg() { for !kv.killed() { select { case applyMsg := \u003c-kv.applyCh: DPrintf(\"kvserver[%d]: 获取到applyCh中新的applyMsg=[%v]\\n\", kv.me, applyMsg) //当为合法命令时 if applyMsg.CommandValid { kv.ApplyCommand(applyMsg) } else if applyMsg.SnapshotValid { //当为合法快照时 kv.ApplySnapshot(applyMsg) } else { //非法消息 DPrintf(\"kvserver[%d]: error applyMsg from applyCh: %v\\n\", kv.me, applyMsg) } } } } 由于从Raft中接收到的日志不仅有命令日志还有快照日志，因此需要也把快照日志接收了 ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:4:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"发送Get/PutAppend请求 根据上述规则，我们可以总结出以下步骤： 当接收到Get/PutAppend请求时，先判断该客户端的该序列号的请求是否已经被正确处理并响应过了，如果有则返回之前响应的结果。 判断当前是否为领袖。(这里的是否为领袖只是自己认为自己是否是领袖，有可能因为网络分区等原因导致了自己认为自己仍然是领袖，但是实际上已经不为领袖了)。若不为领袖，则返回并告诉客户端自己不为领袖，需要去别的服务器那里请求。 将该命令传递给Raft库去进行一致性复制。 由该命令生成的日志的索引为key，创建一个value为chan的通知通道，用于接收该命令被应用过的响应消息。 在该通道上等待，并且设定超时时间，若超时则让客户端重试。 若成功接收到该命令被应用后的响应，则判断此时的响应传来的日志任期和之前传给Raft的时候生成的日志的任期是否相同。(有可能该日志并没有被commit就掉线了，之后别的服务器当选领袖，然后在该索引上提交了一条日志，后来这条日志被应用，但是这个日志就不是我们之前传给Raft的日志，也就是任期肯定不相同)若不相同，则丢弃不处理，否则正常响应给客户端。 Get请求代码 func (kv *KVServer) Get(args *GetArgs, reply *GetReply) { // Your code here. kv.mu.Lock() //defer kv.mu.Unlock() defer func() { DPrintf(\"kvserver[%d]: 返回Get RPC请求,args=[%v];Reply=[%v]\\n\", kv.me, args, reply) }() DPrintf(\"kvserver[%d]: 接收Get RPC请求,args=[%v]\\n\", kv.me, args) //1.先判断该命令是否已经被执行过了 if commandContext, ok := kv.clientReply[args.ClientId]; ok { if commandContext.Command \u003e= args.CommandId { //若当前的请求已经被执行过了,那么直接返回结果 reply.Err = commandContext.Reply.Err reply.Value = commandContext.Reply.Value kv.mu.Unlock() return } } kv.mu.Unlock() //2.若命令未被执行,那么开始生成Op并传递给raft op := Op{ CommandType: GetMethod, Key: args.Key, ClientId: args.ClientId, CommandId: args.CommandId, } index, term, isLeader := kv.rf.Start(op) //3.若不为leader则直接返回Err if !isLeader { reply.Err = ErrWrongLeader //kv.mu.Unlock() return } replyCh := make(chan ApplyNotifyMsg, 1) kv.mu.Lock() kv.replyChMap[index] = replyCh DPrintf(\"kvserver[%d]: 创建reply通道:index=[%d]\\n\", kv.me, index) kv.mu.Unlock() //4.等待应用后返回消息 select { case replyMsg := \u003c-replyCh: //当被通知时,返回结果 DPrintf(\"kvserver[%d]: 获取到通知结果,index=[%d],replyMsg: %v\\n\", kv.me, index, replyMsg) if term == replyMsg.Term { reply.Err = replyMsg.Err reply.Value = replyMsg.Value } else { reply.Err = ErrWrongLeader } case \u003c-time.After(500 * time.Millisecond): DPrintf(\"kvserver[%d]: 处理请求超时: %v\\n\", kv.me, op) reply.Err = ErrTimeout } //5.清除chan go kv.CloseChan(index) } PutAppend请求 func (kv *KVServer) PutAppend(args *PutAppendArgs, reply *PutAppendReply) { // Your code here. kv.mu.Lock() //defer kv.mu.Unlock() defer func() { DPrintf(\"kvserver[%d]: 返回PutAppend RPC请求,args=[%v];Reply=[%v]\\n\", kv.me, args, reply) }() DPrintf(\"kvserver[%d]: 接收PutAppend RPC请求,args=[%v]\\n\", kv.me, args) //1.先判断该命令是否已经被执行过了 if commandContext, ok := kv.clientReply[args.ClientId]; ok { //若该命令已被执行了,直接返回刚刚返回的结果 if commandContext.Command == args.CommandId { //若当前的请求已经被执行过了,那么直接返回结果 reply.Err = commandContext.Reply.Err DPrintf(\"kvserver[%d]: CommandId=[%d]==CommandContext.CommandId=[%d] ,直接返回: %v\\n\", kv.me, args.CommandId, commandContext.Command, reply) kv.mu.Unlock() return } } kv.mu.Unlock() //2.若命令未被执行,那么开始生成Op并传递给raft op := Op{ CommandType: CommandType(args.Op), Key: args.Key, Value: args.Value, ClientId: args.ClientId, CommandId: args.CommandId, } index, term, isLeader := kv.rf.Start(op) //3.若不为leader则直接返回Err if !isLeader { reply.Err = ErrWrongLeader return } replyCh := make(chan ApplyNotifyMsg, 1) kv.mu.Lock() kv.replyChMap[index] = replyCh DPrintf(\"kvserver[%d]: 创建reply通道:index=[%d]\\n\", kv.me, index) kv.mu.Unlock() //4.等待应用后返回消息 select { case replyMsg := \u003c-replyCh: //当被通知时,返回结果 if term == replyMsg.Term { reply.Err = replyMsg.Err } else { reply.Err = ErrWrongLeader } case \u003c-time.After(500 * time.Millisecond): //超时,返回结果,但是不更新Command -\u003e Reply reply.Err = ErrTimeout DPrintf(\"kvserver[%d]: 处理请求超时: %v\\n\", kv.me, op) } go kv.CloseChan(index) } ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:5:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"应用日志中的命令 我们之前开启了一个协程不断从applyCh中接收提交的日志以及快照，那么我们需要对这些日志和快照进行处理。首先，我们需要处理命令，由于这时候有Get/Put/Append三种命令，因此我们需要分别处理。我们可以对其的实际存储操作就行封装，就留三个操作接口即可。 应用流程如下： 接收到日志，先判断是否已经应用过了。应用过则不处理，等待处理请求的协程超时返回。 判断命令类型，做相应处理。 若需要通知，则通知处理请求的协程。 更新clientReply，用于去重。 判断是否需要快照。(后续会提到这个流程) func (kv *KVServer) ApplyCommand(applyMsg raft.ApplyMsg) { kv.mu.Lock() defer kv.mu.Unlock() var commonReply ApplyNotifyMsg op := applyMsg.Command.(Op) index := applyMsg.CommandIndex //当命令已经被应用过了 if commandContext, ok := kv.clientReply[op.ClientId]; ok \u0026\u0026 commandContext.Command \u003e= op.CommandId { DPrintf(\"kvserver[%d]: 该命令已被应用过,applyMsg: %v, commandContext: %v\\n\", kv.me, applyMsg, commandContext) commonReply = commandContext.Reply return } //当命令未被应用过 if op.CommandType == GetMethod { //Get请求时 if value, ok := kv.storeInterface.Get(op.Key); ok { //有该数据时 commonReply = ApplyNotifyMsg{OK, value, applyMsg.CommandTerm} } else { //当没有数据时 commonReply = ApplyNotifyMsg{ErrNoKey, value, applyMsg.CommandTerm} } } else if op.CommandType == PutMethod { //Put请求时 value := kv.storeInterface.Put(op.Key, op.Value) commonReply = ApplyNotifyMsg{OK, value, applyMsg.CommandTerm} } else if op.CommandType == AppendMethod { //Append请求时 newValue := kv.storeInterface.Append(op.Key, op.Value) commonReply = ApplyNotifyMsg{OK, newValue, applyMsg.CommandTerm} } //通知handler去响应请求 if replyCh, ok := kv.replyChMap[index]; ok { DPrintf(\"kvserver[%d]: applyMsg: %v处理完成,通知index = [%d]的channel\\n\", kv.me, applyMsg, index) replyCh \u003c- commonReply DPrintf(\"kvserver[%d]: applyMsg: %v处理完成,通知完成index = [%d]的channel\\n\", kv.me, applyMsg, index) } value, _ := kv.storeInterface.Get(op.Key) DPrintf(\"kvserver[%d]: 此时key=[%v],value=[%v]\\n\", kv.me, op.Key, value) //更新clientReply kv.clientReply[op.ClientId] = CommandContext{op.CommandId, commonReply} DPrintf(\"kvserver[%d]: 更新ClientId=[%d],CommandId=[%d],Reply=[%v]\\n\", kv.me, op.ClientId, op.CommandId, commonReply) kv.lastApplied = applyMsg.CommandIndex //判断是否需要快照 if kv.needSnapshot() { kv.startSnapshot(applyMsg.CommandIndex) } } ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:6:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"主动进行快照 上述我们已经实现了一个没有快照的kv服务层，但是我们的日志和数据不能无限制的增长下去，而且为了快速同步落后很多的节点，我们在之前在Raft层已经实现了部分快照的功能，在当前模块，我们需要对是否进行快照进行判断，并且通知Raft去执行快照操作。 我们首先需要完成的是主动进行快照，也就是当状态机接收到新的提交的日志之后，可以判断一下当前Raft必须状态的大小，如果接近于我们设定的阈值或者超过该阈值，则可以主动进行快照。 主动进行快照，就是先将自己的状态机必要状态(必须要同步的数据)编码并生成状态机快照数据，然后通知Raft进行快照，并且将状态机快照数据传递给它。 判断是否需要快照 //判断当前是否需要进行snapshot(90%则需要快照) func (kv *KVServer) needSnapshot() bool { if kv.maxraftstate == -1 { return false } var proportion float32 proportion = float32(kv.rf.GetRaftStateSize() / kv.maxraftstate) return proportion \u003e 0.9 } 创建快照 //生成server的状态的snapshot func (kv *KVServer) createSnapshot() []byte { w := new(bytes.Buffer) e := labgob.NewEncoder(w) //编码kv数据 err := e.Encode(kv.kvDataBase) if err != nil { log.Fatalf(\"kvserver[%d]: encode kvData error: %v\\n\", kv.me, err) } //编码clientReply(为了去重) err = e.Encode(kv.clientReply) if err != nil { log.Fatalf(\"kvserver[%d]: encode clientReply error: %v\\n\", kv.me, err) } snapshotData := w.Bytes() return snapshotData } 状态机进行快照 //主动开始snapshot(由leader在maxRaftState不为-1,而且目前接近阈值的时候调用) func (kv *KVServer) startSnapshot(index int) { DPrintf(\"kvserver[%d]: 容量接近阈值,进行快照,rateStateSize=[%d],maxRaftState=[%d]\\n\", kv.me, kv.rf.GetRaftStateSize(), kv.maxraftstate) snapshot := kv.createSnapshot() DPrintf(\"kvserver[%d]: 完成service层快照\\n\", kv.me) //通知Raft进行快照 go kv.rf.Snapshot(index, snapshot) } 调用Raft的Snapshot方法来通知Raft进行快照 ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:7:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"被动进行快照 当Raft接收到领袖的InstallSnapshot RPC时，会先将快照命令通过applyCh传递给状态机，然后由状态机来处理，它会对旧的快照进行丢弃，避免回退，然后通知Raft去进行快照，如果Raft快照成功，则状态机再应用该快照的数据到状态机中。 代码如下： // ApplySnapshot 被动应用snapshot func (kv *KVServer) ApplySnapshot(msg raft.ApplyMsg) { kv.mu.Lock() defer kv.mu.Unlock() DPrintf(\"kvserver[%d]: 接收到leader的快照\\n\", kv.me) if msg.SnapshotIndex \u003c kv.lastApplied { DPrintf(\"kvserver[%d]: 接收到旧的日志,snapshotIndex=[%d],状态机的lastApplied=[%d]\\n\", kv.me, msg.SnapshotIndex, kv.lastApplied) return } if kv.rf.CondInstallSnapshot(msg.SnapshotTerm, msg.SnapshotIndex, msg.Snapshot) { kv.lastApplied = msg.SnapshotIndex //将快照中的service层数据进行加载 kv.readSnapshot(msg.Snapshot) DPrintf(\"kvserver[%d]: 完成service层快照\\n\", kv.me) } } 如果是被动进行快照，那么还需要对领袖传来的快照数据进行读取。 读取快照代码如下： func (kv *KVServer) readSnapshot(snapshot []byte) { if snapshot == nil || len(snapshot) \u003c 1 { return } r := bytes.NewBuffer(snapshot) d := labgob.NewDecoder(r) var kvDataBase KvDataBase var clientReply map[int64]CommandContext if d.Decode(\u0026kvDataBase) != nil || d.Decode(\u0026clientReply) != nil { DPrintf(\"kvserver[%d]: decode error\\n\", kv.me) } else { kv.kvDataBase = kvDataBase kv.clientReply = clientReply kv.storeInterface = \u0026kvDataBase } } ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:8:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"总结 这个lab是基于上一个lab的，其实做到这里，就可以改成一个分布式kv数据库并部署应用了。我自己稍微改一下做了个项目：bluedis(没有做很多功能，只是基本实现了一些读写操作，后续有时间会继续进行这个项目)。 Lab4的文档后续有时间会继续写，若对于内容有问题的欢迎联系我一起交流讨论！ ","date":"2022-03-21","objectID":"/mit-6.824-lab3/:9:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab3","uri":"/mit-6.824-lab3/"},{"categories":["分布式系统"],"content":"MIT6.824 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:0:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab1 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:1:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Rules 最后文件需要输出nReduce个，文件名格式为mr-out-X 输出到文件的格式在mrsequential.go中 只用写worker.go/coordinator.go/rpc.go这三个文件 worker将中间文件输出到当前文件夹下，之后worker执行reduce任务的时候从中取 需要实现coordinator.go中的Done()方法，当全部任务被执行完了之后返回true，然后mrcoordinator退出 当所有的任务的完成的时候，worker也应该停止。简单的方法就是使用rpc的回调，当返回err，也就是故障的时候，这里可以理解为coordinator已经结束了，所以这时候worker可以退出了。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:1:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Hints 一开始可以先实现worker的worker方法来和coordinator进行rpc调用来获取任务，coordinator回应给他文件名作为一个还未开始的map任务。然后worker读取这些文件然后调用map方法，参考mrsequential.go中的方法 map和reduce方法是作为插件使用的，记得启动的时候带上参数wc.so 没有wc.so插件就build一个，go build -race -buildmode=plugin ../mrapps/wc.go 中间文件可以命名为mr-X-Y，X表示map任务编号，Y表示reduce任务编号。 中间文件使用json方法来存来读取 map的worker使用ihash(key)方法来获取reduce编号，靠这个存到对应的中间文件中 coordinator作为一个rpc服务端，需要对共享资源进行并发保护 run的时候使用-race检查一下 只有所有的map任务被执行完了之后才能进行reduce任务的分配。一种方法是worker周期的请求coordinator来获取任务，没获取到就sleep一会再来请求。另一种方法是每个rpc的handler可以循环等待一下，使用time.Sleep或者sync.Cond coordinator不能可靠的区别那些故障worker，包括那些执行的太慢的节点。最好就是每次分配了任务之后可以等待10秒，如果10秒都没有完成，就可以认为该worker已故障了。需要重新将该任务分配给别的worker。 测试故障恢复可以使用mrapps/crash.go插件，会随机在map和reduce中故障 为了不让已经故障的节点的产生文件对作为真正的中间文件，可以使用论文中提到的临时文件的方法，worker写的使用可以使用临时文件，使用ioutil.TempFile来创建临时文件，然后完成之后使用os.rename来原子性的命名。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:1:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab2 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:2:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab2a Hints 不可以直接跑你的Raft实现的代码；使用go test -run 2A -race。 根据论文的图二。这个部分只需要你来关注发送和接受RequestVote RPCs、那些关系到选举的服务器规则，以及那些关系到选举的状态。 为Raft增加领袖选举的相关状态在raft.go中，同时你也需要定义一个结构关于每个log entry。 完善RequestVoteArgs和RequestVoteReply结构。修改Make()来创建一个后台协程来开始周期性的发送RequestVote来进行领袖选举当没有在超时时间内接受到别的节点的心跳。通过这个方式，一个节点会知道谁是领袖(如果已经有领袖的情况)，或者开启选举让自己成为领袖。实现RequestVote()来进行投票。 实现心跳机制，定义一个AppendEntries的RPC struct，然后让领袖周期性的发送该请求。写一个AppendEntries的RPC方法来重置选举时间，这样服务器就不会在已经当选领袖情况下再次成为被选举。 确保所有的选举不会总是同时开始。 测试要求领袖发送心跳检测不应该超过每秒十次。 测试要求你的Raft在五秒内选举出一个新的领袖当旧的领袖已经失败的时候。选举可能会进行多轮，为了防止分裂投票(当包丢失或者候选人不幸选择了相同的随机退出时间)必须选择足够短的的超时。 论文的第5.2节提到选举超时的范围是150到300毫秒。这个范围只有在领导者发送心跳频率大大超过每150毫秒一次的情况下才有意义。因为测试者限制你每秒10次心跳，你必须使用比报纸上150到300毫秒更大的选举超时，但不能太大，因为那样你可能无法在5秒内选出领导者。 您需要编写定期执行操作或在时间延迟之后执行操作的代码。最简单的方法是创建一个 goroutine，循环调用time.Sleep()。 记得完成Getstate() 测试会调用你的rf.Kill()当它永久的关闭一个实例时，你可以使用rf.Kill()来检查Killed()是否已经被调用。最好每个循环都来判断一下。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:2:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab2b Hints 实现ledaer和follower的代码去增加新的日志条目，通过go test -run 2B -race来检验正确。 首要目标是通过TestBasicAgree2B。从实现Start()方法开始，然后编写代码去发送和接收新的日志条目通过AppendEntries，以及参考图二(也就是那个经典的图)。 需要实现选举限制(在论文的5.4.1) 在早期的2b 测试中，不能达成一致意见的一个原因是，领导人还活着的情况下，依然重复进行选举。在选举计时器管理中寻找漏洞，或者没有在赢得选举后立即发送心跳。 您的代码可能具有重复检查某些事件的循环。不要让这些循环不停顿地连续执行，因为这会使实现变慢，以至于测试失败。使用 Go 的条件变量，或者使用在每个循环的时候进行10ms睡眠。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:2:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab2c 一个真正的实现会在每次raft改变的时候讲raft的持久化状态写入到磁盘中，以及当重新启动后在重启期间从磁盘中读取状态。你的实现不会真正的使用磁盘；而是，你会保存和恢复持久化状态从一个Persister对象(在persister.go中)。无论谁调用Raft.Make()都会提供一个拥有最近的初始化状态的Persister。Raft应该从这个Persister来初始化这些状态，以及应该使用这个Persister来每次保存raft的持久化状态每当状态改变的时候。使用Persister的ReadRaftState()和SaveRaftState()方法。 Task1 完成函数persist()和readPersist()通过增加代码来保存和恢复持久化状态。你需要编码(或者说“初始化”)这些状态以byte数组的形式为了能将其传递给Persister。使用labgob编码器；看一些那些persist()和readPersist()的注释。labgob就像Go语言的gob编码器，但是会打印错误信息当你尝试编码一些使用小写字段名的结构体。 Task2 在你的实现里那些改变了持久化状态的地方插入对persist()的调用。一旦你完成了，你应该会通过剩下的这些test。 Hints 2C的许多test涉及到server失效和网络丢失rpc的请求和回复，这些事件是不确定的，可能你会很幸运的通过这些test，即使的代码还有bug。通常需要你多次运行test来暴露出这些bug。 你可能需要优化你的nextIndex的备份通过一次备份多个条目。看一下论文的第七页末尾和第八页开头的灰色方框内的字。但是论文对细节含糊其辞，你需要借助6.824的Raft课程来完善。 2C只要求你实现持久化(persistense)和日志快速恢复(fast log backtracking)，2C的test可能会因为之前的部分导致失败。即使你通过了2A和2B的代码，但是你可能还是会有选举和日志的bug在2C的test中被暴露。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:2:3","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab2d 为了支持快照，我们需要在service层和Raft库之间的接口。Raft论文没有指定这个接口，因此很多设计都是可以的。为了一个简单的实现，我们决定使用接下来的接口： Snapshot(index int, snapshot []byte) CondInstallSnapshot(lastIncludedTerm int, lastIncludedIndex int, snapshot []byte) bool service层调用Snapshot()将其状态传递给Raft。快照包含所有的信息以及index。这意味着相应的Raft节点不再需要日志。你的Raft实现应该尽可能的调整log。你必须修改你的Raft代码来只操作末尾的log。 正如Raft论文中讨论的那样，Raft的leaders必须通过安装快照的方式来告诉落后的Raft节点来更新它的的状态。你需要实现InstallSnapshot的RPC发送和处理为了安装快照。这和AppendEntries形成对比，AppendEntries发送日志条目然后一个个被service应用。 InstallSnapshot的RPC是在Raft节点间调用，而提供的框架函数Snapshot/CondInstallSnapshot是service用来和Raft通信的。 当follower接受并处理InstallSnapshot的RPC请求时，它必须使用Raft将包含的快照交给service。InstallSnapshot处理程序可以使用applyCh来发送快照，通过将快照放入到ApplyMsg中。service从applyCh中读取，然后和快照调用CondInstallSnapshot来告诉Raft：service正在切换成传入快照状态，然后Raft应该同时更新自己的日志。(参见config.go中的applierSnap()来查看tester服务是怎么做这个的)。 CondInstallSnapshot应该拒绝安装旧的快照(如果Raft在lastIncludedTerm/lastIncludedIndex之后处理了日志条目)。这个因为Raft可能在处理InstallSnapshotRPC之后和在service调用CondInstallSnapshot之前处理了其他的RPC并在applyCh上发消息了。不允许Raft回到旧的快照，因此必须拒绝旧的快照。当你的实现拒绝快照时，CondInstallSnapshot应该直接返回false使得service知道现在不应该切换这个快照。 如果快照是最近的，Raft应该调整日志，持久化新的状态，返回true，以及service应该切换到这个快照在处理下一个消息到applyCh之前。 CondInstallSnapshot是一种更新Raft和service的状态的方式；其他接口也是可以的。这个特殊的设计允许你的实现检查一个快照是否必须安装在一个地方，并且原子性地将service和Raft切换到快照。你可以自由地以CondInstallSnapshot始终可以返回true的方式来实现你的Raft。 Task 修改你的Raft代码来支持快照；实现SnapShot，CondInstallSnapshot以及InstallSnapshot的RPC。以及对Raft的更改来支持这些(例如：继续使用修剪后的日志)你的解决方案通过2D测试和所有的Lab2测试的时候，它就完成了。 Hints 在一个InstallSnapshot中发送整个快照。不要实现图13的分割快照的offset机制。 Raft必须丢弃旧的日志以允许Go的垃圾回收器来使用和重用内存；这要求对于丢弃的日志没有可达的引用(指针)。 Raft的日志不再使用日志条目的位置或日志长度的方式来确定日志的条目索引；您需要使用独立于日志位置的索引方案。 即使日志被修剪，你的实现仍需要在AppendEntries正确的发送日志的Term和Index；这可能需要保存和引用最新的快照的lastIncludedTerm/lastIncludedIndex(考虑是否应该持久化) Raft必须存储每一个快照到persiser中，通过SaveStateAndSnapshot()。 对于整套的Lab2测试，合理的时间消耗应该是8分钟的实际时间和1.5分钟的CPU时间。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:2:4","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab3 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:3:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Introduction 在这个lab中，你将构建一个基于lab的Raft库实现的容错性kv存储服务。你的kv服务将是一个复制的状态机，由几个使用Raft进行复制的kv服务器组成。你的kv服务应该在当大多数服务器处于活动状态并且可以通信的时候继续处理客户机请求，即使出现其他故障或者网络分区。lab3之后，你将实现raft_diagram中的所有模块(Clerk，Service and Raft)。 这个服务支持三个操作：Put(key, value)，Append(key, arg)以及Get(key)。它维护一个简单的kv键值对数据库。键值对都是字符串。Put()替换数据库中特定键的值，Append(key, arg)将arg附加到key的值上，以及Get()获取当前键的值。Get()一个不存在的key将返回一个空字符串。Append()一个不存在的key就和Put()效果相同。每一个客户端和服务用一个Clerk的Put/Append/Get方法来交互。一个Clerk管理和服务器的RPC交互。 你的服务必须为Clerk的Get/Put/Append方法提供强一致性。以下是我们认为的强一致性。如果一次调用一个，那么Get/Put/Append方法应该像系统只有一个副本那样工作，每次调用都应该观察前面的调用序列所暗示的对状态的修改。对于并发调用，返回值和最终状态必须相同，就像操作以某种顺序一次执行一个操作一样。如果调用在时间上有重叠，例如，客户端X调用了Clerk.Put()，然后客户端Y调用Clerk.Append()，然后X的调用返回结果了。此外，一次调用必须观察在调用开始之前完成的所有调用的效果(所以我们在技术上要求线性化)。 强一致性对于应用程序来说很方便，因此这非正式地意味着，所有客户端看到的状态都是一样的并且看到了都是最新的状态。对于单机来说，提供强一致性相对容易。但是如果服务是有副本的，那就困难了。因为所有的服务器必须为并发请求选择相同的执行顺序，并且必须避免使用不是最新的状态来回复客户端。 这个lab有两个部分。在A部分，你将实现一个不用担心Raft的log会无限增长的服务。在B部分，你将实现一个快照(论文第7节)，也就是让Raft丢弃旧的log。 你应该重新阅读Raft论文，特别是第7和第8节。广阔来看，你可以看一下Chubby、Paxos Made Live、Spanner、Zookeeper、Harp、Viewstamped Replication以及Bolosky et al。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:3:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Getting Started 我们在src/kvraft中提供框架代码和测试。你需要修改kvraft/client.go，kvraft/server.go以及可能也要kvraft/common.go ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:3:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab3a 一个没有快照的kv服务 每一个你的kv服务器都需要有一个与之关联的Raft节点。Clerks发送Put()，Append()和Get()的RPC请求到当前Raft中leader的那个kvserver上。kvserver将Put/Append/Get操作提交给Raft，以便Raft保存一系列Put/Append/Get操作。所有的kvserver从Raft的log中按顺序执行操作，并将这些操作应用到他们的kv数据库上。这样做的目的是在所有的服务器上维护相同的kv数据库副本。 一个Clerk有时候不知道哪一个kvserver是Raft的leader。如果一个Clerk发送一个RPC请求到错误的kvserver上，或者没有到达kvserver上；Clerk应该通过发送到不同的kvserver上来进行重试。如果一个kv服务将操作提交到它的Raft日志中(并将操作用用到kv状态机上)，那么leader将通过RPC来回应Clerk。如果操作没有成功提交(比如leader被更替了)，这个服务器将报告error，然后Clerk将在不同的服务器上重试。 你的kvservers不应该直接进行通信；它们应该只通过他们的Raft来进行通信。 Task1 你的首要任务是实现一个当没有消息丢失和服务器失败的解决方案。 你将需要增加client.go中关于Clerk的Put/Append/Get方法的RPC发送代码，以及实现server.go中的PutAppend()和Get()的RPC处理程序。这些处理程序应该使用Start()来在Raft日志中输入一个Op；你应该完善server.go里面的Op的结构定义以至于它可以描述一个Put/Append/Get操作。每一个服务器应该在Raft提交Op命令时执行这些命令，即当它们出现在applyCh时。RPC处理程序应该在Raft提交其Op时发出通知，然后回复RPC。 当你可靠的通过测试中的第一个测试：“One Client\"时，你就完成了这个任务。 Hints1 调用Start()之后，你的kvservers将需要等待Raft去完成agreement。已经达成一致的命令道道applyCh。你的代码需要保持读取applyCh当PutAppend()和Get()处理程序使用Start()提交了命令到Raft日志中的时候。注意kvserver和Raft库之间的死锁问题。 你可以向ApplyMsg和RPC处理程序比如说AppendEntries中添加字段。但是对于大多数实现都是不需要的。 一个kvserver不应该完成一个Get()的RPC请求当它不是大多数的那一部分。一个简单的解决方案就是在Raft日志中输入每一个Get()(以及每一个Put()和Append())。你不必实现论文章第八节描述的只读操作的优化。 最好一开始就加锁因为避免死锁有时候会影响整个的代码设计。使用go test -race来检查的你的代码是不是race-free。 现在你需要修改你的解决方案，以便在网络和服务器出现故障时继续执行。您将面临的一个问题是，Clerk可能不得不的发送多次RPC直到它找到了一个积极响应的kvserver时。如果一个leader在向Raft日志中提交了一个条目之后失败了，Clerk可能不会收到回应，因此可能需要重发请求到别的leader那里。每次Clerk.Put()或者Clerk.Append()的调用应该最终只执行一次，因此你必须确保重发不会导致服务器执行两次请求。 Task2 添加代码来处理失败，并处理重复的Clerk请求，包括这样的情况：Clerk在一个任期内发送请求到kvserver的leader，等待回复超时，然后再另一个任期重新向新领导发送请求。请求应该只执行一次。您的代码应该能够通过 go test -run 3A -race。 Hints2 你的解决方案需要处理一个领导者，该领导者已经为Clerk的RPC请求调用了Start()，但是在将请求提交到日志之前失去了领导权。在这种情况，你应该安排Clerk去重发请求到其他服务器直到发到了新的leader那里。这样做的一个方法是，服务器通过注意Start()返回的索引出现了不同的请求，或者Raft的任期已经改变，来检测它是否失去了领导权。如果当前领导者自己分区，那么它不会知道新的领导者；但是同一分区的任何客户端也不能和新领导者交互；所以在这种情况下，服务器和客户端可以无限期的等待，直到分区恢复。 你可能必须修改您的Clerk，以便记住最后一个RPC的主机是哪个服务器，并首先将下一个RPC发送到该服务器。这将避免浪费时间在通过RPC来找领导者上，这可能会帮助你快速的通过一些测试。 你将需要唯一地标识客户端的操作，以确保kv服务只执行每个操作一次。 你的重复检测方案应该能够快速的释放服务器内存，例如，让暗示每一个RPC都看到了前一个RPC的回复。假设客户端只在Clerk调用一次是可以的。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:3:3","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab3b 使用快照的kv服务 就目前的代码而言，重启服务器将重新加载完整的Raft日志来恢复其状态。然而，对于一个长时间运行的服务器而言，永远保存完整的Raft日志是不现实的。相反，您将修改kvserver来和Raft协作来使用lab2d中Raft的Snapshot()和CondInstallSnapshot()来节省空间。 tester传递maxraftstate到您的StartKVServer()。maxraftstate以字节表示Raft的持久性状态允许的最大大小(包括日志，但是不包括快照)。你应该将maxraftstate和persister.RaftStateSize()比较。每当你的kv服务器检测到Raft状态大小接近这个阈值，它应该使用Snapshot()来保存一个快照，Snapshot()又使用persister.SaveRaftState()。如果maxraftstate是-1，你不需要进行快照。maxraftstate应用于你的Raft传递给persister.SaveRaftState()的gob编码字节。 Task 修改你的kvserver以便于它检测何时Raft的持久性状态变得太大，然后将快照交给Raft。当kvserver重新启动时，它应该从persister中读取快照，并从快照恢复其状态。 Hints 考虑下kvserver何时应该对其状态进行快照，以及快照中应该包含哪些内容。Raft使用SaveStateAndSnapshot()将每个快照以及响应的Raft状态存储在持久化对象中。你可以使用ReadSnapshot()来读取最新的快照。 你的kvserver必须能够检测到日志中跨检查点的重复操作，因此用于检测它们的任何状态都必须包含在快照中。 将快照中存储的所有结构字段大写。 您的Raft库可能在该lab中暴露出bug。如果您更改了Raft实现，请确保它能继续通过所有的lab2测试。 lab3测试的合理时间是400s的实时时间以及700秒的CPU时间。此外，运行go test -run TestSnapshotSize应该少于20s的实时时间。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:3:4","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab4 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:4:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Introduction 在这个实验中，你将构建一个kv存储系统，其中\"分片\"或分区是一组副本上的键。分片是kv键值对的子集；例如，所有以a开头的键可能是一个分片，所有以b开头的键可能是另一个分片，等等。之所以进行切分是因为性能。每一个副本组只管理几个分片的put和get，并且组之间并行操作；因此总的系统吞吐量(单位时间的put和get)与组的数量成比例增加。 您的kv存储区有两个主要组件。首先，一组副本组。每一个副本组负责分片的一个子集，一个副本由少数服务器组成，这些服务器使用Raft去复制组的分片。第二个组件是\"分片控制器”。分片控制器决定那个副本组应该服务于每个分片；这些信息称之为配置。配置随着时间而变化。客户端查询分片控制器以查找该键的副本组，而副本组查询控制器以查找需要服务的分片。整个系统只有一个分片控制器，使用Raft实现容错服务。 一个分片存储系统必须能够在副本组之间移动分片。原因之一是某些组可能比其他组的负载大，因此需要移动分片来平衡负载。另一个原因是副本组可能会加入和离开系统：可能会增加新的副本组来增加容量，或者现有的副本组可能离线修复或者退休。 该lab的一个主要挑战是处理重新配置——在分配分片到组的变化。在单个副本组中，所有的组成员必须在当重新配置关系到客户端的Put/Append/Get请求的时候达成一致。例如，一个Put请求可能与导致副本组停止对保存的Put键的分片负责的重新配置通知到达。组内的所有副本必须就Put是在重新配置之前还是之后发生达成一致。如果是在之前，Put应该生效，分片的新所有者应该看到其效果；如果之后，Put请求不应该生效以及客户端必须在新的所有者处进行重试。建议的方法是去让每个副本组使用Raft去记录Put/Append/Get的顺序以及重新配置的顺序。你将需要确保在任何时候最多只有一个副本组为每一个分片服务。 重新配置也需要副本组之间的交互。例如，在配置10 中的组G1可能负责分片S1，配置11中的组G2可能需要负责分片S1。在10到11的重新配置过程中，G1和G2必须使用RPC将分片S1(键值对内容)从G1移动到G2。 Note 只有RPC可以用于客户端和服务器之间的交互。例如，不允许服务器的不同实例共享Go变量或者文件。 Note lab中使用\"配置\"来指代分片指向副本组。这和Raft集群成员身份的更改不同。你不必实现Raft集群成员变更。 这个lab的总体架构(一个配置服务和一组副本组)遵循和Flat Datacenter Storage，BigTable，Spanner，FAWN，Apache HBase，Rosehud，Spinnaker等等相同的总体模式。这些系统在许多细节上和lab不同，而且也通常更复杂和有能力。例如，lab没有改进每个Raft组中的对等点集合；它的数据和查询模型十分简单，分片的切换速度很慢，不允许并发客户端访问。 Note 你的lab4的分片服务器，lab4分片控制器以及lab3 kvraft必须都使用相同的Raft实现。我们将重新运行lab2和lab3的测试，你在旧测试中的分数将计入你的lab4总分。这些测试在你的lab4的总分中占了10分。 我们建议你使用src/shardctrler和src/shardkv的骨架代码和测试。 当你完成了之后，你的实现应该通过所有的src/shardctrler目录下的测试，还有src/shardkv下的所有。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:4:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab4a 分片控制器(30分) 首先你需要实现分片控制器，在shardctrler/server.go和client.go中。当你完成的时候，你应该完成shardctrler目录下的所有测试。 shardctrler管理一系列编号的配置。每个配置描述一组副本组以及分片到副本组的分配。每当这个分配需要更改时，分片控制器创建一个有着新的分配的新配置。kv客户端以及服务器如果想知道当前(或者过去)的配置，请和sharfctrler联系。 你的实现必须支持shardctrler/common.go中描述的RPC接口，该接口由Join，Leave，Move以及QueryRPC组成。这些RPC用于允许管理员(以及测试)去控制shardctrler：去增加新的副本组、消除副本组以及在副本组之间移动分片。 JoinRPC由管理员调用来增加新的副本组。它的参数是一组唯一、非零的副本组的标识符(GID)到服务器列表的映射。Shardctrler应该通过创建新的包括了新副本组的新配置来进行响应。新的配置应该在所有组中尽可能平均地分配分片。并尽可能少地移动碎片来实现这一目标。如果有GID不是当前配置的一部分，那么shardctrler应该允许重用它(例如，允许GID加入，然后离开，然后再加入)。 LeaveRPC的参数之前加入的副本组的GID列表。shardctrler应该创建一个不包含这些组的配置，并将这些组的分片分配给其他的组。新的配置应该尽可能地在组之间平均分配分片，并且应该尽可能减少移动分片来实现这一目标。 MoveRPC的参数是一个分片的编号和GID。shardctrler应该创建一个新的配置，将分片分配给副本组。Move的目的是允许我们测试你的软件。Move后的Join和Leave可能会取消Move，因为Join和Leave会进行重新平衡。 Query的RPC参数是一个配置编号。shardctrler用具有该数字的配置来回复。如果编号是-1或者大于已知的最大配置编号，那么应该回复最新的一个配置。Query(-1)的结果应该反应每一个在收到Query(-1)之前shardctrler已经完成的Join，Leave或者MoveRPC。 第一个配置的编号应该为0。它应该不包含任何组，以及所有的分片都应该分配给GID0(一个无效的GID)。下一个配置(为响应JoinRPC而创建)应该是编号1。通常会有明显多于组数的分片(每一个组应该服务超过一个分片)，以便能够以相当细的粒度转移负载。 Task 你的任务是去实现在shardctrler/目录下的client.go和server.go中指定的接口。你的shardctrler必须是容错的，使用lab2/3中的Raft库。注意，我们将重新运行lab2和lab3当给lab4评级时，所以请确保你不会引入bug到Raft实现中。当你通过所有shardctrler中的所有测试时，你就完成了这项任务。 Hints 从一个精简的kvraft服务器副本开始。 你应该实现对分片控制器的rpc的重复客户端请求检测。shardctrler测试不会对此进行测试，但是shardkv测试之后会在不可靠的网络上使用shardctrler；如果你的shardctrler没有过滤掉重复的rpc请求，你可能很难通过shardkv的测试。 执行分片重新平衡的状态机中的代码需要具有确定性。在Go中，map的迭代顺序不确定。 Go中的map是引用的。如果您将一个一个map类型的变量分配给了另一个变量，那么两个变量将引用同一个map。因此，如果希望基于以前的配置创建一个新的Config，你需要创建一个新的map对象(使用make())以及分别复制键值。 Go race探针(go test -race)将帮助你找到Bug。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:4:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Lab4b 分片kv服务器 现在你将建立一个分片kv系统，一个基于分片的容错kv存储系统。你将修改你的Shardkv/client.go，shardkv/common.go以及shardkv/server.go。 每一个分片kv服务器作为复制组的一部分进行操作。每一个副本组为一些kv键值对分片提供Get，Put以及Append操作。使用cleint.go中的key2shard()方法来查找这个键是属于哪一个分片的。多个副本组合作为整套分片服务。一个shardctrler服务实例将分片分配给副本组；当这个分配发生变化时，副本组必须交出分片，同时确实客户端不会看到不一致的响应。 你的存储系统必须为使用其客户端接口的应用提供一个线性化接口。也就是，已完成的应用调用shardkv/client.go中的Clerk.Get()，Clerk.Put()和Clerk.Append()方法必须表现出以相同的顺序影响了所有的副本。Clerk.Get()应该看到由最近的Put/Append写到的同一个键的值。即使在作为配置更改时的Get和Put。 你的每一个分片只有在该分片的Raft副本组中大多数服务器存活并且可以互相通信的时候，才可以取得进展，并且它们可以和大多数的shardctrler服务器通信。你的实现必须进行操作(服务请求以及可以按照需要进行重新配置)即使在一些副本组中的少部分服务器死机、暂不可用或者太慢的时候。 一个分片kv服务器只是一个单副本组的成员。给定副本组中的服务器永远不会更改。 我们向你提供client.go的代码，该代码将每个RPC发送到负责该key的副本组。如果副本组说它不为该key服务，那么客户端进行重试；在这种情况下，客户端代码向分片控制器获取最新的配置然后重试。你必须修改client.go作为处理重复客户端RPC的支持的一部分，就像在kvraft的lab中一样。 当你完成你的代码时你应该通过所有shardkv的测试除了challenge测试。 Note 你的服务器不应该调用分片控制器的Join()请求。测试会在合适的时候调用Join() Task1 你的第一个任务是去通过第一个shardkv测试，在这个测试中，只有一个分片分配，所以您的代码应该和lab3中的服务器代码非常相似。最大的修改将是让您的服务器检测什么时候发生配置，并开始接收匹配当前分片的key的请求。 现在你的解决方案适用于静态分片的情况，那么现在就开始处理配置更改问题了。您需要让服务器监视配置更改，并在检测到更改时启动分片迁移过程。如果一个复制组丢失了一个分片，他必须立即停止对该分片中的key的请求，并开始将该分片的数据迁移到接管所有者的复制组中。 Task2 在配置更改期间实现分片迁移。确保复制组中所有服务器在它们执行的操作操作序列的相同点执行迁移，以便它们都接受或拒绝并发客户端请求。在处理后续测试之前，你应该集中经历通过第二个测试(“join then leave”)。当你完成所有的测试除了TestDelete。就完成了这个任务了。 Note 你的服务器需要定期轮询分片控制器以获取最新的配置。测试期望您的代码大约每100ms轮询一次；更高频率是可以的，但是更少频率可能会导致问题。 Note 服务器需要互相发送RPC以便能够在配置更改期间传递分片。分片控制器的Config结构体包括了服务器的名字，但是你需要labrpc.ClientEnd来发送RPC。你应该使用传递给StartServer()的make_end()函数来将一个服务器的名字转化为CleintEnd。shardkv/client.go包含这样的代码。 Hints 向server.go中添加代码，定期向分片控制器中获取最新的配置，如果接收组不为客户端的key所属的分片负责，则添加代码来拒绝客户端请求。你应该通过第一个测试。 你的服务器使用用一个ErrWrongGroup的错误来响应客户端的请求当请求的key不是该server应该服务的时候(即一个没有分配给该副本组的分片的Key)。确保你的Get，Put以及Append处理程序能够在并发的重新配置下能够正确的做出决定。 按顺序一次只进行一个重新配置。 如果测试失败，检查gob错误(例如：“gob”: type not registered for interface…)。Go不认为gob错误是致命的，尽管它们在lab中是fetal。 考虑一下shardkv客户端和服务端应该如何处理ErrWrongGroup。如果客户端接收到ErrWrongGroup，它是否应该更改序列号？如果服务器在执行Get/Put请求时返回了ErrWrongGroup，那么它是否应该更新客户端状态？ 在服务器迁移到新的配置之后，继续存储它不再拥有的分片是可以接受的(尽管在真正系统这会让人遗憾)。这可能可以帮助简化服务器的实现。 在配置更改期间G1组需要G2组的分片时，在处理日志条目的G2什么时候将分片发给G1是有关系的吗？ 你可以在RPC的请求或响应中发送整个映射，这可能会有助于简化分片传输的代码。 如果您的RPC处理程序之一在其应答中包含了作为服务器状态的一部分映射(比如kv键值对)，那么你有可能由于race而出bug。RPC系统必须读取到map以便将其发送给调用方，但它没有持续覆盖map的锁。你的服务器，会在RPC系统读取map的时候继续修改该map。解决方案是在RPC处理程序中回复该映射的副本。 如果你在Raft日志条目中放置了一个map或者slice，而你的键值对服务之后会看到该日志条目通过applyCh，并将对map/slice的引用保存在键值对服务器的状态中。在你的kv服务器修改map/slice以及Raft在持久化时读取该map/slice时会产生了一个竞争。 在配置更改期间，一对副本组可能需要在它们之间互相移动分片。如果你看到死锁，这可能是一个来源。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:4:3","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Challenge exercises 如果你要为生产使用构建一个这样的系统，接下来的两个特性是必不可少的。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:5:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Garbage collection of state 状态的垃圾回收 当一个副本组失去了分片的所有权时，该副本组应该消除其数据库中应该失去的key。对于它来说，保留这些不再拥有的数据时很浪费的。然而，这给迁移带来一些问题。假设我们有两组，G1和G2，并且有一个新的配置C需要将分片S从G1移动到G2。如果G1从数据库中删除了所有的S中的键值对，那么G2再试图转换到C的时候该如何获取S的数据呢？ Challenage 使每个副本组保留旧的碎片的时间不超过一个绝对必要的时间。即使像上面的G1这样的副本组中所有的服务器崩溃并重新启动，您的解决方案也必须正常工作。如果你通过了TestChallengeDelete那么你就完成了这个挑战。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:5:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"Client requests during configuration changes 在配置更改期间的客户端请求 处理配置更改的最简单的方法就是在转换完成之前禁止所有的客户端操作。虽然从概念上将很简单，但是这种方法在生产系统中是不可行的；它会导致在引入或取出机器时对所有的客户机进行长时间的暂停。最好是继续服务不受当前配置更改影响的分片。 Challenge1 修改你的解决方案以便于客户端在配置更改期间还可以正常操作不受配置更改影响的分片中的key。当你通过TestChallenge2Unaffected时你就完成了这个挑战。 虽然上面的优化是好的，但是我们可以做的更改。假设某个副本组G3在转换到C的时候，需要G1的分片S1，以及G2的分片S2。我们真的希望G3可以在接收到必要的状态后立即开始服务分片，即使它仍然在等待其他分片。例如，如果G1宕机，G3在收到来自G2的适当数据之后，仍然可以为S2的请求服务，尽管向C的转换尚未完成。 Challenge2 修改你的实现以便于副本组可以在即使配置更改仍在进行的时候，为它们可以服务的分片进行服务。当你通过了TestChallenge2Partial的时候你就完成了这个挑战。 ","date":"2022-03-20","objectID":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/:5:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab指导翻译","uri":"/mit-6.824-lab%E6%8C%87%E5%AF%BC%E7%BF%BB%E8%AF%91/"},{"categories":["分布式系统"],"content":"MIT-6.824-Lab2 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:0:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"前言 这个Lab我们主要是为了实现Raft论文中的功能，包括：选举、日志、持久化以及快照。 在开始实现lab之前，需要看一下课程官方提供的lab指导 lab2官方指导 个人翻译后的官方指导 强烈建议把Raft论文多看几遍，并且可以自己做一些总结。 重点看懂如下内容： Raft论文论文中的Figure2 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:1:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"State Raft的状态image-20220317203458927 \"\rRaft的状态\r 所有服务器的持久化状态： 在响应RPC之前在稳定的存储上进行更新 currentTerm 服务器已知的最新任期(在第一次启动时初始化为0，单调递增) voteFor 当前任期投给的候选人的Id(如果为null就是没有投票) log[] 日志项；每一个日志条目包含一个给状态机的命令，以及当它被leader接收的时候所处的任期(第一个索引是1) 所有服务器的易失状态： commitIndex 已知被提交的最高日志的索引(初始化为0，单调递增) lastApplied 被状态机应用的最高日志的索引(初始化为0，单调递增) 领袖节点的易失状态： 选举后重新初始化 nextIndex[] 对于每一个服务器，下一个应该被发送给他们的日志的索引(初始化为领袖的最近一个日志的index的下一位) matchIndex[] 对于每一个服务器，领袖已知的这些节点分别复制到的最高日志的索引(初始化为0，单调递增) ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:1:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"AppendEntries RPC 日志复制请求image-20220317205429607 \"\r日志复制请求\r 由领袖调用去将日志条目复制到节点；也用其作为心跳。 参数： term 领袖的当前任期 leaderId 领袖的Id，可以用于跟随者记录下来，直接重定向客户端的请求 prevLogIndex 最新的日志的前一个日志的索引 prevLogTerm preLogIndex处的日志的所属任期 entries[] 发送给跟随者去存储的日志条目(空的表示当前的RPC为心跳包；可以发送不知一个用于提效率) ledaerCommit 领袖的commitIndex(用于跟随者更新自己的commitIndex) 结果： term 服务器当前的任期，用于领袖更新自己的任期 success 如果跟随着包含匹配preLogIndex和preLogTerm的日志就返回true 接收者实现： 当term \u003c currentTerm，返回false。(即发送AppendEntries的服务器肯定不是现在的领袖，那么返回false表示不接收这个日志复制请求) 如果接收者在prevLogIndex处的日志的任期没有和prevLogTerm匹配，那么返回false。(表示当前追加的日志条目开始的位置不对) 如果服务器存在一个日志和新的需要复制的日志冲突了(相同的索引但是属于不同的任期)，那么删除掉这个日志条目以及所有处于它后面的日志。 追加所有当前服务器没有的新日志。 如果领袖的leaderCommit \u003e 当前服务器的commitIndex，设置commitIndex = min(leaderCommit, index of last new Entry)。 接收者实现细节 规则3 有如下情况： 当A节点当选领袖后，它的任期为3，并且接收到了两条新的日志，它将其发送给别的跟随着B,C,D,E。 但是刚发送给B，B成功复制，但是还没有发送给CDE或者发送给CDE的消息因为网络原因丢失了。那么就出现了如下情况。 日志情况如下 服务器名称 日志的索引和任期(index/term) A 1/1 2/1 3/2 4/3 5/3 B 1/1 2/1 3/2 4/3 5/3 C 1/1 2/1 3/2 D 1/1 2/1 3/2 E 1/1 2/1 3/2 此时日志4和5由于领袖没有成功复制到大多数节点，因此并没有被提交 这时候C/D/E可以通过选举得到除了A/B以外的节点同意，因此可以成为新的领袖，这时候由于日志4/3和5/3没有成功被提交，因此领袖可以对其进行删除后再追加新的日志，这也就是规则3需要应对的情况。 此时日志情况如下 服务器名称 日志的索引和任期(index/term) A 1/1 2/1 3/2 4/4 5/4 B 1/1 2/1 3/2 4/4 5/4 C 1/1 2/1 3/2 4/4 5/4 D 1/1 2/1 3/2 4/4 5/4 E 1/1 2/1 3/2 4/4 5/4 规则5 跟随者成功接收了领袖的AppendEntire RPC时，设置跟随者自己的commitIndex = min(leaderCommit, index of last new entry)。因为有可能领袖的发送给该跟随者的日志最大索引也比当前领袖的commitIndex要小，所以要在leaderCommit和自己当前最新接收的日志中找那个更小的作为更新的数据。可以防止更新了commitIndex但是现在跟随者的日志并没有更新到该索引，导致错误。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:1:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"RequestVote RPC 请求投票RPC方法image-20220317210920159 \"\r请求投票RPC方法\r 由候选人调用来收集选票 参数： term 候选人的任期 candidateId 请求投票的候选人Id lastLogIndex 候选人的最近一个日志条目的索引 lastLogTerm 候选人的最近一个日志条目的所属任期 结果： term 服务器当前的任期，用于候选人更新自己 voteGranted 当候选人符合条件的时候，返回true表示成功投票给它 接收者实现： 如果term \u003c currentTerm返回false。(即候选人的任期比当前服务器的任期还小，自然不可能成为领袖，因此返回false) 当该服务器的voteFor是空或者就是当前请求投票的这个候选人，以及候选人的日志最少和接受者的日志一样新，那么就给它投票。 接收者实现细节 规则2 当服务器的voteFor为空或者就是当前请求投票的这个候选人的id，而且需要候选人的日志最少和它一样新，才会给它投票。 首先，为了防止出现脑裂的情况，我们一个任期只能有一个领袖，因此如果当前任期已经投票了，就不要再投了，除非是当前已经投给的那个候选人又发了一次RequestVote RPC。而且只有当候选人的日志最少和该服务器的日志一样新的时候，才能投票。 这里的最少一样新，可以理解为，最新日志的任期更大，或者相同任期但是索引更大。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:1:3","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"服务器的规则 服务器需要遵守的规则image-20220318215512320 \"\r服务器需要遵守的规则\r 对于所有服务器 如果commitIndex \u003e lastApplied：自增lastApplied，并且将索引为lastApplied的日志应用到状态机中。 如果RPC请求或者响应中的参数term \u003e currentTerm：那么更新currentTerm和term相等，然后转化为跟随者。(也就是当前自己任期比别人的小的时候，自己当前一定只能为跟随者，并且需要更新到相同的任期) 对于跟随者 响应候选人和领袖发来的RPC。 如果选举计时器到期了还没有收到正确的领袖发来的AppendEntries，或者没有投票给候选人，那么就变成了候选人。 对于候选人 转换为候选人的时候，开始选举。 自增当前的任期号currentTerm 给自己投票(防止又给别人投票，出现脑裂) 重置选举计时器 给所有其他的服务器发送RequestVote RPC 如果从大多数服务器收到了选票，那么成为当前任期的领袖。 从一个新的领袖那里接收到AppendEntries RPC，就转换为跟随者。 如果选举计时器到期，开始一轮选举。 对于领袖 选举后：发送初始化的AppendEntries RPC空包(心跳包)到每一个服务器；并在空闲时期不断发送来防止跟随者选举计时器到期。(为了维护自己的领袖地位) 如果从客户端接收到一个命令：追加日志到本机日志，然后当状态机成功应用该日志之后再响应客户端的请求。 如果最新的日志的索引\u003e=该跟随者的nextIndex：发送携带从nextIndex开始的日志条目的AppendEntries RPC请求到跟随者。 如果成功了：更新该跟随者的nextIndex和matchIndex。(这里一定要注意并发问题，后续会提到) 如果是因为日志不一致而导致的失败：递减nextIndex然后重试。(我们采取优化的Fast Backup，可以快速的进行日志的同步) 如果存在一个大于commitIndex的N，大多数跟随者的matchIndex[i] \u003e N，而且索引为N的日志的任期等于当前任期，那么设置commitIndex = N。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:1:4","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"选举 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:2:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"Raft服务器参数 我们需要如下参数： type Raft struct { mu sync.Mutex // Lock to protect shared access to this peer's state peers []*labrpc.ClientEnd // RPC end points of all peers persister *Persister // Object to hold this peer's persisted state me int // this peer's index into peers[] dead int32 // set by Kill() // Your data here (2A, 2B, 2C). // Look at the paper's Figure 2 for a description of what // state a Raft server must maintain. //persistent state currentTerm int //当前任期 voteFor int //当前任期投给的候选人id(为-1时代表没有投票) logEntries []LogEntry //日志条目 commitIndex int //当前log中的最高索引(从0开始,递增) lastApplied int //当前被用到状态机中的日志最高索引(从0开始,递增) //volatile state on leader nextIndex []int //发送给每台服务器的下一条日志目录索引(初始值为leader的commitIndex + 1) matchIndex []int //每台服务器已知的已被复制的最高日志条目索引 //volatile state on all servers state State //当前raft状态 timerElect *time.Timer //选举计时器 timerHeartBeat *time.Timer //心跳计时器 timeoutHeartBeat int //心跳频率/ms timeoutElect int //选举频率/ms applyCh chan ApplyMsg //命令应用通道 applyCond *sync.Cond //命令应用cond //最近快照的数据 snapshotData []byte //最近快照的数据 } // LogEntry 日志条目 type LogEntry struct { Command interface{} //日志记录的命令(用于应用服务的命令) Index int //该日志的索引 Term int //该日志被接收的时候的Leader任期 } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:2:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"计时器 首先，我们是需要周期性的进行选举的，因此肯定是需要实现一个选举计时器的，那么我们可以直接实现如下代码，使用time.Timer来作为定时器。 实现ticker方法，该方法不断进行超时判断 func (rf *Raft) ticker() { for rf.killed() == false { // Your code here to check if a leader election should // be started and to randomize sleeping time using // time.Sleep(). select { case \u003c-rf.timerElect.C: if rf.killed() { break } rf.mu.Lock() DPrintf(\"id[%d].state[%v].term[%d]: 选举计时器到期\\n\", rf.me, rf.state, rf.currentTerm) if rf.state != LEADER { //当不为leader时,也就是超时了,那么转变为Candidate rf.startElection() } //重置选举计时器 rf.resetElectTimer() rf.mu.Unlock() case \u003c-rf.timerHeartBeat.C: if rf.killed() { break } rf.mu.Lock() if rf.state == LEADER { //当心跳计时器到时间后,如果是Leader就开启心跳检测 go rf.Broadcast() } //重置心跳计时器 rf.timerHeartBeat.Reset(time.Duration(rf.timeoutHeartBeat) * time.Millisecond) rf.mu.Unlock() } } } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:2:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现RequestVote 现在需要完成的就是RequestVote RPC。结合上述规则，可得代码如下(相信我的注释应该很详细的~) // RequestVote // example RequestVote RPC handler. // func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { // Your code here (2A, 2B). //无论如何,返回参数中的term应修改为自己的term rf.mu.Lock() defer rf.mu.Unlock() defer rf.persist() DPrintf(\"id[%d].state[%v].term[%d]: 接收到[%d]的选举申请\\n\", rf.me, rf.state, rf.currentTerm, args.CandidateId) defer func() { DPrintf(\"id[%d].state[%v].term[%d]: 给[%d]的选举申请返回%v\\n\", rf.me, rf.state, rf.currentTerm, args.CandidateId, reply.VoteGranted) }() defer func() { reply.Term = rf.currentTerm }() reply.VoteGranted = false //1.如果Term\u003ccurrentTerm或者已经投过票了,则之直接返回拒绝 if args.Term \u003c rf.currentTerm || (args.Term == rf.currentTerm \u0026\u0026 rf.voteFor != -1 \u0026\u0026 rf.voteFor != args.CandidateId) { return } //2.如果t \u003e currentTerm,则更新currentTerm,并切换为follower if args.Term \u003e rf.currentTerm { rf.currentTerm = args.Term rf.toFollower() rf.voteFor = -1 } //3.判断候选人的日志是否最少一样新 //如果两份日志最后的条目的任期号不同,那么任期号大的日志更加新;如果两份日志最后的条目任期号相同,那么日志比较长的那个就更加新 if rf.lastLog().Term == -1 || args.LastLogTerm \u003e rf.lastLog().Term || (args.LastLogTerm == rf.lastLog().Term \u0026\u0026 args.LastLogIndex \u003e= rf.lastLog().Index) { //重置选举时间 rf.resetElectTimer() //投票给候选人 rf.voteFor = args.CandidateId //投赞成 reply.VoteGranted = true } } // AppendEntriesArgs 日志追加RPC的请求参数 type AppendEntriesArgs struct { Term int //当前leader的任期 LeaderId int //leader的id,follower可以将client错发给它的请求转发给leader PrevLogIndex int //最新日志前的那一条日志条目的索引 PrevLogTerm int //最新日志前的那一条日志条目的任期 Entries []LogEntry //需要被保存的日志条目(为空则为心跳包) LeaderCommit int //leader的commitIndex } // AppendEntriesReply 日志追加的RPC的返回值 type AppendEntriesReply struct { Term int //接收者的currentTerm Success bool //如果prevLogIndex和prevLogTerm和follower的匹配则返回true XTerm int //若follower和leader的日志冲突,则记载的是follower的log在preLogIndex处的term,若preLogIndex处无日志,返回-1 XIndex int //follower中的log里term为XTerm的第一条log的index XLen int //当XTerm为-1时,此时XLen记录follower的日志长度(不包含初始占位日志) } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:2:3","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现选举 上述的ticker方法，我们会调用一个rf.startElection()方法来进行选举，代码实现如下。 // StartElection 发起选举 func (rf *Raft) startElection() { rf.toCandidate() defer rf.persist() voteNums := 1 for i := range rf.peers { if i == rf.me { continue } args := \u0026RequestVoteArgs{ Term: rf.currentTerm, CandidateId: rf.me, LastLogIndex: rf.lastLog().Index, LastLogTerm: rf.lastLog().Term, } go func(i int) { reply := \u0026RequestVoteReply{} ok := rf.sendRequestVote(i, args, reply) if ok { rf.mu.Lock() defer rf.mu.Unlock() if rf.currentTerm == args.Term \u0026\u0026 rf.state == CANDIDATE { if reply.VoteGranted { voteNums++ if voteNums \u003e len(rf.peers)/2 { go rf.toLeader() } } } else if reply.Term \u003e rf.currentTerm { rf.currentTerm = reply.Term rf.toFollower() rf.voteFor = -1 } } }(i) } } 当获得大多数选票的时候，就成为了当前任期的领袖。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:2:4","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"状态转换 成为领袖 上述选举后，若成功的当选领袖，那么就需要转化为领袖的操作，代码实现如下，全部基于解析的论文中的Figure2中的规则： // toLeader 转变为leader func (rf *Raft) toLeader() { rf.mu.Lock() defer rf.mu.Unlock() DPrintf(\"id[%d].state[%v].term[%d]: 成为Leader\\n\", rf.me, rf.state, rf.currentTerm) rf.state = LEADER //1.初始化volatile state on leader rf.nextIndex = make([]int, len(rf.peers)) rf.matchIndex = make([]int, len(rf.peers)) //初始化nextIndex为commitIndex+1 for i := range rf.nextIndex { rf.nextIndex[i] = rf.commitIndex + 1 } //初始化matchIndex为0(实例化的时候已经赋值0了,不需要自己再赋值一次了) //当为leader时,开始启动协程来实时更新commitIndex go rf.updateCommitIndex() //追加一条空日志,用于更新到最新的commitIndex go rf.Start(nil) //立马开始一轮心跳 rf.timerHeartBeat.Reset(0) } 当成为领袖之后，需要启动一个协程来更新自己的commitIndex(上述解析中提到，领袖的commitIndex需要再合适的时候进行更新)。 还需要立马开始一轮心跳，也就是发送一轮AppendEntries RPC。 这里的代码中还有一个立马进行保存一个空日志的逻辑，这个后续会讲到，属于是优化的内容。 成为候选人 当我们选举计时器到期后，需要转化为候选人并进行选举。根据论文中规则，需要自增自己的任期，并且给自己投票。代码实现如下： //转变为候选人 func (rf *Raft) toCandidate() { //切换状态 rf.state = CANDIDATE //自增任期号 rf.currentTerm++ //给自己投票 rf.voteFor = rf.me DPrintf(\"id[%d].state[%v].term[%d]: 变成Candidate\\n\", rf.me, rf.state, rf.currentTerm) } 成为跟随者 当服务器接收到比自己任期大的服务器发来的请求或者响应的时候，或者投出选票的时候，需要转化为跟随者。 代码实现如下： //转变为follower func (rf *Raft) toFollower() { if rf.state == FOLLOWER { return } rf.state = FOLLOWER DPrintf(\"id[%d].state[%v].term[%d]: 变成Follower\\n\", rf.me, rf.state, rf.currentTerm) } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:2:5","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"广播 当服务器成为领袖之后，为了维护自己的领袖地位，需要周期性的发送AppendEntries RPC到跟随者，为了重置他们的选举计时器，也是为了进行日志复制。代码实现如下： // Broadcast 发起广播发送AppendEntries RPC func (rf *Raft) Broadcast() { rf.mu.Lock() defer rf.mu.Unlock() if rf.state == LEADER { DPrintf(\"id[%d].state[%v].term[%d]: 开始一轮广播\\n\", rf.me, rf.state, rf.currentTerm) for i := range rf.peers { if i != rf.me { go rf.HandleAppendEntries(i) } } } } 这里的rf.HandleAppendEntries()方法在选举模块只需要实现发送空的AppendEntries RPC即可，发送日志在后续实现 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:2:6","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"日志 这一模块需要实现Raft之间的日志复制，重点是实现AppendEntries RPC以及发送AppendEntries RPC的方法HandleAppendEntries。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:3:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"日志的存储 我们将日志存在Raft的变量logEntries中，是由LonEntry数组构成，并且LogEntry中存有命令、索引和任期。 LogEntry数据结构 // LogEntry 日志条目 type LogEntry struct { Command interface{} //日志记录的命令(用于应用服务的命令) Index int //该日志的索引 Term int //该日志被接收的时候的Leader任期 } 我们将索引存在该数据结构里面而不是以数组的下标为索引，原因是： 更容易操作，数组反而涉及到很多下标变化之类的问题。 后续我们的日志会进行快照，也就是该数组中第一个日志并不是索引为1了。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:3:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现AppendEntries 根据上述分析的论文中的规则，可以实现代码如下： // AppendEntriesArgs 日志追加RPC的请求参数 type AppendEntriesArgs struct { Term int //当前leader的任期 LeaderId int //leader的id,follower可以将client错发给它的请求转发给leader PrevLogIndex int //最新日志前的那一条日志条目的索引 PrevLogTerm int //最新日志前的那一条日志条目的任期 Entries []LogEntry //需要被保存的日志条目(为空则为心跳包) LeaderCommit int //leader的commitIndex } // AppendEntriesReply 日志追加的RPC的返回值 type AppendEntriesReply struct { Term int //接收者的currentTerm Success bool //如果prevLogIndex和prevLogTerm和follower的匹配则返回true XTerm int //若follower和leader的日志冲突,则记载的是follower的log在preLogIndex处的term,若preLogIndex处无日志,返回-1 XIndex int //follower中的log里term为XTerm的第一条log的index XLen int //当XTerm为-1时,此时XLen记录follower的日志长度(不包含初始占位日志) } // AppendEntries 日志追加的RPC handler func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { rf.mu.Lock() defer rf.mu.Unlock() defer rf.persist() //将自己的term返回 defer func() { reply.Term = rf.currentTerm }() reply.Success = true DPrintf(\"id[%d].state[%v].term[%d]: 接收到[%d],term[%d]的日志追加,preLogIndex = [%d], preLogTerm = [%d],entries = [%v]\\n\", rf.me, rf.state, rf.currentTerm, args.LeaderId, args.Term, args.PrevLogIndex, args.PrevLogTerm, args.Entries) //DPrintf(\"id[%d].state[%v].term[%d]: 此时已有的log=[%v]\\n\", rf.me, rf.state, rf.currentTerm, rf.logEntries) //判断term是否小于当前任期 if args.Term \u003c rf.currentTerm { DPrintf(\"id[%d].state[%v].term[%d]: 追加日志的任期%d小于当前任期%d\\n\", rf.me, rf.state, rf.currentTerm, args.Term, rf.currentTerm) reply.Success = false return } //若请求的term大于该server的term,则更新term并且将voteFor置为未投票 if args.Term \u003e rf.currentTerm { rf.currentTerm = args.Term rf.voteFor = -1 } //重置选举时间 rf.resetElectTimer() //转变为follower rf.toFollower() //进行日志一致性判断(快速恢复) //若leader在preLogIndex处没有日志 if rf.lastLog().Index \u003c args.PrevLogIndex { reply.Term = 0 reply.Success = false //preLogIndex处无日志,记录XTerm为-1 reply.XTerm = -1 //记录XLen为当前最新日志的index reply.XLen = rf.lastLog().Index DPrintf(\"id[%d].state[%v].term[%d]: 追加日志的和现在的日志不匹配\\n\", rf.me, rf.state, rf.currentTerm) return } if args.PrevLogIndex \u003c rf.logEntries[0].Index { reply.XTerm = -1 reply.Term = 0 reply.XLen = rf.logEntries[0].Index reply.Success = false return } //若preLogIndex处的日志的term和preLogTerm不相等(或者) if rf.logEntries[0].Index \u003c= args.PrevLogIndex \u0026\u0026 rf.index(args.PrevLogIndex).Term != args.PrevLogTerm { reply.Success = false //更新XTerm为冲突的Term reply.XTerm = rf.index(args.PrevLogIndex).Term //更新XIndex为XTerm在本机log中第一个Index位置 reply.XIndex = rf.binaryFindFirstIndexByTerm(reply.XTerm) DPrintf(\"id[%d].state[%v].term[%d]: 追加日志的和现在的日志不匹配\\n\", rf.me, rf.state, rf.currentTerm) return } //追加 for i, logEntry := range args.Entries { index := args.PrevLogIndex + i + 1 if index \u003e rf.lastLog().Index { rf.logEntries = append(rf.logEntries, logEntry) } else if index \u003c= rf.logEntries[0].Index { //当追加的日志处于快照部分,那么直接跳过不处理该日志 continue } else { if rf.index(index).Term != logEntry.Term { rf.logEntries = rf.logEntries[:rf.binaryFindRealIndexInArrayByIndex(index)] // 删除当前以及后续所有log rf.logEntries = append(rf.logEntries, logEntry) // 把新log加入进来 } // term一样啥也不用做，继续向后比对Log } } if len(args.Entries) \u003e 0 { DPrintf(\"id[%d].state[%v].term[%d]: 追加后的的log=[%v]\\n\", rf.me, rf.state, rf.currentTerm, rf.logEntries) } //更新follower的commitIndex rf.updateCommitIndexForFollower(args.LeaderCommit) } 但是这里大家会发现返回值的参数有一些不同，是因为我们使用了快速恢复进行优化 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:3:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"快速恢复 上述提到的快速恢复，也就是Fast Backup，是用来快速的使跟随者复制到和领袖一样的日志。论文中提到的如果AppendEntries RPC因为日志不一致而导致失败，那么就需要将nextIndex自减，然后再次发送请求。 论文中的发送失败后的规则image-20220319124309763 \"\r论文中的发送失败后的规则\r 那么如果出现一个情况，领袖这时候的最新日志的索引已经达到了100万，然后有一个服务器因为网络原因，一直没有被正确的接收到领袖的AppendEnrties RPC，导致现在该跟随者最新的日志索引为1。这时候网络恢复正常了，就需要同步几万条，但是这时候如果领袖是新当选的，它会初始化nextIndex为最新的日志索引+1，也就是100万零1，那么如果我们每次自减一次，就需要进行100万次AppendEntries RPC才能正确的发送日志。会严重影响到效率。 因此可以采用快速恢复的方法。 快速恢复原理 可以在AppendEntries RPC的回复参数中加上三个参数： XTerm：这个是跟随者中与领袖冲突的日志对应的任期。如果跟随者在请求的参数中的prevLogIndex处的日志任期号和参数中的prevLogTerm不匹配，它会拒绝领袖的AppendEntries消息，并将自己的任期号放在XTerm中。如果跟随者在对应位置没有日志，那么这里会返回 -1。 XIndex：这个是跟随者中，对应任期号为XTerm的第一条日志条目的槽位号。(也就代表着从该处开始冲突，也就需要从该处开始复制日志) XLen：如果跟随者在prevLogIndex处没有日志，那么XTerm会返回-1，XLen表示最新的日志。 那么这时候只需要一次AppendEntries RPC就可以让领袖知道我需要给跟随者发送从哪里开始的日志，也就达到了快速将跟随者的日志和领袖保持一致了。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:3:3","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"发送AppendEntries 也就是实现上面提到的HandleAppendEntries方法，由于我已经全部完成了，所以下面代码中还包括了快照模块的代码，自行忽略即可。 // HandleAppendEntries handle对AppendEntries的发送和返回处理(这里返回值表示这次请求目标follower是否仍认为自己是leader) func (rf *Raft) HandleAppendEntries(server int) (success bool) { success = false rf.mu.Lock() if rf.state != LEADER { rf.mu.Unlock() return } //DPrintf(\"id[%d].state[%v].term[%d]: leader此时的log=[%v]\\n\", rf.me, rf.state, rf.currentTerm, rf.logEntries) DPrintf(\"id[%d].state[%v].term[%d]: server[%d]的nextIndex=[%d],matchIndex=[%d],lastIncludedIndex=[%d]\\n\", rf.me, rf.state, rf.currentTerm, server, rf.nextIndex[server], rf.matchIndex[server], rf.logEntries[0].Index) //检查此时是否传的日志存在于快照中 if rf.nextIndex[server] \u003c= rf.logEntries[0].Index { args := InstallSnapshotArgs{ Term: rf.currentTerm, LeaderId: rf.me, LastIncludedIndex: rf.logEntries[0].Index, LastIncludedTerm: rf.logEntries[0].Term, Data: rf.snapshotData, } reply := InstallSnapshotReply{} DPrintf(\"id[%d].state[%v].term[%d]: 发送installSnapshot to [%d];lastIncludedIndex=[%d],lastIncludedTerm=[%d]\\n\", rf.me, rf.state, rf.currentTerm, server, rf.logEntries[0].Index, rf.logEntries[0].Term) rf.mu.Unlock() ok := rf.sendInstallSnapshot(server, \u0026args, \u0026reply) rf.mu.Lock() defer rf.mu.Unlock() //过期的请求直接结束 if rf.state != LEADER || args.Term != rf.currentTerm { return } if !ok { DPrintf(\"id[%d].state[%v].term[%d]: 发送installSnapshot to [%d] error\\n\", rf.me, rf.state, rf.currentTerm, server) return } if reply.Term \u003e rf.currentTerm { rf.currentTerm = reply.Term rf.toFollower() rf.voteFor = -1 rf.persist() DPrintf(\"id[%d].state[%v].term[%d]: 发送installSnapshot to [%d] 过期,转变为follower\\n\", rf.me, rf.state, rf.currentTerm, server) return } success = true //若安装成功,则更新nextIndex和matchIndex rf.matchIndex[server] = args.LastIncludedIndex rf.nextIndex[server] = rf.matchIndex[server] + 1 DPrintf(\"id[%d].state[%v].term[%d]: 发送installSnapshot to [%d] 成功,更新nextIndex-\u003e[%d];matchIndex-\u003e[%d]\\n\", rf.me, rf.state, rf.currentTerm, server, rf.nextIndex[server], rf.matchIndex[server]) return } //若不存在于快照中,则正常appendEntries args := AppendEntriesArgs{ Term: rf.currentTerm, LeaderId: rf.me, PrevLogIndex: rf.nextIndex[server] - 1, PrevLogTerm: rf.index(rf.nextIndex[server] - 1).Term, LeaderCommit: rf.commitIndex, } //添加需要发送的日志 args.Entries = make([]LogEntry, len(rf.logEntries)-rf.binaryFindRealIndexInArrayByIndex(rf.nextIndex[server])) copy(args.Entries, rf.logEntries[rf.binaryFindRealIndexInArrayByIndex(rf.nextIndex[server]):]) reply := AppendEntriesReply{} DPrintf(\"id[%d].state[%v].term[%d]: 发送appendEntries to [%d];PrevLogIndex=[%d];Entries=[%v]\\n\", rf.me, rf.state, rf.currentTerm, server, args.PrevLogIndex, args.Entries) rf.mu.Unlock() ok := rf.sendAppendEntries(server, \u0026args, \u0026reply) rf.mu.Lock() defer rf.mu.Unlock() //过期的请求直接结束 if rf.state != LEADER || args.Term != rf.currentTerm { return } if !ok { DPrintf(\"id[%d].state[%v].term[%d]: 发送ae to [%d] error\\n\", rf.me, rf.state, rf.currentTerm, server) return } //判断是否任期更大,更新自身状态 if reply.Term \u003e rf.currentTerm { //修改term rf.currentTerm = reply.Term //转变为follower rf.toFollower() //更新为未投票 rf.voteFor = -1 rf.persist() DPrintf(\"id[%d].state[%v].term[%d]: 发送ae to [%d] 过期,转变为follower\\n\", rf.me, rf.state, rf.currentTerm, server) return } DPrintf(\"id[%d].state[%v].term[%d]: follower仍认为自己是leader\\n\", rf.me, rf.state, rf.currentTerm, server) success = true //若返回失败 if !reply.Success { //更新nextIndex //当follower的preLogIndex处无日志时 if reply.XTerm == -1 { //更新nextIndex为follower的最后一条日志的下一个位置 rf.nextIndex[server] = reply.XLen + 1 } else { //当preLogIndex处的日志任期冲突时 //更新nextIndex为该冲突任期的第一条日志的位置,为了直接覆盖冲突的任期的所有的日志 rf.nextIndex[server] = reply.XIndex } DPrintf(\"id[%d].state[%v].term[%d]: 追加日志到server[%d]失败,更新nextIndex-\u003e[%d],matchIndex-\u003e[%d]\\n\", rf.me, rf.state, rf.currentTerm, server, rf.nextIndex[server], rf.matchIndex[server]) return } //若成功 rf.matchIndex[server] = args.PrevLogIndex + len(args.Entries) rf.nextIndex[server] = rf.matchIndex[server] + 1 //只有发送了不为空的日志(也就不是心跳包的时候)才真正的更新了,心跳包相当于没更新 if len(args.Entries) \u003e 0 { DPrintf(\"id[","date":"2022-03-18","objectID":"/mit-6.824-lab2/:3:4","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"更新commitIndex 根据论文中提到的规则，我们需要不断更新领袖的commitIndex，那么我们就可以写一个方法，周期性的检查从最新的日志一直到当前的commitIndex+1处的日志，是否有超过一半的matchIndex达到了其中的日志索引处，若有则更新commitIndex为那个索引。之所以从最新的开始往前检查，是因为这样可以快速定位到目标索引。 代码实现如下： // updateCommitIndex 检查更新commitIndex func (rf *Raft) updateCommitIndex() { for !rf.killed() { time.Sleep(5 * time.Millisecond) rf.mu.Lock() if rf.state != LEADER { rf.mu.Unlock() return } //从lastLog开始 for i := rf.lastLog().Index; i \u003e rf.commitIndex; i-- { updateConNum := len(rf.peers) / 2 num := 0 for j := range rf.peers { if j == rf.me { continue } //若match[j] \u003e= i 而且log[i].Term == currentTerm则该server符合更新要求 if rf.matchIndex[j] \u003e= i \u0026\u0026 rf.index(i).Term == rf.currentTerm { num++ } } //若过半数则更新commitIndex if num \u003e= updateConNum { rf.commitIndex = i DPrintf(\"id[%d].state[%v].term[%d]: n = %d, 过半节点的matchIndex \u003e= n而且log[n].Term == currentTerm,则更新commitIndex = %d\\n\", rf.me, rf.state, rf.currentTerm, i, i) //唤醒ApplyCommand routine rf.applyCond.Broadcast() break } } rf.mu.Unlock() } } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:3:5","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"应用日志到状态机 我们在检查到有可以提交的日志的时候，即可以将其传递给状态机应用，那么就可以开启一个协程用来检查并更新lastApplied以及将其应用到状态机中。而且当当前没有可以应用的日志的时候，就在cond上等待，所有更新commitIndex的代码后面都会唤醒在该cond上等待的协程，也就是ApplyCommand协程。 代码实现如下： // ApplyCommand 检查是否 commitIndex \u003e lastApplied,若是则lastApplied递增,并将log[lastApplied]应用到状态机 func (rf *Raft) ApplyCommand() { for !rf.killed() { rf.mu.Lock() //不符合条件时放弃锁进行等待 for rf.lastApplied \u003e= rf.commitIndex { rf.applyCond.Wait() } //被唤醒而且符合条件 //当前的commitIndex commitIndex := rf.commitIndex lastApplied := rf.lastApplied DPrintf(\"id[%d].state[%v].term[%d]: apply command [%d,%d]\\n\", rf.me, rf.state, rf.currentTerm, lastApplied+1, commitIndex) var applyEntries = make([]LogEntry, rf.commitIndex-rf.lastApplied, rf.commitIndex-rf.lastApplied) copy(applyEntries, rf.logEntries[rf.binaryFindRealIndexInArrayByIndex(lastApplied+1):rf.binaryFindRealIndexInArrayByIndex(commitIndex+1)]) rf.mu.Unlock() //解锁后进行apply for _, entry := range applyEntries { rf.applyCh \u003c- ApplyMsg{ CommandValid: true, Command: entry.Command, CommandIndex: entry.Index, CommandTerm: entry.Term, } } rf.mu.Lock() //更新lastApplied,由于在apply过程中进行了解锁,因此不能使用现在的commitIndex,而是之前情况的commitIndex //(若在解锁过程中,进行了新的log的apply导致lastApplied更新至比该次更新目标的commitIndex还大,那么保持不变,因此这里的更新需要一个Max()来辅助) rf.lastApplied = Max(rf.lastApplied, commitIndex) rf.mu.Unlock() } } 由于该写成是每一个服务器都需要的，和服务器状态无关，那么就需要在初始化Raft的时候开启(ticker也同理) func Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft { rf := \u0026Raft{} rf.peers = peers rf.persister = persister rf.me = me // Your initialization code here (2A, 2B, 2C). rf.currentTerm = 0 rf.voteFor = -1 rf.logEntries = make([]LogEntry, 0) rf.logEntries = append(rf.logEntries, LogEntry{-1, -1, 0}) rf.commitIndex = 0 rf.lastApplied = 0 rf.state = FOLLOWER rf.nextIndex = make([]int, len(peers)) rf.matchIndex = make([]int, len(peers)) rf.timeoutHeartBeat = 150 rf.timeoutElect = 300 rf.timerHeartBeat = time.NewTimer(time.Duration(rf.timeoutHeartBeat) * time.Millisecond) rf.timerElect = time.NewTimer(time.Duration(rf.timeoutElect+rand.Intn(1000)) * time.Millisecond) rf.applyCh = applyCh rf.applyCond = sync.NewCond(\u0026rf.mu) DPrintf(\"id[%d].state[%v].term[%d]: finish init\\n\", rf.me, rf.state, rf.currentTerm) // initialize from state persisted before a crash rf.readPersist(persister.ReadRaftState()) rf.snapshotData = persister.snapshot rf.lastApplied = rf.logEntries[0].Index rf.commitIndex = rf.logEntries[0].Index // start ticker goroutine to start elections go rf.ticker() go rf.ApplyCommand() return rf } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:3:6","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"持久化 由于我们现在都是保存在内存中的，那么断电即失，因此我们肯定是需要持久化保存起来的，比如说写入磁盘中。由于lab测试方便，官方提供的是一个类Persister来模拟持久化存储的容器，实际上这部分可以换成直接对磁盘的写入或者通过RockDB之类的进行持久化。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:4:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"持久化数据 func (rf *Raft) persist() { // Your code here (2C). // Example: // w := new(bytes.Buffer) // e := labgob.NewEncoder(w) // e.Encode(rf.xxx) // e.Encode(rf.yyy) // data := w.Bytes() // rf.persister.SaveRaftState(data) w := new(bytes.Buffer) e := labgob.NewEncoder(w) //编码currentTerm err := e.Encode(rf.currentTerm) if err != nil { DPrintf(\"id[%d].state[%v].term[%d]: encode currentTerm error: %v\\n\", rf.me, rf.state, rf.currentTerm, err) return } //编码voteFor err = e.Encode(rf.voteFor) if err != nil { DPrintf(\"id[%d].state[%v].term[%d]: encode voteFor error: %v\\n\", rf.me, rf.state, rf.currentTerm, err) return } //编码log[] err = e.Encode(rf.logEntries) if err != nil { DPrintf(\"id[%d].state[%v].term[%d]: encode logEntries[] error: %v\\n\", rf.me, rf.state, rf.currentTerm, err) return } data := w.Bytes() //保存持久化状态 rf.persister.SaveRaftState(data) } 在我们对如上的数据进行更改的时候，都需要进行一次持久化 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:4:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"读取持久化数据 func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) \u003c 1 { // bootstrap without any state? return } // Your code here (2C). // Example: // r := bytes.NewBuffer(data) // d := labgob.NewDecoder(r) // var xxx // var yyy // if d.Decode(\u0026xxx) != nil || // d.Decode(\u0026yyy) != nil { // error... // } else { // rf.xxx = xxx // rf.yyy = yyy // } r := bytes.NewBuffer(data) d := labgob.NewDecoder(r) var currentTerm int var voteFor int var logEntries []LogEntry if d.Decode(\u0026currentTerm) != nil || d.Decode(\u0026voteFor) != nil || d.Decode(\u0026logEntries) != nil { DPrintf(\"id[%d].state[%v].term[%d]: decode error\\n\", rf.me, rf.state, rf.currentTerm) } else { rf.currentTerm = currentTerm rf.voteFor = voteFor rf.logEntries = logEntries } } 我们初始化的时候，需要从persister中读取持久化数据 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:4:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"快照 我们的日志肯定是不可以持续的增长下去的，因为当我们日志数量达到很大的时候，比如说我们的日志数据已经达到了几千万条的时候，我们和一个还没有多少数据的跟随者进行同步的话，需要将这些日志全部发送，其实是十分浪费资源和时间的。 那么我们其实可以使用快照，也就是对领袖某一个时刻它的状态机的数据进行保存，然后将这个快照发送给那些很落后的节点进行快速的同步，同时由于快照已经记录此时的所有必要数据，那么我们可以将这些日志删除，避免日志无限度的增长下去。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:5:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"论文解析 论文中的Figure 13是安装快照的RPC的参数和实现。 安装快照RPCimage-20220319210923396 \"\r安装快照RPC\r 由领袖调用，用于发送一个快照的分块给跟随者。领袖领袖按照顺序发送分块 参数： term 领袖的任期 leaderId 领袖的id，便于跟随者用于重定向客户端的请求 lastIncludedIndex 快照取代的所有的日志中最后一个日志的索引 lastIncludedTerm lastIncludedIndex处的日志的任期 offset 该分块在快照文件中的字节偏移量 data[] 从offset开始的分块的纯字节数据 done 如果是最后一个分块则为true 结果： term 服务器的currentTerm，用于领袖更新自己的任期 接收者实现： 如果term \u003c currentTerm则立马回复。 如果是第一个分块则创建一个新的快照文件。(offset为0) 在给定的offset处开始写入数据。 如果done不为true，那么回复然后等待更多的数据分块传来。 保存快照文件，丢弃任何比lastIncludedIndex小的快照或者部分快照。 如果存在一个日志和快照最后包含的日志有着一样的索引和任期，那么保留这个日志以及其以后的日志，并回复。 丢弃所有日志。 使用快照的内容重置状态机。(以及加载快照的集群配置) ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:5:1","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现快照 但是我们lab不需要实现这么复杂的快照，因此对其进行了简化。 我们一次直接发送一整个快照过去。 调用流程是： 状态机发现自己的目前的存储数据过大，那么就保存当前的状态机必须状态以及日志和Raft的必须状态到快照中。然后通知Raft对自己的日志进行丢弃，也就是调用Raft的Snapshot()。(日志数组第一位要么为空占位日志，也就是一次快照都没进行的时候日志数组下标为0位置的日志，要么为快照后索引为lastIncludeIndex的日志) 当领袖发送ApppendEntries RPC的时候，发现需要跟随者的nextIndex \u003c= 日志数组中第一个日志的索引的时候，也就是需要发送的日志已经被丢弃了，那么就调用InstallSnapshot()来安装快照。 当跟随者接收到领袖发来的快照的时候，若快照是正确的，那么就接收，并通过applyCh传递给状态机。 状态机接收到安装快照的请求，进行快照数据的应用，并且通知Raft去更新到该快照。也就是调用Raft的CondInstallSnapshot()。 Raft被调用CondInstallSnapshot()之后，对响应的日志进行丢弃。 交互流程图image-20220319213012992 \"\r交互流程图\r ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:5:2","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现InstallSnapshot // InstallSnapshotArgs 快照安装RPC的参数 type InstallSnapshotArgs struct { Term int //leader的任期 LeaderId int //leader的id LastIncludedIndex int //快照中包含的最后一个日志条目的index LastIncludedTerm int //快照中包含的最后一个日志条目的term Data []byte //快照数据 } // InstallSnapshotReply 快照安装的返回值 type InstallSnapshotReply struct { Term int //接收者的currentTerm } // InstallSnapshot 快照安装的RPC func (rf *Raft) InstallSnapshot(args *InstallSnapshotArgs, reply *InstallSnapshotReply) { rf.mu.Lock() defer rf.mu.Unlock() defer func() { reply.Term = rf.currentTerm }() //1.判断参数中的term是否小于currentTerm if args.Term \u003c rf.currentTerm { //该快照为旧的,直接丢弃并返回 return } DPrintf(\"id[%d].state[%v].term[%d]: 接收到leader[%d]的快照:lastLogIndex[%d],lastLogTerm[%d]\\n\", rf.me, rf.state, rf.currentTerm, args.LeaderId, args.LastIncludedIndex, args.LastIncludedTerm) //2.若参数中term大于currentTerm if args.Term \u003e rf.currentTerm { rf.currentTerm = args.Term rf.voteFor = -1 rf.persist() } //3.重置选举时间 rf.resetElectTimer() //4.转变为follower rf.toFollower() //5.若快照过期 if args.LastIncludedIndex \u003c= rf.commitIndex { DPrintf(\"id[%d].state[%v].term[%d]: leader[%d]的快照:lastLogIndex=[%d],lastLogTerm=[%d]已过期,commitIndex=[%d]\\n\", rf.me, rf.state, rf.currentTerm, args.LeaderId, args.LastIncludedIndex, args.LastIncludedTerm, rf.commitIndex) return } //5.通过applyCh传至service applyMsg := ApplyMsg{ SnapshotValid: true, Snapshot: args.Data, SnapshotIndex: args.LastIncludedIndex, SnapshotTerm: args.LastIncludedTerm, } go func(msg ApplyMsg) { rf.applyCh \u003c- msg }(applyMsg) } 异步传递msg，避免持有锁的时候阻塞，导致死锁了 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:5:3","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现Snapshot func (rf *Raft) Snapshot(index int, snapshot []byte) { // Your code here (2D). rf.mu.Lock() defer rf.mu.Unlock() if index \u003e rf.lastLog().Index || index \u003c rf.logEntries[0].Index || index \u003e rf.commitIndex { return } //1.获取需要压缩末尾日志的数组内索引 realIndex := rf.binaryFindRealIndexInArrayByIndex(index) lastLogEntry := rf.logEntries[realIndex] DPrintf(\"id[%d].state[%v].term[%d]: 安装snapshot:lastIncludedIndex=[%d],lastIncludedTerm=[%d];commitIndex=[%d]\\n\", rf.me, rf.state, rf.currentTerm, lastLogEntry.Index, lastLogEntry.Term, rf.commitIndex) //2.清除log中[1,realIndex]之间的数据 rf.logEntries = append(rf.logEntries[:1], rf.logEntries[realIndex+1:]...) //3.保存三项快照数据 rf.snapshotData = snapshot //4.更改日志占位节点 rf.logEntries[0].Index = lastLogEntry.Index rf.logEntries[0].Term = lastLogEntry.Term //5.持久化 rf.persistStateAndSnapshot() DPrintf(\"id[%d].state[%v].term[%d]: 安装snapshot:lastIncludedIndex=[%d],lastIncludedTerm=[%d] 成功\\n\", rf.me, rf.state, rf.currentTerm, lastLogEntry.Index, lastLogEntry.Term) } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:5:4","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现CondInstallSnapshot func (rf *Raft) CondInstallSnapshot(lastIncludedTerm int, lastIncludedIndex int, snapshot []byte) bool { // Your code here (2D). rf.mu.Lock() defer rf.mu.Unlock() DPrintf(\"id[%d].state[%v].term[%d]: 安装snapshot:lastIncludedIndex=[%d],lastIncludedTerm=[%d];commitIndex=[%d]\\n\", rf.me, rf.state, rf.currentTerm, lastIncludedIndex, lastIncludedTerm, rf.commitIndex) //1.判断快照是否过期 if lastIncludedIndex \u003c= rf.commitIndex { DPrintf(\"id[%d].state[%v].term[%d]:安装 snapshot:lastIncludedIndex=[%d],lastIncludedTerm=[%d]已过期,安装失败\\n\", rf.me, rf.state, rf.currentTerm, lastIncludedIndex, lastIncludedTerm) return false } if rf.lastLog().Index \u003c lastIncludedIndex { //若快照的最后一个log比当前最新的log还晚,那么清空log中除了0位的log rf.logEntries = rf.logEntries[:1] } else { //清除log中[1,realIndex]之间的数据 realIndex := rf.binaryFindRealIndexInArrayByIndex(lastIncludedIndex) rf.logEntries = append(rf.logEntries[:1], rf.logEntries[realIndex+1:]...) } //3.保存快照数据 rf.snapshotData = snapshot //4.更改日志占位节点 rf.logEntries[0].Index = lastIncludedIndex rf.logEntries[0].Term = lastIncludedTerm //5.更新commitIndex和lastAppliedIndex rf.commitIndex = lastIncludedIndex rf.lastApplied = lastIncludedIndex //6.持久化 rf.persistStateAndSnapshot() DPrintf(\"id[%d].state[%v].term[%d]: 安装snapshot:lastIncludedIndex=[%d],lastIncludedTerm=[%d] 成功;commitIndex=[%d]\\n\", rf.me, rf.state, rf.currentTerm, lastIncludedIndex, lastIncludedTerm, rf.commitIndex) return true } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:5:5","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现快照持久化 //保存raft状态和snapshot func (rf *Raft) persistStateAndSnapshot() { w := new(bytes.Buffer) e := labgob.NewEncoder(w) //编码currentTerm err := e.Encode(rf.currentTerm) if err != nil { DPrintf(\"id[%d].state[%v].term[%d]: encode currentTerm error: %v\\n\", rf.me, rf.state, rf.currentTerm, err) return } //编码voteFor err = e.Encode(rf.voteFor) if err != nil { DPrintf(\"id[%d].state[%v].term[%d]: encode voteFor error: %v\\n\", rf.me, rf.state, rf.currentTerm, err) return } //编码log[] err = e.Encode(rf.logEntries) if err != nil { DPrintf(\"id[%d].state[%v].term[%d]: encode logEntries[] error: %v\\n\", rf.me, rf.state, rf.currentTerm, err) return } data := w.Bytes() rf.persister.SaveStateAndSnapshot(data, rf.snapshotData) } ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:5:6","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"实现发送InstallSnap 这一步就是在日志模块中的HandleAppendEntries|()中已经实现，只需要发送的时候判断一下是否需要发送快照即可。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:5:7","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"总结 Lab2算是该课程中最难的一个Lab了，个人前前后后做了由半个月多才达到bugfree(自测2000次无fail)。接下来会继续更新Lab3以及Lab4的实现文档。 ","date":"2022-03-18","objectID":"/mit-6.824-lab2/:6:0","tags":["分布式系统","Raft","Golang","MIT-6.824"],"title":"MIT 6.824 Lab2","uri":"/mit-6.824-lab2/"},{"categories":["分布式系统"],"content":"KV层的Read请求优化 ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:0:0","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"问题 之前我们为了实现线性化的读写，我们将每一个读写操作都封装成Log打入到Raft中，因为Raft可以保证我们的log在多个节点之间是由共识的，不会乱序，因此可以实现线性化的读写操作。 但是我们发现read操作并不需要在状态机中应用，它只是需要读写到目前的最新值，那么如果我们将其放入log中，然后走Raft流程使其可以被线性化的读取。但是一般的场景都是读多写少的情况，如果我们每一个读操作都走一遍log，会导致性能较低。 ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:1:0","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"分析 我们可以参考Raft拓展论文中的第八节的客户端交互部分。 只读的操作可以直接处理而不需要记录日志。但是，在不增加任何限制的情况下，这么做可能会冒着返回脏数据的风险，因为响应客户端请求的领导人可能在他不知道的时候已经被新的领导人取代了。线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点。首先，领导人必须有关于被提交日志的最新信息。领导人完全特性保证了领导人一定拥有所有已经被提交的日志条目，但是在他任期开始的时候，他可能不知道哪些是已经被提交的。为了知道这些信息，他需要在他的任期里提交一条日志条目。Raft 中通过领导人在任期开始的时候提交一个空白的没有任何操作的日志条目到日志中去来实现。第二，领导人在处理只读的请求之前必须检查自己是否已经被废黜了（他自己的信息已经变脏了如果一个更新的领导人被选举出来）。Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题。可选的，领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时间来保证安全性（假设时间误差是有界的）。 总结一下为如下几点: 根据Raft算法，Leader一定拥有当前所有的已经被提交的日志，因此请求到的数据一定是准确的（Leader仍在任期内）。因此可以直接进行读操作。 但是Leader可能已经被替代了，但是自己还不知道，若这种情况直接进行读操作，那么是很有可能读到脏数据的，因此Leader需要确定自己仍然是Leader。 新上线的Leader有可能自己并不知道目前哪些日志已经被提交了（选举Leader的时候，该节点拥有最新的日志，但是可能它在接收到这一批新日志之后，原来的Leader宕机了，导致这些日志虽然存在，但是没有被Leader告知可以提交，也就是没有更新commitIndex），因此需要自己上线后提交一条空日志，这样可以立马发送新的这个空日志，然后更新自己保存的nextIndex数据(在Leader上线时，会更新成commitIndex+1，因此commitindex若不是最新的，会导致nextIndex也不是最新的)，然后我们的updateCommitIndex协程会将commitIndex更新，这时候我们的commitIndex就是最新的了。 ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:2:0","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"ReadIndex Read 若我们需要实现线性化的读。之前的方案是直接写进log里面，即可以保证，该log的index前的日志都可以被读到，但是其实这个log本身是没有作用的，只是为了确保我们可以读到index前的数据。那么我们可以进行优化，我们就不用该log，就直接当读请求来的时候，记录下来当前的commitIndex，当该index在apply协程处被应用的时候，代表我们可以读到index前的最新的数据了，这时候执行读操作，然后返回即可。 因此根据论文和上述分析，我们可以总结为如下流程： 当接收到到一个读请求的时候，先判断该请求是否已经被执行过了，若是则直接返回上次读到的结果。 当接收到到一个读请求的时候，先判断自己认为自己还是不是Leader。(这里只是自己认为，因为实际可能因为网络分区等原因，自己已经不是Leader了) 这时，立马发送一轮心跳，当收到大部分节点的对应响应之后。可以确定目前仍是Leader了。 记录下来当前的commitIndex和其log的term，然后判断该term和leader的当前term是否一致（若是一个新上线leader，但是它的空日志还没提交成功，这时候commitIndex还是旧的，所以就不可以进行ReadIndex Read，需要等到term一致的时候，因为我们提交的空日志的term一定是最新的，和当前leader的term一致），若不一致则继续等待取到一致的，然后进行等待该commitIndex的log被应用到状态机。 状态机执行到该index处其以后的日志的时候，则可以进行读操作，并返回给client。 ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:3:0","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"修改KV层Get/Apply 原先的Get操作和Apply操作（以kvraft/server为例） func (kv *KVServer) Get(args *GetArgs, reply *GetReply) { // Your code here. kv.mu.Lock() defer func() { DPrintf(\"kvserver[%d]: 返回Get RPC请求,args=[%v];Reply=[%v]\\n\", kv.me, args, reply) }() DPrintf(\"kvserver[%d]: 接收Get RPC请求,args=[%v]\\n\", kv.me, args) //1.先判断该命令是否已经被执行过了 if commandContext, ok := kv.clientReply[args.ClientId]; ok { if commandContext.Command \u003e= args.CommandId { //若当前的请求已经被执行过了,那么直接返回结果 reply.Err = commandContext.Reply.Err reply.Value = commandContext.Reply.Value kv.mu.Unlock() return } } kv.mu.Unlock() //2.若命令未被执行,那么开始生成Op并传递给raft op := Op{ CommandType: GetMethod, Key: args.Key, ClientId: args.ClientId, CommandId: args.CommandId, } index, term, isLeader := kv.rf.Start(op) //3.若不为leader则直接返回Err if !isLeader { reply.Err = ErrWrongLeader //kv.mu.Unlock() return } replyCh := make(chan ApplyNotifyMsg, 1) kv.mu.Lock() kv.replyChMap[index] = replyCh DPrintf(\"kvserver[%d]: 创建reply通道:index=[%d]\\n\", kv.me, index) kv.mu.Unlock() //4.等待应用后返回消息 select { case replyMsg := \u003c-replyCh: //当被通知时,返回结果 DPrintf(\"kvserver[%d]: 获取到通知结果,index=[%d],replyMsg: %v\\n\", kv.me, index, replyMsg) if term == replyMsg.Term { reply.Err = replyMsg.Err reply.Value = replyMsg.Value } else { reply.Err = ErrWrongLeader } case \u003c-time.After(500 * time.Millisecond): DPrintf(\"kvserver[%d]: 处理请求超时: %v\\n\", kv.me, op) reply.Err = ErrTimeout } //5.清除chan go kv.CloseChan(index) } func (kv *KVServer) applyCommand(applyMsg raft.ApplyMsg) { kv.mu.Lock() defer kv.mu.Unlock() var commonReply ApplyNotifyMsg op := applyMsg.Command.(Op) index := applyMsg.CommandIndex //当命令已经被应用过了 if commandContext, ok := kv.clientReply[op.ClientId]; ok \u0026\u0026 commandContext.Command \u003e= op.CommandId { DPrintf(\"kvserver[%d]: 该命令已被应用过,applyMsg: %v, commandContext: %v\\n\", kv.me, applyMsg, commandContext) commonReply = commandContext.Reply return } //当命令未被应用过 if op.CommandType == GetMethod { //Get请求时 if value, ok := kv.storeInterface.Get(op.Key); ok { //有该数据时 commonReply = ApplyNotifyMsg{OK, value, applyMsg.CommandTerm} } else { //当没有数据时 commonReply = ApplyNotifyMsg{ErrNoKey, value, applyMsg.CommandTerm} } } else if op.CommandType == PutMethod { //Put请求时 value := kv.storeInterface.Put(op.Key, op.Value) commonReply = ApplyNotifyMsg{OK, value, applyMsg.CommandTerm} } else if op.CommandType == AppendMethod { //Append请求时 newValue := kv.storeInterface.Append(op.Key, op.Value) commonReply = ApplyNotifyMsg{OK, newValue, applyMsg.CommandTerm} } //通知handler去响应请求 if replyCh, ok := kv.replyChMap[index]; ok { replyCh \u003c- commonReply } value, _ := kv.storeInterface.Get(op.Key) DPrintf(\"kvserver[%d]: 此时key=[%v],value=[%v]\\n\", kv.me, op.Key, value) //更新clientReply kv.clientReply[op.ClientId] = CommandContext{op.CommandId, commonReply} DPrintf(\"kvserver[%d]: 更新ClientId=[%d],CommandId=[%d],Reply=[%v]\\n\", kv.me, op.ClientId, op.CommandId, commonReply) kv.lastApplied = applyMsg.CommandIndex //判断是否需要快照 if kv.needSnapshot() { kv.startSnapshot(applyMsg.CommandIndex) } } 更改后 Get func (kv *KVServer) Get(args *GetArgs, reply *GetReply) { // Your code here. kv.mu.Lock() defer func() { DPrintf(\"kvserver[%d]: 返回Get RPC请求,args=[%v];Reply=[%v]\\n\", kv.me, args, reply) }() DPrintf(\"kvserver[%d]: 接收Get RPC请求,args=[%v]\\n\", kv.me, args) //1.先判断该命令是否已经被执行过了 if request, ok := kv.clientReply[args.ClientId]; ok { if request.RequestId \u003e= args.CommandId { //若当前的请求已经被执行过了,那么直接返回结果 reply.Err = request.Err reply.Value = request.Value DPrintf(\"kvserver[%d]: 该Get RPC请求为历史请求,args=[%v],reply=[%v]\\n\", kv.me, args, reply) kv.mu.Unlock() return } } kv.mu.Unlock() //2.发送一轮心跳来检查自己是否还是Leader if !kv.rf.CheckIsLeader() { //不为Leader则返回 reply.Err = ErrWrongLeader return } //3.当前仍为Leader,取当前的commitIndex(一定可以取到和节点term相同的log的commitIndex,由Raft的该方法自行保证) readIndex := kv.rf.GetCommitIndex() DPrintf(\"kvserver[%d]:获取ReadIndex: %d\\n\", kv.me, readIndex) //等待该readIndex被应用到状态机 kv.mu.Lock() var cond *sync.Cond if c, ok := kv.readCon[readIndex]; !ok { //若没有该readIndex的con,则新建一个 lock := \u0026sync.Mutex{} cond = sync.NewCond(loc","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:4:0","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"增加CheckReadIndex方法 我们需要等到状态机至少应用到readIndex的位置的时候才能进行目标读取操作，因此我们可以单独使用一个协程来完成。定期去检查所有的readCon中的readIndex是否小于等于lastApplied，若是则可以直接唤醒所有在该readIndex处等待的Get的handler。 func (kv *KVServer) checkReadIndex() { for !kv.killed() { //检查是否有可以返回的readIndex了 kv.mu.Lock() for readIndex, cond := range kv.readCon { if readIndex \u003c= kv.lastApplied { DPrintf(\"kvserver[%d]: 检查到ReadIndex: %v,lastApplied: %v,因为通知更新返回get结果\\n\", kv.me, readIndex, kv.lastApplied) cond.Broadcast() } } kv.mu.Unlock() time.Sleep(100 * time.Millisecond) } } ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:5:0","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"Raft改动 我们为了实现ReadIndex特性，需要对之前我们的raft模块进行更改。主要是为了实现一个可以检验当前是否仍然是leader的方法以及一个获取当前最新的commitIndex方法，以及leader一上线立马追加一个空日志用于及时更新到最新的commitIndex。 ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:6:0","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"上线Start一个空日志 我们为了领袖能够迅速将自己的commitIndex更新为目前集群中最新的commitIndex，因此需要一上线start一条空日志。为什么会出现领袖并没有当前的最新的commitIndex呢，其实可以举个例子。 一开始A节点为领袖(当前所有节点的commitIndex都为3) 服务器Id Index/Term A(leader) 1/1 2/1 3/1 B 1/1 2/1 3/1 C 1/1 2/1 3/1 D 1/1 2/1 3/1 E 1/1 2/1 3/1 然后现在领袖A接收到两条新日志，并且将其发送给其他节点，但是这时候A宕机了。导致当前集群中超过一半的节点拥有索引为4和5的日志，但是commitIndex并没有更新。(commitIndex是需要领袖去通知跟随者更新的) 服务器Id Index/Term A(leader，已宕机) 1/1 2/1 3/1 4/1 5/1 B 1/1 2/1 3/1 4/1 5/1 C 1/1 2/1 3/1 4/1 5/1 D 1/1 2/1 3/1 4/1 5/1 E 1/1 2/1 3/1 4/1 5/1 此时大家的commitIndex仍为3，假如B选举当选了新的领袖 服务器Id Index/Term A(已宕机) 1/1 2/1 3/1 4/1 5/1 B(leader) 1/1 2/1 3/1 4/1 5/1 C 1/1 2/1 3/1 4/1 5/1 D 1/1 2/1 3/1 4/1 5/1 E 1/1 2/1 3/1 4/1 5/1 那么这时候领袖的commitIndex就不是该集群中实际的可被提交的最高日志索引了，因此需要快速更新commitIndex的话，可以一上线提交一条空日志。 空日志索引为6任期为2，在一轮AppendEntries后C D E都成功复制了该日志，此时B根据规则可以更新commitIndex为6，并且此时该commitIndex处的日志的任期也和当前领袖的任期相同。 服务器Id Index/Term A(已宕机) 1/1 2/1 3/1 4/1 5/1 B(leader) 1/1 2/1 3/1 4/1 5/1 6/2 C 1/1 2/1 3/1 4/1 5/1 6/2 D 1/1 2/1 3/1 4/1 5/1 6/2 E 1/1 2/1 3/1 4/1 5/1 6/2 在下一轮AppendEntires后，根据规则，commitIndex = min(leaderCommit, last log index)，C D E也正确更新commitIndex为6了 这就是为什么需要新领袖一上线立马提交一条空白日志，因为这样可以快速的将每个服务器的commitIndex更新到最新 ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:6:1","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"检验自己仍是Leader 该函数由KV层调用，用于在请求读的时候检验自己是否仍然是leader。通过发送一批appendEntries，并且判断是否过半的follower仍然认为我是leader CheckIsLeader方法 // CheckIsLeader 检查当前是否仍是leader func (rf *Raft) CheckIsLeader() (isLeader bool) { //发送一轮广播 rf.mu.Lock() defer rf.mu.Unlock() defer func() { DPrintf(\"id[%d].state[%v].term[%d]: 检查目前是否为Leader: %v\\n\", rf.me, rf.state, rf.currentTerm, isLeader) }() DPrintf(\"id[%d].state[%v].term[%d]: 开始检查自己是否为leader\\n\", rf.me, rf.state, rf.currentTerm) if rf.state != LEADER { return false } cond := sync.NewCond(\u0026rf.mu) ch := make(chan bool, 1) go rf.BoardCastOneRound(cond, ch) cond.Wait() DPrintf(\"id[%d].state[%v].term[%d]: 检查Leader协程被唤醒: %v\\n\", rf.me, rf.state, rf.currentTerm, isLeader) return \u003c-ch } BroadcastOneRound 该函数发送一轮appendEntries来判断是否过半节点认为自己仍是leader。 // BroadcastOneRound 发起广播发送AppendEntries RPC func (rf *Raft) BroadcastOneRound(cond *sync.Cond, ch chan bool) { rf.mu.Lock() wg := \u0026sync.WaitGroup{} var successNums int64 successNums = 1 if rf.state == LEADER { DPrintf(\"id[%d].state[%v].term[%d]: 开始一轮检验leader广播\\n\", rf.me, rf.state, rf.currentTerm) for i := range rf.peers { if i != rf.me { wg.Add(1) go func(server int) { DPrintf(\"server = %v\\n\", server) if rf.HandleAppendEntries(server) { atomic.AddInt64(\u0026successNums, 1) DPrintf(\"id[%d].state[%v].term[%d]: 节点%v 同意本节点仍为leader\\n\", rf.me, rf.state, rf.currentTerm, server) } wg.Done() }(i) } } } rf.mu.Unlock() //等待所有的返回 wg.Wait() //广播完,通知正在等待的CheckIsLeader协程 if cond != nil { DPrintf(\"id[%d].state[%v].term[%d]: 通知checkIsLeader协程,successNums: %v\\n\", rf.me, rf.state, rf.currentTerm, successNums) cond.Signal() ch \u003c- successNums \u003e int64(len(rf.peers)/2) } } ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:6:2","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"获取最新的CommitIndex 由处理读请求的协程来调用，返回当前leader的最新commitIndex。因为有可能leader刚上线，这时候的commitIndex不是最新的，而且发的空日志也还没完成追加，导致这时候的commitIndex是旧的，因此我们需要等待，直到我们取到的commitIndex处的log的任期和当前相同即可返回。 // GetCommitIndex 返回最新的commitIndex func (rf *Raft) GetCommitIndex() int { rf.mu.Lock() defer rf.mu.Unlock() for rf.index(rf.commitIndex).Term != rf.currentTerm { rf.applyCond.Wait() } DPrintf(\"id[%d].state[%v].term[%d]: 当前的commitIndex: %d\\n\", rf.me, rf.state, rf.currentTerm, rf.commitIndex) return rf.commitIndex } ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:6:3","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["分布式系统"],"content":"总结 其实还有继续优化的空间，比如说PingCap提到的LeaseRead，使用租约的形式来简化读请求，因为我们可以设定一个租约，它是比选举超时的时间要小的，既可以保证在这个租约内，不会发送leader变更，因此可以直接省略掉向集群内节点发送appendEntries来确定自己是否是leader的步骤。 由于这个实现会较为复杂，而且该lab的架构不太适合进行这个优化，因此我只实现了ReadIndex的优化策略。 ","date":"2022-03-10","objectID":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/:7:0","tags":["分布式系统","Raft","KV","Golang","MIT-6.824"],"title":"KV层的Read请求优化","uri":"/kv%E5%B1%82%E7%9A%84read%E8%AF%B7%E6%B1%82%E4%BC%98%E5%8C%96/"},{"categories":["算法"],"content":"Java实现生产者消费者模型 ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:0:0","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"前言 什么是生产者消费者模型？ 简单来讲就是有两种线程，分别称为生产者线程和消费者线程。生产者线程生产出\"产品\"放置到公共的一个队列中，然后消费者线程从队列中去取该\"商品\"。这就是该模型的简单描述。 异步和解耦 该模型实现了生产者和消费的异步和解耦。生产者只需要生产出\"产品\"放到不满的队列中就可以，并不需要关心是谁来消费，可能是小红可能是大黄，也不需要等待消费者消费完了之后再接着生产。消费者也不需要关心\"产品\"是谁生产的，只要队列里面有\"产品\"，我就可以拿去用，不用等生产者一个个生产。 那该怎么实现？ 总结一下该模型的几点需要实现的点： 生产者和消费者线程需要通信 需要一个线程安全的队列来存放\"产品\" 那么我们可以根据所学过的线程间通信的方法来选择 线程通信方式 volatile关键字保证共享变量在线程间的可见性 基于synchronized锁的wait/notify的等待通知机制 基于AQS并发包的ReentrantLock等并发工具 管道流 保证队列线程安全 使用线程不安全的队列，但是对其访问进行加锁 使用线程安全的队列 那么可以总结出以下几种方法 基于synchronized锁的wait/notify的等待通知机制 + 线程不安全的队列 基于AQS并发包的Lock和Condition的条件等待机制 + 线程不安全的队列 基于BlockingQueue阻塞队列的入列和出列机制(BlockingQueue本身是线程安全的) ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:1:0","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"synchronized 先讲基于synchronized的等待通知机制。 synchronized锁 当我们对queue加上synchronized锁之后，我们调用方法queue.wait就会在该锁的一个叫做waitSet的等待队列上进行等待，直到有别的线程调用notify/notifyAll(notify则进行随机唤醒)。那么我们就可以得到如下的流程图。 使用synchronized锁的生产者消费者模型image-20220109211436965 \"\r使用synchronized锁的生产者消费者模型\r 每个线程大致的步骤是： 线程运行到获取的锁的位置 尝试获取锁，若获取失败，则在同步队列中继续获取，回到步骤1。若成功则拿到锁则到步骤3。 获取锁之后，尝试将产品加入到queue/从queue拿出产品。若不符合条件(生产者发现队列满了，消费者发现队列为空)，则加入到该锁的等待队列waitSet中，并且让出该锁，到步骤4。若符合条件则正常操作，并且唤醒所有等待在该锁的waitSet上的线程，也就是notifyAll，并且让出该锁、回到步骤1。 在waitSet等待唤醒，若被唤醒则从到锁的同步队列中继续尝试获取锁，若获取成功则到直接到操作队列步骤。失败则继续尝试获取锁。 ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:2:0","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"代码实例 废话不多说，直接上代码。 定义 生产者：新手程序员 消费者：老手程序员 产品：Bug 对立：系统 新手程序员作为Bug的生产者来生产Bug到系统中，由老手程序员作为消费者来从系统中找到Bug并修复 Bug类 /** * @author TheR1sing3un * @date 2022/1/9 17:18 * @description Bug类 */ public class Bug { private Integer bugId; public Bug(Integer bugId){ this.bugId = bugId; } public Integer getBugId() { return bugId; } } Producer类 import java.util.Queue; import java.util.Random; /** * @author TheR1sing3un * @date 2022/1/9 17:03 * @description */ public class Producer extends Thread { private String name; private int maxSize; private Queue\u003cBug\u003e queue; /** * 构造方法 * @param name * @param queue * @param maxSize */ public Producer(String name,Queue\u003cBug\u003e queue,int maxSize){ this.name = name; this.maxSize = maxSize; this.queue = queue; } /** * 重写run方法,来不断生产bug到队列中 */ @Override public void run() { Random random = new Random(); while(true){ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } //生产者不断生产bug Bug bug = this.produceBug(random.nextInt()); //模拟耗时操作 //加锁,避免并发问题 synchronized (this.queue){ while(queue.size() == maxSize){ //当前的队列已满,无法将bug放进去,那么就等待,直到被唤醒(消费者会来唤醒的) try { System.out.println(\"[\" + name + \"]: 当前系统的Bug达到上限,歇会儿,不满的时候跟俺说一声~\"); queue.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } //现在可以把bug放进去了 queue.add(bug); System.out.println(\"[\" + name + \"]: 俺往系统里面放了一个Bug\" + bug.getBugId() + \",嘿嘿~嘿嘿嘿~\"); //唤醒在睡眠的消费者(当时那些消费者消费的时候发现队列为空,就sleep去了,现在我刚放进去一个bug,队列肯定不为空,所以唤醒他们) queue.notifyAll(); } } } /** * * 生产者生产一个带编号的Bug(可真是和我一模一样呢) * @param i * @return */ public Bug produceBug(int i){ Bug bug = new Bug(i); return bug; } } Cunsumer类 import java.util.Queue; /** * @author TheR1sing3un * @date 2022/1/9 17:32 * @description 消费者 */ public class Consumer extends Thread{ private String name; private Queue\u003cBug\u003e queue; public Consumer(String name, Queue\u003cBug\u003e queue) { this.name = name; this.queue = queue; } /** * 重写run方法,消费者从队列中拿Bug,然后去修复(消费者就是修复Bug的可怜程序员) */ @Override public void run() { while (true){ //操作队列就要上锁! synchronized (queue){ while (queue.isEmpty()){ //当队列中没有Bug时,就等待 try { System.out.println(\"[\" + name + \"]: 这系统做的可以,咋没Bug,整挺好,我摸鱼去了,有Bug叫我\"); queue.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } //有bug时取出 Bug bug = queue.poll(); //模拟耗时操作 try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } fixBug(bug); //唤醒生产者线程 queue.notifyAll(); } } } /** * 消费者修复Bug * @param bug */ public void fixBug(Bug bug){ System.out.println(\"[\" + name + \"]: 修复了Bug\" + bug.getBugId()); } } 测试 import java.util.LinkedList; import java.util.Queue; /** * @author TheR1sing3un * @date 2022/1/9 17:41 * @description */ public class Test { public static void main(String[] args) { Queue\u003cBug\u003e queue = new LinkedList\u003c\u003e(); int maxSize = 3; for (int i = 0; i \u003c 5; i++) { new Producer(\"新手程序员\"+i+\"号\",queue,maxSize).start(); } for (int i = 0; i \u003c 2; i++) { new Consumer(\"老手程序员\"+i+\"号\",queue).start(); } } } 测试截图如下： ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:2:1","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"AQS并发包 方案二采取AQS并发包中的Lock和Condition的条件等待来实现。 加锁步骤和方案一差不多，只需要将queue.wait改成相应的条件等待，以及唤醒改成条件唤醒即可 生产者 生产者尝试往队列里面放的时候若队列是满的，则在条件变量noFull上等队列不为满的时候。 若生产者成功将元素放进队列，那么此时队列一定不为空，所以唤醒在noEmpty条件变量上等待的消费者。 消费者 消费者尝试从队列里面取的时候若队列为空，则在条件变量noEmpty上等到队列不为空的时候。 若消费者成功从队列取到元素，那么此时队列一定不是满的，所以唤醒在noFull条件变量上等待的生产者。 ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:3:0","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"代码实例 Bug类 package ProAndCon; /** * @author TheR1sing3un * @date 2022/1/9 17:18 * @description Bug类 */ public class Bug { private Integer bugId; public Bug(Integer bugId){ this.bugId = bugId; } public Integer getBugId() { return bugId; } } Producer类 package ProAndCon; import javax.security.auth.login.Configuration; import java.util.Queue; import java.util.Random; import java.util.concurrent.locks.Condition; import java.util.concurrent.locks.Lock; /** * @author TheR1sing3un * @date 2022/1/9 22:03 * @description 生产者类 */ public class Producer extends Thread{ private String name; private int maxSize; private Queue\u003cBug\u003e queue; private Lock lock; private Condition noFull; private Condition noEmpty; /** * 构造方法 * @param name * @param queue * @param maxSize */ public Producer(String name, Queue\u003cBug\u003e queue, int maxSize, Lock lock, Condition noFull, Condition noEmpty){ this.name = name; this.maxSize = maxSize; this.queue = queue; this.lock = lock; this.noFull = noFull; this.noEmpty = noEmpty; } /** * 重写run方法,来不断生产bug到队列中 */ @Override public void run() { Random random = new Random(); while(true){ try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } //生产者不断生产bug Bug bug = this.produceBug(random.nextInt()); //模拟耗时操作 //加锁,避免并发问题 lock.lock(); try{ while(queue.size() == maxSize){ //当前的队列已满,无法将bug放进去,那么就等待,直到被唤醒(消费者会来唤醒的) try { System.out.println(\"[\" + name + \"]: 当前系统的Bug达到上限,歇会儿,不满的时候跟俺说一声~\"); //在noFull条件上等待,等待不为满的时候将其唤醒 noFull.await(); } catch (InterruptedException e) { e.printStackTrace(); } } //现在可以把bug放进去了 queue.add(bug); System.out.println(\"[\" + name + \"]: 俺往系统里面放了一个Bug\" + bug.getBugId() + \",嘿嘿~嘿嘿嘿~\"); //唤醒在睡眠的消费者(当时那些消费者消费的时候发现队列为空,就在noEmpty条件变量上等待,现在我刚放进去一个bug,队列肯定不为空,所以唤醒他们) noEmpty.signalAll(); }finally { //解锁 lock.unlock(); } } } /** * * 生产者生产一个带编号的Bug(可真是和我一模一样呢) * @param i * @return */ public Bug produceBug(int i){ Bug bug = new Bug(i); return bug; } } Consumer类 package ProAndCon; import java.util.Queue; import java.util.concurrent.locks.Condition; import java.util.concurrent.locks.Lock; /** * @author TheR1sing3un * @date 2022/1/9 17:32 * @description 消费者 */ public class Consumer extends Thread{ private String name; private Queue\u003cBug\u003e queue; private Lock lock; private Condition noFull; private Condition noEmpty; public Consumer(String name, Queue\u003cBug\u003e queue, Lock lock, Condition noFull, Condition noEmpty) { this.name = name; this.queue = queue; this.lock = lock; this.noEmpty = noEmpty; this.noFull = noFull; } /** * 重写run方法,消费者从队列中拿Bug,然后去修复(消费者就是修复Bug的可怜程序员) */ @Override public void run() { while (true){ //操作队列就要上锁! lock.lock(); try{ while (queue.isEmpty()){ //当队列中没有Bug时,就等待 try { System.out.println(\"[\" + name + \"]: 这系统做的可以,咋没Bug,整挺好,我摸鱼去了,有Bug叫我\"); //当前队列为空,需要等待不为空的时候继续,于是在noEmpty上等待 noEmpty.await(); } catch (InterruptedException e) { e.printStackTrace(); } } //有bug时取出 Bug bug = queue.poll(); //模拟耗时操作 try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } fixBug(bug); //唤醒在noFull上等待的生产者,因为刚刚该线程才消费了,目前肯定不满了 noFull.signalAll(); }finally { //解锁 lock.unlock(); } } } /** * 消费者修复Bug * @param bug */ public void fixBug(Bug bug){ System.out.println(\"[\" + name + \"]: 修复了Bug\" + bug.getBugId()); } } Test测试 package ProAndCon; import java.util.LinkedList; import java.util.Queue; import java.util.concurrent.locks.Condition; import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; /** * @author TheR1sing3un * @date 2022/1/9 17:41 * @description */ public class Test { public static void main(String[] args) { Queue\u003cBug\u003e queue = new LinkedList\u003c\u003e(); Lock lock = new ReentrantLock(); Condition noFull = lock.newCondition(); Condition noEmpty = lock.newCondition(); int maxSize = 5; for (int i = 0; i \u003c 10; i++) { new Producer(\"新手程序员\"+i+\"号\",queue,maxSize,lock,noFull,noEmpty).start(); } for (int i = 0; i \u003c 10; i++) { new Consumer(\"老手程序员\"+i+\"号\",queue,lock,noFull,noEmpty).start(); } } } 测试截图： ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:3:1","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"BlockingQueue BlockingQueue，人称阻塞队列，是一个线程安全的容器，而且其内部结合了AQS的Lock和Condition，来保证从中取的时候，如果没有元素会被阻塞直到不为空；往里面放的时候，如果队列满了也会阻塞直到不满。 那这就简单了，我们就只需要从这个容器取/放，其他的交给它来。 ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:4:0","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"代码实例 Bug类(不再贴了) Producer类 package ProAndCon2; import ProAndCon.Bug; import java.util.Random; import java.util.concurrent.BlockingDeque; /** * @author TheR1sing3un * @date 2022/1/9 22:58 * @description */ public class Producer extends Thread{ private String name; private BlockingDeque blockingDeque; public Producer(String name, BlockingDeque blockingDeque){ this.name = name; this.blockingDeque = blockingDeque; } /** * 重写run方法,来不断生产bug到队列中 */ @Override public void run() { Random random = new Random(); while(true){ try { //模拟耗时操作 Thread.sleep(new Random().nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } //生产者不断生产bug ProAndCon.Bug bug = this.produceBug(random.nextInt()); //往阻塞队列里面放 try { blockingDeque.put(bug); System.out.println(\"[\" + name + \"]: 俺往系统里面放了一个Bug\" + bug.getBugId() + \",嘿嘿~嘿嘿嘿~\"); } catch (InterruptedException e) { e.printStackTrace(); } } } /** * * 生产者生产一个带编号的Bug(可真是和我一模一样呢) * @param i * @return */ public ProAndCon.Bug produceBug(int i){ ProAndCon.Bug bug = new Bug(i); return bug; } } Consumer类 package ProAndCon2; import ProAndCon.Bug; import java.util.Random; import java.util.concurrent.BlockingDeque; /** * @author TheR1sing3un * @date 2022/1/9 23:00 * @description */ public class Consumer extends Thread{ private String name; private BlockingDeque\u003cBug\u003e blockingDeque; public Consumer(String name, BlockingDeque blockingDeque) { this.name = name; this.blockingDeque = blockingDeque; } /** * 重写run方法,消费者从队列中拿Bug,然后去修复(消费者就是修复Bug的可怜程序员) */ @Override public void run() { while (true){ //从阻塞队列中取bug Bug bug = null; try { bug = blockingDeque.take(); } catch (InterruptedException e) { e.printStackTrace(); } try { //模拟耗时操作 Thread.sleep(new Random().nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } fixBug(bug); } } /** * 消费者修复Bug * @param bug */ public void fixBug(Bug bug){ System.out.println(\"[\" + name + \"]: 修复了Bug\" + bug.getBugId()); } } Test测试 package ProAndCon2; import java.util.concurrent.BlockingDeque; import java.util.concurrent.LinkedBlockingDeque; /** * @author TheR1sing3un * @date 2022/1/9 17:41 * @description */ public class Test { public static void main(String[] args) { BlockingDeque\u003cBug\u003e blockingDeque = new LinkedBlockingDeque\u003c\u003e(10); for (int i = 0; i \u003c 10; i++) { new Producer(\"新手程序员\"+i+\"号\",blockingDeque).start(); } for (int i = 0; i \u003c 10; i++) { new Consumer(\"老手程序员\"+i+\"号\",blockingDeque).start(); } } } 测试截图： ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:4:1","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"总结 上述就是三种实现简易的生产者消费者模型的方法，实际中比较少用Java中的生产者消费者模型，更多的使用消息队列来当作一种生产者消费者模型。主要是理解这个多线程间通信和生产者消费者模式的特性。 ","date":"2022-01-09","objectID":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/:5:0","tags":["Java","多线程","面试"],"title":"Java实现生产者消费者模型","uri":"/java%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"},{"categories":["算法"],"content":"LRU算法原理及实现 ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:0:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"前言 什么是LRU算法？ LRU（Least recently used，最近最少使用）算法根据数据的历史访问记录来进行淘汰数据，其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。 那么该数据结构就是当存储队列到达上限时，清除的是最久未被访问的节点，该节点一般认为是最可能无用的节点，保留下来的是最近都有使用过的节点，因此可以实现对\"有用\"数据的最大程度保留。 LRU算法应用场景？ LRU算法有许多的应用场景。 Redis中使用LRU来进行淘汰 操作系统底层的内存管理，比如说页面置换算法中的LRU算法 业务处理，比如说做一个用户最近10个浏览记录，那么就可以使用LRU算法来维护一个大小为10的LRU队列 ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:1:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"原理 LRU算法需要实现如下特性 实现get/put方法(都为O(1)的时间复杂度) 每次get时需要将访问的节点提前至队首 每次put需要判断队列是否已满，满了则将最后的节点删除，并且将该节点放至队首，不满则直接放队首 基于上述特性需要实现如下数据结构 首先需要实现队列，如果使用单向链表，当我们需要使用删除操作时，需要获得前置节点的指针，单向链表则不能做到直接获取。因此使用双向链表。 又我们需要get方法达到O(1)的时间复杂度，因此需要一个Hashmap，可以根据key定位到我们双向链表的Node节点。 由于我们HashMap中有key，所以我们可不可以Node中只存value，其实是不可以的，后续会提到这个原因。 因此我们实现了如下数据结构 双向链表+HashMap的数据结构HashLinkedList \"\r双向链表+HashMap的数据结构\r 缓存淘汰过程如下 LRU缓存淘汰过程LRU算法缓存淘汰策略- Mr.Ming2 - 博客园 \"\rLRU缓存淘汰过程\r ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:2:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"实现 那么现在我们就开始实现一个简易的LRU算法 首先需要实现一个Node节点 public class Node { //Node中存键值对 public int key,val; //前置节点和后置节点 public Node next,pre; public Node (int key,int val){ this.key = key; this.val = val; } } 接下来实现一个关于Node节点的双向链表以及相关方法 需要有头尾节点 有大小size 实现addFirst()/removeLast()/remove()/size()方法 /** * @author TheR1sing3un * @date 2021/12/30 11:14 * @description 双向链表数据结构实现 */ public class DoubleList { //头节点和尾节点 private Node head,tail; //大小 private int size; /** * 构造方法并且完成首尾节点初始化 */ public DoubleList (){ head = new Node(0,0); tail = new Node(0,0); head.next = tail; tail.pre = head; } /** * 添加一个节点到队首 * @param node */ public void addFirst(Node node){ node.pre = head; node.next = head.next; head.next.pre = node; head.next = node; size++; } /** * 删除最后一个节点 * @return 返回被删除的节点 */ public Node removeLast(){ //没有节点时候,返回null if (head.next == tail) return null; Node last = tail.pre; last.pre.next = tail; tail.pre = last.pre; size--; return last; } /** * 删除某个节点 * @param node * @return 返回被删除的节点 */ public Node remove(Node node){ node.pre.next = node.next; node.next.pre = node.pre; size--; return node; } /** * 获取当前链表大小 * @return 链表大小 */ public int size(){ return size; } /** * 打印该链表 */ public void print(){ Node cur = head.next; while(cur != tail){ System.out.print(cur.key+\"-\u003e\"+ cur.val+\" \"); cur = cur.next; } System.out.println(); } } 接下来实现LRU队列 需要有一个HashMap完成key-\u003eNode的映射 需要有一个DoubleList来存放Node节点 有容量上限cap 使用私有的delete()/removeLast()/makeFirst()/addFirst()来辅助put()和get()方法，避免直接操作node 有put()/get()/size()方法 /** * @author TheR1sing3un * @date 2021/12/30 11:49 * @description LRU队列实现 */ public class LRUCache { //key-\u003enode的映射 private HashMap\u003cInteger,Node\u003e map; //双向链表作为缓存队列 private DoubleList cache; //最大容量 private int cap; //构造方法,并且完成map和list的初始化 public LRUCache(int cap){ this.cap = cap; map = new HashMap\u003cInteger,Node \u003e(); cache = new DoubleList(); } /** * 删除某个key对于的键值对 * @param key */ private void delete(int key){ //从map中获取该节点 Node node = map.get(key); //从链表删除该节点 cache.remove(node); //从map中删除该key map.remove(key); } /** * 删除缓存队列中最后一个节点,也就是最久未被使用的节点 */ private void removeLast(){ //从链表中删除该节点 Node node = cache.removeLast(); //根据该节点的key来删除map中对应的键值对 map.remove(node.key); //此处也就是为什么我们需要Node中存key的原因,因为需要根据key来删除map中的键值对 } /** * 将某个键提前到队首,也就是变成最近使用的节点 * @param key */ private void makeFirst(int key){ //从map中获取该节点 Node node = map.get(key); //删除该节点 cache.remove(node); //将该节点重新插入到队首 cache.addFirst(node); } /** * 添加一个键值对到队首,也就是到最近使用的位置 * @param key * @param val */ private void addFirst(int key,int val){ Node node = new Node(key,val); //添加到队首 cache.addFirst(node); //添加映射 map.put(key,node); } /** * 暴露出来的put()方法,向LRUCache中插入一个键值对,若key已存在则更新 * @param key * @param val */ public void put(int key,int val){ if (map.containsKey(key)){ //若key已存在,那么需要先删除旧数据 delete(key); //插入到队首 addFirst(key,val); return; } //当达到容量上限时 if(cache.size() == cap){ //删除最久未被使用的节点 removeLast(); } //添加到队首 addFirst(key,val); } /** * 暴露出来的get()方法,根据key获取到val,若不存在,则返回-1(假定val都为正整数) * @param key */ public int get(int key){ if (!map.containsKey(key)){ //key不存在 return -1; } //将该key移动到队首,也就是最近使用的第一个 makeFirst(key); //返回值　 return map.get(key).val; } /** * 暴露出来的size()方法,返回当前的大小 * @return 返回当前队列大小 */ public int size(){ return cache.size(); } /** * 打印缓存队列 */ public void print(){ cache.print(); } } ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:3:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"测试 public class Test { public static void main(String[] args) { LRUCache lruCache = new LRUCache(5); Scanner sc = new Scanner(System.in); int key,val; while(true){ System.out.println(\"请输入插入的key和val(以空格隔开,输入-1则结束)\"); key = sc.nextInt(); if (key == -1) break; val = sc.nextInt(); lruCache.put(key,val); lruCache.print(); } while(true){ System.out.println(\"请输入需要获取的key(输入-1则结束)\"); key = sc.nextInt(); if (key == -1) break; System.out.println(\"key-\u003eval: \"+key+\"-\u003e\"+lruCache.get(key)); lruCache.print(); } } } 自行输入测试image-20211230132907832 \"\r自行输入测试\r 至此，我们就完成了一个简易的LRU算法，除此之外，我们还可以使用Java中自带的api来简化LRU实现 ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:4:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"LRU(使用LinkedHashMap) LinkedHashMap内部数据结构就是一条双向链表+HashMap，因此我们可以不用自己定义这些数据结构，具体实现交给LinkedHashMap，我们只需要处理put()/get()方法即可。 /** * @author TheR1sing3un * @date 2021/12/30 13:35 * @description 使用LinkedHashMap实现的LRU */ public class LRUCacheSimple { private int cap; private LinkedHashMap\u003cInteger,Integer\u003e cache = new LinkedHashMap(); public LRUCacheSimple(int cap){ this.cap = cap; } /** * 将某个键改成最近使用的 * @param key */ private void makeRecently(int key){ //先获取值 Integer val = cache.get(key); //删除该键 cache.remove(key); //重新插入 cache.put(key,val); } /** * 插入key,val * @param key * @param val */ public void put(int key,int val){ if (cache.containsKey(key)){ //如果已存在,那么覆盖 cache.put(key,val); //将key变成最近使用 makeRecently(key); return; } if (cap \u003c= cache.size()){ //当满了之后 //获取第一个key(也就是LinkedHashMap中最久未被访问的节点) Integer first = cache.keySet().iterator().next(); //删除该节点 cache.remove(first); } //将新的key,val插入 cache.put(key,val); } /** * 根据key获取value(假定value都是正整数) * @param key * @return value(-1表示不存在) */ public int get(int key){ if (!cache.containsKey(key)){ return -1; } //将该key变成最近使用的 makeRecently(key); return cache.get(key); } /** * 打印该缓存 */ public void print(){ Iterator\u003cInteger\u003e iterator = cache.keySet().iterator(); while(iterator.hasNext()){ int key = iterator.next(); System.out.print(key+\"-\u003e\"+ cache.get(key)+\" \"); } System.out.println(); } } ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:5:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"LRU(GoLang实现) 上述是Java版本的LRU实现，下述我使用Golang语言进行实现，大致思路和逻辑是相同的。 代码如下 package LRU import \"fmt\" type Node struct { key int val int pre *Node next *Node } type LRUCache struct { Cap int //最大容量 bucket map[int]*Node //HashMap head *Node tail *Node Size int } //LRUCache的构造 func New(cap int) *LRUCache { cache := \u0026LRUCache{ Cap: cap, bucket: make(map[int]*Node, cap), head: \u0026Node{0, 0, nil, nil}, tail: \u0026Node{0, 0, nil, nil}, Size: 0, } cache.head.next = cache.tail cache.tail.pre = cache.head return cache } //添加一个节点到首位(也就是最近一个访问的) func (this *LRUCache) addNodeFirst(node *Node) { //判断是否到容量上限 if this.Size == this.Cap { //到达上限之后,删除最后一个节点 this.deleteNode(this.tail.pre) } //添加节点到首位 node.pre = this.head node.next = this.head.next this.head.next.pre = node this.head.next = node //添加该映射 this.bucket[node.key] = node this.Size++ } //将某个key变成最近使用的(假定该key一定存在) func (this *LRUCache) makeNodeFirst(key int) { //根据key获取该节点 node := this.bucket[key] //先删除该节点 this.deleteNode(node) //再加入该节点到首位 this.addNodeFirst(node) } //删除某个节点 func (this *LRUCache) deleteNode(node *Node) { //先删除映射 delete(this.bucket, node.key) //从双向链表中删除该节点 node.pre.next = node.next node.next.pre = node.pre this.Size-- } //put方法 func (this *LRUCache) Put(key, val int) { //先判断是否已有该节点,有则更新 node := this.bucket[key] if node == nil { //当没有该节点时,直接加入到首位 node := \u0026Node{key, val, nil, nil} this.addNodeFirst(node) } else { //如果已经有该节点,那么先直接更新该节点的值并且提前至首位 node.val = val this.makeNodeFirst(key) } } //get方法,如果不存在则返回-1(假设值都是正数) func (this *LRUCache) Get(key int) int { node := this.bucket[key] if node == nil { //不存在则返回-1 return -1 } else { //将节点提前到首位 this.makeNodeFirst(key) //返回值 return node.val } } func (this *LRUCache) Print() { cur := this.head.next for ; cur != this.tail; cur = cur.next { fmt.Printf(\"%d-\u003e%d \", cur.key, cur.val) } fmt.Println() } 测试代码如下 package LRU import ( \"fmt\" \"testing\" ) func TestLRUCache_New(t *testing.T) { cache := New(5) for i := 1; i \u003c 8; i++ { fmt.Printf(\"加入节点%d\\n\", i) cache.Put(i, 11*i) cache.Print() } fmt.Printf(\"查询节点%d ,值为%d\\n\", 3, cache.Get(3)) cache.Print() fmt.Printf(\"查询节点%d ,值为%d\\n\", 6, cache.Get(6)) cache.Print() fmt.Printf(\"查询节点%d ,值为%d\\n\", 7, cache.Get(7)) cache.Print() } ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:6:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"优化 上述的LRU容器还是一个根本不能投入生产使用的玩具级实现，可以在进一步进行优化。 值的类型 上述的实现我们都是默认value是int，而且是正整数的int，然而生产中，不应该使用固定的value值。Java中应该使用泛型，GoLang中可以使用interface。 最大容量 上述我们的容器最大容量的单位是键值对的个数，这是不太合理的，因为实际中我们应该限制的是缓存占用大小，因此可以将最大限制改成byte为单位，而且需要对淘汰算法进行优化，这时候我们可能超出容量后，需要淘汰的不止是一个缓存，可以是多个，直到当前已用内存小于最大内存。 并发安全 上述我们写的只能在单线程下使用，没有考虑到并发问题，那么其实只需要对每次链表和队列的写查进行相应的加锁即可。Java可以使用synchronized关键字也可以使用ReentrantLock/ReentrantReadWriteLock来对其加锁。GoLang可以使用标准库的sync.Mutex来加锁。 其他 我们这次写的是每次都更新到首位的LRU，称为lru-1，也有lru-k的方式，这个需要根据情况进行优化。 ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:7:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"优化后代码 针对上述的几个问题，我写了一个GoLang的优化后的LRU算法(仍然是lru-1) package lru import ( \"container/list\" \"fmt\" ) type Cache struct { //缓存队列最大缓存大小 maxBytes int64 //缓存队列目前已经使用大小 usedBytes int64 //链表 cacheList *list.List //map映射key-\u003eElement cacheMap map[string]*list.Element //删除记录时的回调函数 OnEvicted func(key string, value Value) } //键值对类型 type entry struct { key string value Value } //缓存的值的类型 type Value interface { //返回Value的内存大小 Len() int } //实例化函数 func New(maxBytes int64, onEvicted func(string, Value)) *Cache { return \u0026Cache{ maxBytes: maxBytes, usedBytes: 0, cacheList: list.New(), cacheMap: make(map[string]*list.Element), OnEvicted: onEvicted, } } //查找 func (c *Cache) Get(key string) (Value, bool) { //先从map里查找是否有该值 if element, ok := c.cacheMap[key]; ok { //若有则将该提至最近使用的 c.cacheList.MoveToFront(element) //返回(将element的value强转成自定义的Value类型 e := element.Value.(*entry) return e.value, true } return nil, false } //删除最久未被使用的节点 func (c *Cache) RemoveOldest() { //获取最后一个节点 cur := c.cacheList.Back() if cur != nil { //从list中删除 c.cacheList.Remove(cur) //从map中删除 entry := cur.Value.(*entry) delete(c.cacheMap, entry.key) //更新当前已用内存 c.usedBytes -= int64(len(entry.key)) + int64(entry.value.Len()) //运行回调函数 if c.OnEvicted != nil { c.OnEvicted(entry.key, entry.value) } } } //添加缓存 func (c *Cache) Put(key string, value Value) { //判断当前是否已经有该key if element, ok := c.cacheMap[key]; ok { //已经有该节点,则将该节点更新并移至队首(最近访问) c.cacheList.MoveToFront(element) entry := element.Value.(*entry) //更新当前已用内存(加上当前新增的value大小再减去原本的value大小) c.usedBytes += int64(value.Len()) - int64(entry.value.Len()) //更新值 entry.value = value } else { //若不存在,则添加至队首,并更新map element := c.cacheList.PushFront(\u0026entry{key, value}) //添加至map c.cacheMap[key] = element //更新已用内存 c.usedBytes += int64(len(key)) + int64(value.Len()) } //添加之后判断是否已经超过最大内存(当最大内存不为0时而且已用内存超过最大内存时删除最近未使用的节点) for c.maxBytes != 0 \u0026\u0026 c.maxBytes \u003c c.usedBytes { c.RemoveOldest() } } //获取当前缓存的数据条数 func (c *Cache) Len() int { return c.cacheList.Len() } func (c *Cache) Print() { for cur := c.cacheList.Front(); cur != nil; cur = cur.Next() { kv := cur.Value.(*entry) fmt.Printf(\"%s-\u003e%s \", kv.key, kv.value) } fmt.Println() } package gocache import ( \"github.com/TheR1sing3un/gocache/gocache/lru\" \"sync\" ) type cache struct { //互斥锁 lock sync.Mutex //lru缓存队列 lru *lru.Cache //最大缓存大小 cacheBytes int64 } //缓存put方法 func (c *cache) put(key string, value ByteView) { c.lock.Lock() defer c.lock.Unlock() if c.lru == nil { //懒加载lru c.lru = lru.New(c.cacheBytes, nil) } c.lru.Put(key, value) } //缓存get方法 func (c *cache) get(key string) (value ByteView, ok bool) { c.lock.Lock() defer c.lock.Unlock() if c.lru == nil { //还未初始化(当前肯定没有数据) return } if value, ok := c.lru.Get(key); ok { return value.(ByteView), true } return } ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:7:1","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["算法"],"content":"总结 至此，我们就完成了三种手写LRU算法，不需要过多练习，只需要记住核心思想就是每次get时候我就提前，每次put也要提前，并且需要判断是否满载，若满则删除最后的(最久未被访问的节点)。 ","date":"2021-12-30","objectID":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/:8:0","tags":["算法","Java","面试","GoLang"],"title":"LRU算法原理及实现","uri":"/lru%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/"},{"categories":["杂文"],"content":"我的大二\"秋招\"之旅 ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:0:0","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"前言 昨天看到了室友(友链中第一个，很强的大佬)写了一篇《我的大二\"秋招\"总结》,我也感同身受，也写一篇关于我的大二\"秋招\"滑铁卢记录史 ​ 从今年(2021)五月份开始下定决心走后端开发的道路，到现在也已经学习了有整整7个月了，虽然自己学的时间比较短，但是鉴于每天都花了不少的时间，综合来看，还算达到了自己当初的一个预期吧。 ​ 比较幸运的是自己一路上成长都没有遇到什么太大的坎坷，一路上都有在朋友的建议和个人的规划下慢慢成长。暑假和室友关于寒假去不去实习的问题讨论了很久，我那时候还很犹豫，因为我那时候我才接触后端开发3个月，项目只跟着视频做了个很简单的博客系统，更别谈计算机基础课/八股文/算法了。但是随着我8月份进入了为之工作室之后，感觉自己成长速度快了起来，做了几个小项目，看了不少技术类的书籍，认识了不少厉害的学长，从他们那里学到了许多自己不知道的知识。因此自己也有了尝试去面试的想法，特别是在室友的反复熏陶之下，竟然都敢有面中大厂的心思了。 这是我当时八月份定下来的一些小目标，除了入职其他的已经超出我的预期达到了 这也是我八月份给2021年剩下的四个月定下的目标，没想到我在10月末就完成了许多，剩下的部分如果没有可恶的期末考😣，估计我也可以粗略的完成了吧(算法除外😪) ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:1:0","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"开始投递 ​ 当时间来到十月底，也就是10月30号，特地挑了我生日这天，我和我的室友连麦开始投递了几家公司的实习岗位，那时候我还非常有仪式感的在售货机买了瓶阿萨姆奶茶大牌子。 ​ 当时第一批投递的几家公司image-20211210180259929 \"\r当时第一批投递的几家公司\r ​ 投完之后，我激动了改了自己的QQ个性签名如同小学生的QQ签名IMG_20211210_183502 \"\r如同小学生的QQ签名\r 不出所料，上述的四家公司，我没有一家拥有面试机会，果然还是简历不够优秀啊(也许也是对于大二学生一种天然的不看好) ​ 但是好在我在11.1号，从超强学长yxr那里加上了Momenta的HR小姐姐(确实很漂亮🤣)，于是我就拥有了人生中第一次面试的经历(工作室面试除外，得益于超强学长xpf的放水嘿嘿😁)。 ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:2:0","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"Momenta ​ 11.1上午才加的HR，没想到立马把面试约到了晚上7点，我赶紧一波临时抱佛脚，请教了几个同样面试了该公司并且已经oc的学长，下午便匆匆忙忙去面了。 ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:3:0","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"一面问题 Momenta一面问题 回答思路 学了数据结构吗？讲一讲常见的数据结构 讲了栈、堆、队列，扩展了Java中PriorityQueue的底层数据结构 现在有一颗多叉树，你怎么存进数据库呢？ 用一个字段记录父节点，引出我之前写的项目的多级评论的实现也是一颗多叉树 Java怎么实现生产者消费者模型 1.BlockingQueue 2.sync+wait+notify 3.lock+await+signal 分治算法和动态规划的异同点 都是将一个大问题分成一个个小问题，但是分治算法的小问题之间独立，而动态规划的小问题之间是有关系的 Redis怎么实现分布式锁？ 使用setnx进行加锁，key是锁名，value是一个类似于UUID，时间戳也可以，解锁的时候，写一个lua脚本进行解锁，判断value是不是自己设进去的那个，防止将别人的加的锁给删除了。 持有锁的线程挂掉怎么办？ 设置过期时间，防止死锁。扩展了下，如果是Redis的master拿到锁，但是还没同步给salve就挂掉了，那么这时候就可以被重复加锁了，那么就需要使用RedLock来解决 计网分层模型 OSI七层模型：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层；普遍理解的四层模型：物理接口层、网络层、传输层、应用层 能实习多久？ 三个月以上 学校的课怎么办？ 自学 ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:3:1","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"面试通过 ​ 一面的问题并不难，但是总共只面了29分钟，而且没有问我项目，一度感觉自己凉了，再加上面之前HR说这个面试官是刷人机器，让我一度很紧张。晚上忍不住问了HR，没想到过了，而且只有一面，直接发Offer。🤑 ​ 第二天就开心的接到了HR的电话，并且开心了收到了Offer邮件 Momenta的Offer邮件image-20211211120015672 \"\rMomenta的Offer邮件\r ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:3:2","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"字节跳动 ​ 由于自己还是一路太顺，第一次面试就拿下了Offer，于是有点飘飘然了，就和室友一起开始投了其他的中大厂。百度投了之后，很快就打电话问个人情况，然后就杳无音讯了(本来也不想去百度)。秉承着对工作室学长学姐们的传承精神(好多学长学姐大二都去过字节了🤓)，我对字节十分的向往。字节投了好几个地方，从牛客到实习僧还有内推码，都投了个遍。接下来就开始漫长的等待了。 ​ 11.25那天接到了抖音音乐的电话，确认了下基本情况，还问了问我可以接受转客户端吗。我已经苦苦等了十天，管它那么多，有面试机会我就很知足了，于是就说了可接受。果不其然，在11.29号那天接到了电话，说我通过了简历筛选，是客户端岗位(早知道我就之前直接说不接受了，简历还被卡在抖音音乐这么久，可恶😩)。我直接拒绝，并且请求声音好听的HR小姐姐帮我简历解锁，让我赶紧有机会面下一家。 ​ 于是在12.1号，我接到了字节Data的面试通知，但是我没想到是推荐架构部门的，我一口答应了，并且约到了12.3的下午的面试。当我在12.2号晚上才看到是推荐架构的面试时，我就知道自己已经走错了一大步！☹ 字节的面试通知邮件image-20211210185049771 \"\r字节的面试通知邮件\r ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:4:0","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"一面问题 字节一面面试问题 回答思路 自我介绍 30秒说完，毕竟自己毫无亮点 能实习多久？ 四个月以上 你做的这个SpringCloud项目是自己做的还是学校的项目？ 自己构思然后找了个前端(友链里的🍅)一起做的 你这个项目用Redis做什么了？ 用来完成邮件验证码的过期时间，和前期部分的接口频率限制(zset实现) Redis和Mysql区别？ 一个是关系型一个是Nosql。Redis做分布式更好，Redis可以实现分布式锁和分布式缓存等等，Mysql一般用于更底层的数据存储，而且Redis有Mysql中无法完成的一些功能，比如redis的Geo可以做位置相关的一些功能，bitmap可以做布隆过滤器之类的 算法题:LeetCode：679 自己太菜了，而且第一次视频面试，很紧张，虽然面试官一直耐心提示，但是最后还是没完全写出来 Url从输入到显示的过程 老八股了，但是还是没答的很好，因为很多地方忘了(心理素质不行😓) Tcp/Udp区别分别什么使用场景 可靠性答了点，提到了现在的Udp也可以可靠连接了，基于quic那个。分别的使用场景讲了讲 一面反问问题 面试官回答 部门业务 推荐架构 技术栈 C++居多，也有Golang 还有什么需要改进的? 面试官说觉得我答得可以，作为大二已经很好了，但是还是让继续努力，学校的基础课可以好好学(潜台词：你这么菜就别出来实习了😵) ​ 面完我就知道自己基本凉了，在牛客发了面经，大家也都有说大二的这个程度已经很强了，但是我知道这和年级无关，我也有朋友大二就进字节了(参考友链的Attack204)。自己还是不行，得继续加油。 ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:4:1","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"一面不通过 ​ 经过了一个周末的苦苦等待，哪怕我已经知道了自己挂定了，但是还是抱有一丝希望。 ​ 周一11点就赶快联系了HR，HR立马一个电话过来，我还以为自己有戏了。HR说：一面面试官觉得你时间上不太允许来实习。我大概就知道了这是面试官安慰我呢，其实算法没写出来，基本就没有通过的机会了。(好像大部分人说实习一面出Hard的算法就是找个理由劝退，其实总的来说，不管是啥难度的题，没写出来就是能力不行) ​ 下午便收到了感谢信，然后后面陆续也有几个部门把我捞起来了，但是我觉得还是自己寒假好好学习，再认真准备一下暑假实习吧！ 字节感谢信image-20211210191532264 \"\r字节感谢信\r ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:4:2","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["杂文"],"content":"寒假期望 ​ 由于只有一家Momenta的Offer，然后听说上海那边是比996还猛，我为了身体健康还是选择不去了，而且寒假还是想在家里自己学一学，还可以陪家人过年，毕竟随着成长，以后回家的时间也越来越少了。就算去的话，下学期还得逃课，那么也就没啥时间准备二战字节了。😝 ​ 总的来说我的大二”秋招“之旅就这么结束了，还是有许多不甘，大二的身份让我连见到面试官的机会都基本没有。当然也是自己简历不够优秀。那也别在一次挫折中一蹶不振了，让自己兴奋起来，寒假认真准备，明年再卷土重来！🤩 ​ 寒假还是要给自己定下一些目标的，以防自己疯狂摸鱼。 寒假目标 学习Golang，掌握基本语法和常见框架 写100道算法题，高频题不能落下 产出10篇高质量的技术博客 刷穿字节面经 继续深度学习Mysql 学习操作系统 将八股文反复背背背 好好打球🏀 ","date":"2021-12-09","objectID":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/:5:0","tags":["面试","字节跳动"],"title":"大二\"秋招\"滑铁卢实录","uri":"/%E5%A4%A7%E4%BA%8C%E7%A7%8B%E6%8B%9B%E6%BB%91%E9%93%81%E5%8D%A2%E5%AE%9E%E5%BD%95/"},{"categories":["Spring"],"content":"SpringCould学习 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:0:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"前言 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:1:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"版本选择 SpringCloud:Hoxtom.SR1SpringBoot:2.2.2RELEASECloudAibaba:2.1.0RELEASeJava:1.8Maven:3.5及以上Mysql:5.7及以上 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:1:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"模块构建 新建项目 选择maven创建 选择骨架 修改字符编码 都改成utf-8 注解生效激活 设置java编译版本 File Type过滤(将项目下显示的一些杂七杂八的文件过滤掉) 将src删掉 将pom.xml中加入如下 统一管理jar包版本 \u003c!--统一管理Jar包版本--\u003e \u003cproperties\u003e \u003cproject.build.sourceEncoding\u003eUTF-8\u003c/project.build.sourceEncoding\u003e \u003cmaven.compiler.source\u003e12\u003c/maven.compiler.source\u003e \u003cmaven.compiler.target\u003e12\u003c/maven.compiler.target\u003e \u003cjunit.version\u003e4.12\u003c/junit.version\u003e \u003clombok.version\u003e1.18.10\u003c/lombok.version\u003e \u003clog4j.version\u003e1.2.17\u003c/log4j.version\u003e \u003cmysql.version\u003e5.1.47\u003c/mysql.version\u003e \u003cdruid.version\u003e1.1.16\u003c/druid.version\u003e \u003cmybatis.spring.boot.version\u003e1.3.0\u003c/mybatis.spring.boot.version\u003e \u003c/properties\u003e 编写dependencyManagement 作用: 统一声明版本号 升级时只需该父工程的pom.xml中的版本号就可以处处升级 子模块不用写版本号 如果子模块要使用别的版本，只用自己声明version \u003c!--子模块继承之后，提供作用：锁定版本+子module不用写groupId和version--\u003e \u003cdependencyManagement\u003e\u003c!--定义规范，但不导入--\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-project-info-reports-plugin\u003c/artifactId\u003e \u003cversion\u003e3.0.0\u003c/version\u003e \u003c/dependency\u003e \u003c!--spring boot 2.2.2--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-dependencies\u003c/artifactId\u003e \u003cversion\u003e2.2.2.RELEASE\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cscope\u003eimport\u003c/scope\u003e \u003c/dependency\u003e \u003c!--spring cloud Hoxton.SR1--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-dependencies\u003c/artifactId\u003e \u003cversion\u003eHoxton.SR1\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cscope\u003eimport\u003c/scope\u003e \u003c/dependency\u003e \u003c!--spring cloud 阿里巴巴--\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.alibaba.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-alibaba-dependencies\u003c/artifactId\u003e \u003cversion\u003e2.1.0.RELEASE\u003c/version\u003e \u003ctype\u003epom\u003c/type\u003e \u003cscope\u003eimport\u003c/scope\u003e \u003c/dependency\u003e \u003c!--mysql--\u003e \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003cversion\u003e${mysql.version}\u003c/version\u003e \u003cscope\u003eruntime\u003c/scope\u003e \u003c/dependency\u003e \u003c!-- druid--\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.alibaba\u003c/groupId\u003e \u003cartifactId\u003edruid\u003c/artifactId\u003e \u003cversion\u003e${druid.version}\u003c/version\u003e \u003c/dependency\u003e \u003c!--mybatis--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis.spring.boot\u003c/groupId\u003e \u003cartifactId\u003emybatis-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e${mybatis.spring.boot.version}\u003c/version\u003e \u003c/dependency\u003e \u003c!--junit--\u003e \u003cdependency\u003e \u003cgroupId\u003ejunit\u003c/groupId\u003e \u003cartifactId\u003ejunit\u003c/artifactId\u003e \u003cversion\u003e${junit.version}\u003c/version\u003e \u003c/dependency\u003e \u003c!--log4j--\u003e \u003cdependency\u003e \u003cgroupId\u003elog4j\u003c/groupId\u003e \u003cartifactId\u003elog4j\u003c/artifactId\u003e \u003cversion\u003e${log4j.version}\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/dependencyManagement\u003e 增加热启动插件 \u003c!--热启动插件--\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cfork\u003etrue\u003c/fork\u003e \u003caddResources\u003etrue\u003c/addResources\u003e \u003c/configuration\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e maven中跳过单元测试 父工程创建完成后执行 mvn:install将父工程发布到仓库方便子工程继承 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:2:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"支付模块构建 构建微服务模块统一步骤 建module 改pom 写yml 主启动 业务类 创建支付模块 创建module，继承自父亲模块(maven无骨架创建) 改pom文件 把需要使用的pom文件导入 \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-actuator\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.mybatis.spring.boot\u003c/groupId\u003e \u003cartifactId\u003emybatis-spring-boot-starter\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.alibaba\u003c/groupId\u003e \u003cartifactId\u003edruid-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e1.1.10\u003c/version\u003e \u003c/dependency\u003e \u003c!--mysql-connector-java--\u003e \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003c/dependency\u003e \u003c!--jdbc--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-jdbc\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-devtools\u003c/artifactId\u003e \u003cscope\u003eruntime\u003c/scope\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 改yml 在模块下的resources下新建application.yaml server:port:8001#服务端口号spring:application:name:cloud-payment-service#服务名称datasource:type:com.alibaba.druid.pool.DruidDataSource#当前数据源操作类型driver-class-name:org.gjt.mm.mysql.Driver#mysql驱动包url:jdbc:mysql://${person.mysql.host}:${person.mysql.port}/springcloud?useUnicode-true\u0026charcterEncoding=utf-8\u0026useSSL=falseusername:rootpassword:lcy021030person:mysql:host:49.234.111.177port:3306 主启动类 创建主启动类 业务类 数据库建表 CREATE TABLE `payment` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT 'ID', `serial` varchar(200) DEFAULT '', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 创建实体类 @NoArgsConstructor @AllArgsConstructor @Data public class Payment implements Serializable { private Long id; private String serial; } @Data @AllArgsConstructor @NoArgsConstructor public class CommonResult\u003cT\u003e { public static final Integer SUCCESS = 20000; public static final Integer ERROR = 40000; private Integer code; private String message; private T data; public static CommonResult ok(){ return new CommonResult(SUCCESS,\"success\",null); } public static CommonResult error(){ return new CommonResult(ERROR,\"error\",null); } public CommonResult msg(String message){ this.message = message; return this; } public CommonResult code(int code){ this.code = code; return this; } public CommonResult\u003cT\u003e data(T data){ this.data = data; return this; } } 编写PaymentDao层 编写PaymentMapper 文件头 \u003c?xml version=\"1.0\" encoding=\"UTF-8\" ?\u003e \u003c!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"\u003e \u003cmapper namespace=\"pro.risingsun.springcloud.dao.PaymentDao\"\u003e \u003c/mapper\u003e 写Service 写controller postman自测 创建消费者模块 创建模块 写实体类(Payment和CommonResult) 使用RestTemplate RestTemplate 创建config.ApplicationContexConfig 将RestTemplate注册成bean 完善消费者的OrderConsumer ​ 测试 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:2:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"工程重构 将部分相似的代码进行重构整合 创建新模块cloud-api-commons 写pom 将entites移入 放入本地库 maven:clean maven:install 改造原本的两个模块的公共内容 删掉entities 在各自的pom文件中引入 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:2:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Eureka 一种服务注册中心，使用Eureka的客户端连接到到Eureka Server并维持心跳连接。这样就可以通过Eureka Server来监控系统的各个微服务是否正常运行 EurekaServer 提供注册服务，各个微服务节点通过配置启动后，会从EurekaServer中心进行注册。这样RurekaSever中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观看到 EurekaClient 一个Java客户端，用于简化EurekaServer的交互，客户端同时也具备一个内置的、使用轮询负载算法的负载均衡器。在应用启动后，将会用EurekaServer发送心跳(默认30秒为一个周期)。如果EurekaServer在多个心跳周期内没有接收到某个节点的心跳，EurekaServer就会从服务注册表中把这个服务节点移除(默认90秒) ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"IDEA创建EurekaServer 创建模块 改pom \u003cdependencies\u003e \u003c!--自定义的api通用包--\u003e \u003cdependency\u003e \u003cgroupId\u003epro.risingsun.springcloud\u003c/groupId\u003e \u003cartifactId\u003ecloud-api-commons\u003c/artifactId\u003e \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e \u003c/dependency\u003e \u003c!--eureka-server--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-eureka-server\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-actuator\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 写yaml server:port:7001eureka:instance:hostname:localhost#eureka服务端的实例名称client:register-with-eureka:false#不注册自己fetch-registry:false#false表示自己就是注册中心,不需要去检索自己的服务service-url:#设置与EurekaServer交互的地址查询服务和注册服务都需要依赖这个地址defaultzone:http://${eureka.instance.hostname}:${server.port}/eureka/ 写启动类 ​ 打上注解@EnbaleEurekaServer表示自己是EurekaServer 测试 输入localhost:7001 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"将payment注册进Eureka中 改pom 增加EurekaClient的依赖 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-eureka-client\u003c/artifactId\u003e \u003c/dependency\u003e 改配置 eureka:client:#表示自己注册进EurekaServerregister-with-eureka:true#是否从EurekaServer抓取已有的配置信息,默认为true(单节点无所谓,集群必须设置为true才可以配置ribbon使用负载均衡)fetch-registry:trueservice-url:defaultZone:http://localhost:7001/eureka#EurekeServer的地址 改主启动类 加上注解@EnableEurekaClient 启动测试 这里的应用名称是我们在yaml文件中配置的,如下图: ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"将order注册进Eureka中 步骤如上，基本一致 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:3","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"集群Eureka搭建 互相注册，相互守望 创建cloud-eureka-server7002 参考7001进行创建 修改映射文件 写yaml 互相注册，相互守望 修改7001的配置文件 修改7002的配置文件 主启动类 测试访问eureka7001:7001 测试访问eureka7002:7002 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:4","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"将服务注册进集群 改配置文件 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:5","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"PaymentProvider集群搭建 参考8001搭建8002和8003 修改consumer的服务调用URL 但是这样消费者还是找不到是哪个生产者，因为三个生产者的服务名称都是cloud-payment-service，因此需要开启负载均衡 开启负载均衡 使用@LoadBalances注解赋予RestTemplate负载均衡能力 集群信息完善 增加实例名称 在payment8001和8002的配置文件中增加 增加ip显示 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:6","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"服务发现 对于注册进eureke里面的微服务，可以通过服务发现来获得该服务的信息 修改cloud-provider-payment8001的Controller 主启动类增加注解 测试 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:7","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Eureka自我保护 某时刻某一个微服务不可用了，Eureka不会立刻清理，仍旧会对该微服务的消息进行保存 属于CAP中的AP分支 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:3:8","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"ZooKeeper ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:4:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"创建payment 服务器上启动Zookeeper 新建模块cloud-provider-payment8004 改pom文件 将Eureka的依赖改成zookeeper的 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-zookeeper-discovery\u003c/artifactId\u003e \u003c/dependency\u003e 改配置文件(将Eureka的改成Zookeeper) 主启动类 写Controller 测试localhost:8004/payment/zk ​ ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:4:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"创建order …省略 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:4:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Consul … ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:5:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Ribbon Ribbon是一套实现负载均衡的工具 spring-cloud-starter-netflix-eureka-client已经集成了Ribbon 如何替换负载均衡策略 该配置类不能放在@CompomentScan可以扫到的位置 主启动类增加注解 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:6:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"OpenFeign 用在消费端 创建模块cloud-consumer-feign-order9000 改pom \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-web\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-actuator\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003epro.risingsun.springcloud\u003c/groupId\u003e \u003cartifactId\u003ecloud-api-commons\u003c/artifactId\u003e \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e \u003c/dependency\u003e \u003c!--eureka-client--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-eureka-client\u003c/artifactId\u003e \u003c/dependency\u003e \u003c!--openfeign客户端--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-openfeign\u003c/artifactId\u003e \u003c/dependency\u003e \u003c/dependencies\u003e 改配置文件 server:port:9000spring:application:name:cloud-order-serviceeureka:client:register-with-eureka:truefetch-registry:trueservice-url:defaultZone:http://eureka7001:7001/eureka,http://eureka7002:7002/eureka#EurekeServer的地址(集群版) 注册不注册都可以，消费端本来就不用注册 写主启动类 写服务调用接口 要点： 这里相当于是直接根据服务名和路径去服务端找相应的controller的方法，所以这里的服务名、路径、请求类型要一致才能在服务端找到方法 参数也要保证一致，而且需要打上相应的注解，没有SpringMVC帮忙封装了 负载均衡 自带负载均衡，使用Ribbon ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:7:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"OpenFeign超时控制 OpenFeign默认等待一秒钟 如果超过1秒种，Feign客户端直接报错 修改超时时间(OpenFeign自带Ribbon) ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:7:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"OpenFeign日志 日志级别: NONE：默认的，不显示任何日志 BASIC：仅记录请求方法、URL、相应状态码及执行时间 HEADERS：除了记录BASIC中定义的信息外，还有请求和相应的头信息 FULL：除了HEADERS中定义的信息外，还有请求和相应的正文及元数据 开启日志 创建日志配置类 配置文件中配置日志 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:7:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Hystix ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:8:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"重要概念 服务 降级 fallback 出现情况 程序运行异常 超时 服务熔断出发服务降级 线程池/常量池打满 服务熔断 当达到最大服务访问时，直接拒绝访问，然后调用服务降级 服务限流 限制服务访问 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:8:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"实践 创建生产者 新建模块cloud-provider-hystrix-payment8005 pom文件 \u003c!--hystrix依赖--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-hystrix\u003c/artifactId\u003e \u003c/dependency\u003e 写配置文件 写主启动类 写Service 写Controller 测试 创建消费者 … service，远程调用 controller 服务降级 服务提供方降级 激活服务降级功能 在主启动类上加上注解@EnbaleCircuitBeaker 使用注解@HystrixComman在业务方法上标记 服务消费方降级 改yml 改启动类 业务上处理 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:8:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"全局服务降级 现在的降级方式和业务代码耦合严重，而且重复冗余。那么需要进行全局的一个服务降级配置 @DefaultProerties(defaultFallback = “xxx”)设置默认的服务降级配置 如果某个方法有自己的配置，就用自己的，没有就用默认的 在Feign接口进行统一处理 写一个类实现PaymentFeignService接口，对方法进行重写 配置fallback ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:8:3","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"服务熔断 生产方Service代码如下 在错误了一定次数内失败率达到了设定值，那么进入熔断状态，然后在一定时间后(默认是5秒)会放出几个请求进去，如果还是错误降级了，那么就保持熔断状态，刷新休眠窗口期。如果请求正确了，那么熔断取消，正常运行。 参考链接 https://blog.csdn.net/loushuiyifan/article/details/82702522 参数说明 请求总数阈值：在快照时间窗内，必须满足请求总数阈值才有资格熔断。 快照时间窗：断路器通过在统计一定时间内的请求和错误等数据决定是否开启熔断，而统计的时间范围就是快照时间窗，默认就是最近10秒 错误百分比阈值，当请求总数在快照时间窗内超过了阈值，而且错误请求的次数占的百分比超过了该错误百分比阈值，那么就会将断路器打开 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:8:4","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"监控页面 建模块 改pom 增加： \u003c!--hystrix仪表盘图形化页面--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-netflix-hystrix-dashboard\u003c/artifactId\u003e \u003c/dependency\u003e 改yml 写主启动类 启动，访问localhost:9001/hystrix 修改8005 监控8005 访问监控网址，测试 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:8:5","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"GateWay ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:9:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"入门配置 创建模块 改pom \u003c!--gateway网关--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-gateway\u003c/artifactId\u003e \u003c/dependency\u003e 写yml server:port:9527spring:application:name:cloud-gatewayeureka:instance:hostname:cloud-gateway-serviceclient:register-with-eureka:truefetch-registry:trueservice-url:defaultZone:http://eureka7001:7001/eureka,http://eureka7002:7002/eureka#EurekeServer的地址(集群版) 写主启动类 写配置 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:9:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"动态路由 配置文件配置 server:port:9527spring:application:name:cloud-gatewaycloud:gateway:discovery:locator:enabled:true#开启从服务注册中心中动态创建路由的功能,利用微服务名进行路由routes:- id:payment_routh#配置路由的id,没有固定规则,但是要求唯一uri:lb://cloud-payment-service#匹配后提供的服务地址predicates:- Path=/payment/**#断言,根据路径进行路由##################- id:payment_routh2uri:lb://cloud-payment-service#匹配后提供服务的地址predicates:- Path=/service#断言,根据路径进行路由eureka:instance:hostname:cloud-gateway-serviceclient:#表示自己注册进EurekaServerregister-with-eureka:true#是否从EurekaServer抓取已有的配置信息,默认为true(单节点无所谓,集群必须设置为true才可以配置ribbon使用负载均衡)fetch-registry:trueservice-url:defaultZone:http://eureka7001:7001/eureka,http://eureka7002:7002/eureka#EurekeServer的地址(集群版) 访问localhost:9527/payment/1测试 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:9:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"常用Predicate 也就是常用的断言配置 After Route Predicate 请求时间在该时间之后可以匹配 在配置文件中配置 可以通过如下Api进行获得该格式的日期: 在该日期之前无法访问 该日期之后可以访问 Before Route Predicate 请求的时间在这个时间之前可以匹配 Between Route Predicate 请求时间在两个时间内，可以匹配 Cookie Route Predicate 匹配Cookie断言 需要两个参数，第一个是Cookie name，第二个是政策表达式 Gateway会去检验对应的Cookie的value是否符合正则表达式 配置Predicate Header Route Predicate 请求头中进行匹配 两个参数，一个是请求头名，一个是值 测试不匹配 测试匹配 Method Route Predicate Path Route Predicate Query Route Predicate ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:9:3","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Filter 自定义全局日志过滤器 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:9:4","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Config ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:10:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"服务端(连接github) 微服务统一配置 建模块cloud-config-center-3344 写pom 增加 \u003c!--配置中心--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-config-server\u003c/artifactId\u003e \u003c/dependency\u003e 写配置文件 主启动类 测试localhost:3344/master/config-dev.yml 如果连接失败，使用ssh方式连接，使用旧方法创建私匙，在github中添加，然后配置文件中改成ssh的地址 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:10:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"客户端(连接服务端) 创建模块cloud-config-client-3355 改pom \u003c!--配置中心客户端--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-config\u003c/artifactId\u003e \u003c/dependency\u003e 写bootstrap.yml application.yml是用户级的资源配置 bootstrap.yml是系统级，优先级更高 写主启动类 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:10:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"客户端动态更新 服务端可以实时更新，但是客户端不能实时更新 引入监控依赖 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-actuator\u003c/artifactId\u003e \u003c/dependency\u003e 配置文件暴露监控端点 在controller添加注解@RefreshScope ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:10:3","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Bus 什么是总线？ 在微服务架构的系统总，通常用使用轻量级的消息代理来构建一个公用的消息主题，并让系统中所有微服务的实例都连接上来，由于该主题中产生的消息会被所有实例监听和消费，所以称它为消费总线。在总线上的各个实例，都可以方便的广播一些需要其他连接到该主题上的实例都知道的消息 基本原理 ConfigClient实例都监听MQ中同一个topic(默认是SpringCloudBus)。当一个服务刷新数据的时候，它会把这个消息放入到Topic中，这样其他监听同一Topic的服务就能得到通知，然后去更新自身的配置 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:11:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"实践 再创建一个cloud-config-client-3366，步骤略 设计 利用消息总线触发一个客户端/bus/refresh，而刷新所有客户端配置 利用消息总线触发一个服务端ConfigServer的/bus/refresh端点，而刷新所有客户端的配置 方法二更适合 服务端添加Rabbitmq \u003c!--整合rabbitmq的bus消息总线--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-bus-amqp\u003c/artifactId\u003e \u003c/dependency\u003e 增加rabbitmq的配置 增加暴露刷新配置的端点 3344和3355添加消息总线的支持 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-bus-amqp\u003c/artifactId\u003e \u003c/dependency\u003e rabbitmq:host:49.234.111.177port:5672username:lcypassword:lcy021030 发送请求POST: localhost:3344/actuator/bus-refresh完成自动更新 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:11:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Stream 驱动，屏蔽底层消息中间件的差异 应用程序通过inputs或者outputs来与SpringCloudStream中的binder对象交互 通过配置来绑定(binding)，而SpringCloudStream的binder对象负责与消息中间件交互 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:12:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"实践 生产者 创建模块cloud-stream-rabbitmq-provider8801 写pom \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-stream-rabbit\u003c/artifactId\u003e \u003c/dependency\u003e 写yml server:port:8801spring:application:name:cloud-stream-providercloud:stream:bindings:# 服务的整合处理output:# 这个名字是一个通道的名称destination:studyExchange# 表示要使用的Exchange名称定义content-type:application/json# 设置消息类型，本次为json，文本则设置“text/plain”binder:rabbit1# 设置要绑定的消息服务的具体设置#可以配置多个底层MQ的配置binders:# 在此处配置要绑定的rabbitmq的服务信息；rabbit1:# 表示定义的名称，用于于binding整合type:rabbit# 消息组件类型environment:# 设置rabbitmq的相关的环境配置spring:rabbitmq:host:49.234.111.177port:5672username:lcypassword:lcy021030rabbitmq:host:49.234.111.177port:5672username:lcypassword:lcy021030eureka:client:service-url:defaultZone:http://eureka7001:7001/eureka,http://eureka7002:7002/eureka#EurekeServer的地址(集群版)instance:instance-id:send-8801.com#在信息列表显示的主机名称lease-renewal-interval-in-seconds:2#设置心跳的时间(默认是30秒)lease-expiration-duration-in-seconds:5prefer-ip-address:true 写主启动类 写业务接口 消费者 新建模块cloud-stream-rabbitmq-consumer8802 写pom 写yml 写业务 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:12:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"重复消费 此时两个消费者是不同组的，都监听一个交换机，所以都会收到消息，因此我们需要将两个消费者放到同一个组中 自定义分组 如果我们不自定义分组，那么默认使用的是不同的分组，因此可以自定义选择分组 在8802和8803中加入分组，那么收到的就是将两个消费者监听在同一个组中，消息则只能被消费一次 此时两个消费者就轮询接受消息。 原理 其实就是rabbitmq中的fandout和direct，默认情况两个消费者是不同的队列，然后绑定到同一个交换机，那么我们生产者生产一个消息到交换机，就会被两个队列都拿到。 当我们配置了组名时，其实就是变成了同一个队列，那么生产的消息发送到交换机时，交换机只发了一条消息给队列，那么监听同一个队列的消费者，只有一个能消费到消息，默认是轮询消费 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:12:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"Sleuth SpringCloudSleuth提供了分布式追踪的解决方案 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:13:0","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"搭建zipkin 下载jar包后直接启动 访问localhost:9411查看面板 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:13:1","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":["Spring"],"content":"实践 cloud-provider8001添加依赖 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.cloud\u003c/groupId\u003e \u003cartifactId\u003espring-cloud-starter-zipkin\u003c/artifactId\u003e \u003c/dependency\u003e 改配置 在consumer做一样的依赖和配置 ","date":"2021-12-09","objectID":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/:13:2","tags":["Java","SpringCloud","微服务"],"title":"SpringCloud基础学习","uri":"/springcloud%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/"},{"categories":null,"content":" 西安电子科技大学 2020级本科CS专业在读 会一点Golang \u0026 Java 对分布式系统感兴趣 仍处于躺平阶段 首要目标是顺利毕业 远大目标是在35岁之前光荣退休 ","date":"2021-12-09","objectID":"/about/:0:0","tags":null,"title":"关于 TheR1sing3un","uri":"/about/"},{"categories":null,"content":"以下是我的朋友们 对底层比较执着的大佬兼我的舍友 🐧刘陶峰的个人博客 难得一遇的超级前端大哥 🍅大faga的个人博客 前端+ACM双修的神 🔮Littlechai的个人博客 算法竞赛大神和后端开发同届大佬 🏆Attack204的个人博客 ","date":"2021-12-09","objectID":"/friendship/:0:0","tags":null,"title":"我的朋友","uri":"/friendship/"},{"categories":["开发工具学习"],"content":"Linux下开发工具之Tmux","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"Tmux学习 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:0:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"一.什么是Tmux ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:1:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"1.1传统命令行 命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称\"窗口\"），在里面输入命令。用户与计算机的这种临时的交互，称为一次\"会话\"（session） 。 会话的一个重要特点是，窗口与其中启动的进程是连在一起的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完。 一个典型的例子就是，SSH 登录远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。 为了解决这个问题，会话与窗口可以\"解绑\"：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话\"绑定\"其他窗口。 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:1:1","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"1.2Tmux作用 将会话与窗口解绑的工具 功能 允许在单个窗口中，同时访问多个会话。 可以让新窗口接入已经存在的会话 允许每个会话有多个连接窗口，可以多人实时共享会话 支持窗口任意的垂直和水平拆分 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:1:2","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"二.基本用法 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:2:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"2.1安装 centos系统下的安装 sudo yum install tmux ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:2:1","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"2.2启动和退出 启动 tmux 启动Tmux窗口,底部有一个状态栏。状态栏左侧是窗口信息(编号和名称)，右侧是系统信息 退出 exit或者Ctrl + D ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:2:2","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"2.3前缀键 Tmux的快捷键需要前缀键进行唤醒，默认前缀键是 Ctrl + B。先按下前缀键，快捷键才会生效 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:2:3","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"三.会话管理 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:3:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"3.1新建会话 默认启动的Tmux窗口编号从0开始，我们可以自己给会话起名字 tmux new -s \u003csession-name\u003e ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:3:1","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"3.2分离会话 分离会话 在Tmux窗口下，按住Ctrl + B D或者输入tmux detach，就会将当前会话和窗口分离，上面命令执行后，就会退出当前的Tmux窗口，但是后台仍在运行里面的会话和进程 查看所有会话 tmux ls ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:3:2","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"3.3接入会话 用于重新接入某个已经存在的会话 tmux attach -t \u003csession-name\u003e ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:3:3","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"3.4杀死会话 tmux kill-session -t \u003csession-name\u003e ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:3:4","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"3.5切换会话 tmux switch -t \u003csession-name\u003e ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:3:5","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"3.6重命名会话 tmux rename-session -t \u003csession-name\u003e \u003cnew-name\u003e ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:3:6","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"3.7会话快捷键 Ctrl + B D：分离当前会话 Ctrl + B S：列出所有会话 Ctrl + B $：重命名当前会话 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:3:7","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"四.最简操作流程 新建会话tmux new -s \u003csession-name\u003e 在Tmux窗口运行所需的程序 按下快捷键Ctrl +B D将会话分离 下次使用时，重新连接到会话tmux attach-session -t \u003csession-name\u003e ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:4:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"五.网格操作 Tmux可以将窗口分成多个窗格，每个窗格运行不同的命令 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:5:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"5.1划分窗格 tmux split-window #划分上下两个窗格 tmux split-window -h #划分左右两个窗格 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:5:1","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"5.2移动光标 使用tmux select-pane命令用来移动光标位置 tmux select-pane -U #光标切换到上方窗格 tmux select-pane -D #光标切换到下方窗格 tmux select-pane -L #光标切换到左边窗格 tmux select-pane -R #光标切换到右边窗格 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:5:2","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"5.3交换窗格位置 tmux swap-pane 命令来交换窗格位置 tmux swap-pane -U #当前窗格上移 tmux swap-pane -D #当前窗格下移 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:5:3","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"5.4窗格快捷键 Ctrl+b %：划分左右两个窗格。 Ctrl+b \"：划分上下两个窗格。 Ctrl+b \u003carrow key\u003e：光标切换到其他窗格。\u003carrow key\u003e是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键↓。 Ctrl+b ;：光标切换到上一个窗格。 Ctrl+b o：光标切换到下一个窗格。 Ctrl+b {：当前窗格与上一个窗格交换位置。 Ctrl+b }：当前窗格与下一个窗格交换位置。 Ctrl+b Ctrl+o：所有窗格向前移动一个位置，第一个窗格变成最后一个窗格。 Ctrl+b Alt+o：所有窗格向后移动一个位置，最后一个窗格变成第一个窗格。 Ctrl+b x：关闭当前窗格。 Ctrl+b !：将当前窗格拆分为一个独立窗口。 Ctrl+b z：当前窗格全屏显示，再使用一次会变回原来大小。 Ctrl+b Ctrl+\u003carrow key\u003e：按箭头方向调整窗格大小。 Ctrl+b q：显示窗格编号 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:5:4","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"六.窗口管理 Tmux允许新建多个窗口 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:6:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"6.1新建窗口 tmux new-window命令用来创建新窗口 tmux new-window #新建一个窗口 tmux new-wndow -n \u003cwindow-name\u003e #新建一个指定名称的窗口 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:6:1","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"6.2切换窗口 tmux select-window命令用来切换窗口 tmux select-window -t \u003cwindow-number\u003e #切换到指定编号的窗口 tmux select-window -t \u003cwidnow-name\u003e #切换到指定编号的窗口 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:6:2","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"6.3重命名窗口 tmux rename-window命令用于为当前窗口起名(或重命名) tmux rename-window \u003cnew-name\u003e ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:6:3","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"6.4窗口快捷键 Ctrl+b c：创建一个新窗口，状态栏会显示多个窗口的信息。 Ctrl+b p：切换到上一个窗口（按照状态栏上的顺序）。 Ctrl+b n：切换到下一个窗口。 Ctrl+b \u003cnumber\u003e：切换到指定编号的窗口，其中的\u003cnumber\u003e是状态栏上的窗口编号。 Ctrl+b w：从列表中选择窗口。 Ctrl+b ,：窗口重命名。 ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:6:4","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"七.其他命令 # 列出所有快捷键，及其对应的 Tmux 命令 tmux list-keys # 列出所有 Tmux 命令及其参数 tmux list-commands # 列出当前所有 Tmux 会话的信息 tmux info # 重新加载当前的 Tmux 配置 tmux source-file ~/.tmux.conf ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:7:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"},{"categories":["开发工具学习"],"content":"八.参考连接 https://www.ruanyifeng.com/blog/2019/10/tmux.html ","date":"2021-12-09","objectID":"/tmux%E5%AD%A6%E4%B9%A0/:8:0","tags":["linux","开发工具"],"title":"Tmux学习","uri":"/tmux%E5%AD%A6%E4%B9%A0/"}]